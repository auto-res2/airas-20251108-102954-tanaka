{
  "research_topic": "Improving fine-tuning performance of language models.",
  "queries": [
    "fine-tuning optimization",
    "parameter-efficient tuning",
    "adapter modules",
    "low-resource fine-tuning",
    "hyperparameter optimization"
  ],
  "research_study_list": [
    {
      "title": "Hint-Aug: Drawing Hints From Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning",
      "abstract": "Despite the growing demand for tuning foundation vision transformers (FViTs)\non downstream tasks, fully unleashing FViTs' potential under data-limited\nscenarios (e.g., few-shot tuning) remains a challenge due to FViTs' data-hungry\nnature. Common data augmentation techniques fall short in this context due to\nthe limited features contained in the few-shot tuning data. To tackle this\nchallenge, we first identify an opportunity for FViTs in few-shot tuning:\npretrained FViTs themselves have already learned highly representative features\nfrom large-scale pretraining data, which are fully preserved during widely used\nparameter-efficient tuning. We thus hypothesize that leveraging those learned\nfeatures to augment the tuning data can boost the effectiveness of few-shot\nFViT tuning. To this end, we propose a framework called Hint-based Data\nAugmentation (Hint-Aug), which aims to boost FViT in few-shot tuning by\naugmenting the over-fitted parts of tuning samples with the learned features of\npretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an\nAttentive Over-fitting Detector (AOD) to detect over-confident patches of\nfoundation ViTs for potentially alleviating their over-fitting on the few-shot\ntuning data and (2) a Confusion-based Feature Infusion (CFI) module to infuse\neasy-to-confuse features from the pretrained FViTs with the over-confident\npatches detected by the above AOD in order to enhance the feature diversity\nduring tuning. Extensive experiments and ablation studies on five datasets and\nthree parameter-efficient tuning techniques consistently validate Hint-Aug's\neffectiveness: 0.04% ~ 32.91% higher accuracy over the state-of-the-art (SOTA)\ndata augmentation method under various low-shot settings. For example, on the\nPet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training\ndata over SOTA data augmentation methods.",
      "full_text": "Hint-Aug: Drawing Hints from Foundation Vision Transformers towards Boosted Few-shot Parameter-Efficient Tuning Zhongzhi Yu1, Shang Wu 2, Yonggan Fu 1, Shunyao Zhang 2, Yingyan (Celine) Lin 1 1Georgia Institute of Technology 2Rice University {zyu401, yfu314, celine.lin}@gatech.edu {sw99,sz74}@rice.edu Abstract Despite the growing demand for tuning foundation vision transformers (FViTs) on downstream tasks, fully unleash- ing FViTs’ potential under data-limited scenarios (e.g., few- shot tuning) remains a challenge due to FViTs’ data-hungry nature. Common data augmentation techniques fall short in this context due to the limited features contained in the few-shot tuning data. To tackle this challenge, we first iden- tify an opportunity for FViTs in few-shot tuning: pretrained FViTs themselves have already learned highly representa- tive features from large-scale pretraining data, which are fully preserved during widely used parameter-efficient tun- ing. We thus hypothesize that leveraging those learned fea- tures to augment the tuning data can boost the effective- ness of few-shot FViT tuning. To this end, we propose a framework called Hint-based Data Augmentation ( Hint- Aug), which aims to boost FViT in few-shot tuning by aug- menting the over-fitted parts of tuning samples with the learned features of pretrained FViTs. Specifically, Hint-Aug integrates two key enablers: (1) an Attentive Over-fitting Detector (AOD) to detect over-confident patches of founda- tion ViTs for potentially alleviating their over-fitting on the few-shot tuning data and (2) a Confusion-based Feature Infusion ( CFI) module to infuse easy-to-confuse features from the pretrained FViTs with the over-confident patches detected by the above AOD in order to enhance the feature diversity during tuning. Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consistently validate Hint-Aug’s effectiveness: 0.04% ∼ 32.91% higher accuracy over the state-of-the-art (SOTA) data augmentation method under various low-shot settings. For example, on the Pet dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data over SOTA data augmentation methods. 1. Introduction Foundation vision transformers (FViTs) [16, 41, 54, 55, 64] with billions of floating point operations (FLOPs) and parameters have recently demonstrated significant poten- tial in various downstream tasks [40, 41]. The success of FViTs has ushered in a new paradigm in deep learning: pretraining-then-tuning [16, 40, 67], which first pretrains an FViT on a large-scale dataset, then uses recently developed parameter-efficient tuning methods (e.g., visual prompt tun- ing (VPT) [34], visual prompting [2], LoRA [33], and Adapter [72]) to tune pretrained FViTs on downstream tasks with limited tuning data. However, although it is highly de- sirable, effectively tuning pretrained FViTs for real-world applications, especially under few-shot tuning scenarios, re- mains a particularly challenging task. The reason is that al- though parameter-efficient tuning methods are dedicatedly designed for FViTs and can alleviate the overfitting issue by reducing the number of trainable parameters [2, 34, 72], the data-hungry nature of FViTs [16, 54] is not mitigated and thus the achievable accuracy under data-limited scenarios (e.g., few-shot tuning scenarios) are still limited. Therefore, how to effectively tune pretrained FViTs on various down- stream tasks with few-shot tuning is still an open question. To enhance the effectiveness of parameter-efficient FViT tuning under few-shot settings, one promising direction is to leverage data augmentation techniques to increase the data diversity and thus the feature diversity of the models when being tuned on few-shot data, boosting the achievable accu- racy [12, 31, 68, 71]. Nevertheless, it has been shown that existing data augmentation techniques fall short in boost- ing the model accuracy under few-shot tuning scenarios. This is because most of the existing data augmentation tech- niques are random-based (e.g., RandAugment [13], Au- toAugment [12], color jitter, mixup [71], and cutmix [68]), which only randomly permute existing features in the train- ing data and thus cannot generate new and meaningful features [63]. As illustrated in Fig. 1, we observe that neither the widely-used random-based data augmentation techniques (i.e., a dedicated combination of techniques in- cluding RandAugment [13], color jitter, and random eras- ing [74] as in [72]) nor training without data augmentation can consistently achieve a satisfactory accuracy across dif- †Our code is available at https://github.com/GATECH- EIC/Hint-Aug arXiv:2304.12520v3  [cs.CV]  26 Jun 20234-shot on Aircraft 12-shot on Aircraft 16-shot on Aircraft 8-shot on Pets 12-shot on Pets 16-shot on Pets 4-shot on Food 8-shot on Food 12-shot on Food 16-shot on Food8-shot on Aircraft No-Aug Ours NPS  4-shot on Pets Figure 1. The normalized achieved accuracies when few-shot tun- ing the ViT-base model [16] on various datasets and numbers of tuning shots using (1) vanilla training without augmentation (i.e., No-Aug), (2) the SOTA parameter-efficient tuning technique [72] (i.e., NPS), and (3) our proposed Hint-Aug. ferent datasets under few-shot tuning. Specifically, when being applied to fine-grained classification tasks, e.g., the Aircraft dataset [45], these random-based data augmenta- tion techniques actually hurt the achievable accuracy. The reason is that random-based data augmentation techniques can easily create out-of-manifold samples [68, 71], espe- cially on commonly used fine-grained datasets. Such out- of-manifold samples can largely degrade the achievable ac- curacy given the limited number of training samples under few-shot tuning scenarios [27]. Therefore, it is crucial to develop data augmentation techniques that can adaptively augment the given training samples with diverse, but still within-manifold, features to boost the effectiveness of tun- ing FViTs on various downstream tasks. This work sets out to close the increasing gap be- tween the growing demand for effective few-shot FViT tun- ing and the unsatisfactory achievable accuracy by exist- ing techniques. In particular, we identify that in few-shot parameter-efficient tuning, the pretrained FViTs’ weights are fixed during tuning. Meanwhile, existing works have shown that (1) pretrained transformer models have already learned complex but generalizable features [16, 41, 54] and (2) gradient-based methods can extract the learned features from pretrained models and then add them to the input images [44, 46]. Therefore, we hypothesize that FViTs’ few-shot tuning accuracies can be non-trivially improved by leveraging the learned features in the pretrained FViTs. Specifically, we make the following contributions: • We propose a framework called Hint-based Data Augmentation (Hint-Aug), which is dedicated to boosting the achievable accuracy of FViTs under few-shot tuning scenarios by leveraging the learned features of pretrained FViTs to guide the data augmentation strategy used for the training dataset in an input-adaptive manner. • Our Hint-Aug framework integrates two enablers: (1) an Attentive Over-fitting Detector (AOD) to identify the over- fitting samples and patches in the given training dataset by making use of the attention maps of pretrained FViTs and (2) a Confusion-based Feature Infusion (CFI) module to adaptively infuse pretrained FViTs’ learned features into the training data to better tuning those models on down- stream tasks, alleviating the commonly recognized chal- lenge of having limited features under few-shot tuning. • Extensive experiments and ablation studies on five datasets and three parameter-efficient tuning techniques consis- tently validate the effectiveness of our proposed Hint-Aug framework, which achieves a 0.04% ∼ 32.91% higher accuracy over state-of-the-art (SOTA) data augmentation methods [72] across different datasets and few-shot set- tings. For example, on the Pets dataset, Hint-Aug achieves a 2.22% higher accuracy with 50% less training data com- pared with the SOTA augmentation method. 2. Related Works 2.1. FViTs Inspired by the recent success of vision transformers (ViTs), one of the most notable directions in ViTs is to scale up ViTs’ model size to build FViTs, aiming to replicate the success of large-scale neural language processing founda- tion models [15, 19, 50] in the field of computer vision [16, 40, 67]. Existing efforts in developing FViTs mainly fall into two categories: (1) exploring how to scale up ViTs’ architectures to construct powerful FViTs [40, 69, 75]; (2) developing self-supervised pretraining techniques to train FViTs so that their learned representations can be more ef- fectively generalized to downstream tasks [4, 7, 17, 30, 36]. Unlike conventional convolutional neural networks (CNNs), FViTs extensively use the self-attention mech- anism to extract global features, resulting in improved task accuracy with larger models (e.g., over 10G FLOPs). Specifically, in ViTs, a series of N input image patches X = [x1, ··· , xN ]⊤ ∈ RN×D, where D is the embedding dimension, is sequentially processed by ViT blocks. In each block, the input is first converted into queries Q ∈ RN×d, keys K ∈ RN×d and values V ∈ RN×d (d denotes the hid- den dimension) via linear projection, followed by the com- putation of the self-attention, which is calculated as: Attention(Q, K, V) = softmax(QKT √ d )V (1) The outputs are then fed into a feed-forward network to extract information in the channel dimension.2.2. Parameter-efficient Tuning Motivated by the impressive pretraining performance of FViTs on large-scale datasets, there has been a grow- ing interest in applying FViTs to real-world applications. The common solution follows the pretraining-then-tuning paradigm, which tunes pretrained FViTs on various down- stream tasks based on the corresponding applications’ needs. However, with conventional weight tuning, each task would need to store an additional set of model weights, which can lead to cumbersome and prohibitive storage over- head. To this end, various parameter-efficient tuning meth- ods have been proposed [2,33,53,72]. In parameter-efficient tuning, a set of tiny learnable modules are added to the pre- trained FViTs, while the weights of the backbone FViTs remain unchanged during tuning [32–34]. This approach offers two benefits: (1) it allows FViTs to be tuned on new downstream tasks with negligible additional parame- ters, and (2) the pretrained FViTs can be easily retrieved at any time by simply removing the added parameter-efficient tuning modules. Among recent parameter-efficient tuning techniques, LoRA [33] proposes to learn a set of low-rank weights and apply them on top of the backbone weights, and VPT [34] proposes to use the idea of prompt tuning, inserting a set of task-specific prompts as additional tokens. More recently, NPS [72] proposes to search for the optimal combination of parameter-efficient tuning techniques and their correspond- ing hyperparameters through neural architecture search. 2.3. Few-shot Tuning Few-shot tuning aims to tune pretrained models on new tasks with limited samples per class [18, 21, 28, 39, 42]. It has gained increasing attention in recent years [59] as high- quality data is scarce in many real-world applications [3]. Recently, a few pioneering works that target few-shot tun- ing for ViTs propose to customize meta-learning tasks and learning objectives under the guidance of self-attention modules [8, 10, 38, 61, 65]. In this paper, we aim to en- hance FViTs’ few-shot tuning accuracy from an orthogonal direction, i.e., adaptively augmenting the few-shot tuning samples to compensate for their lack of diverse features. 2.4. Data Augmentation Data augmentation aims to enhance data diversity and thus the feature diversity of the models [9, 11, 24, 31, 49, 60, 68, 71, 74]. An effective data augmentation strat- egy should properly enhance data diversity, while simul- taneously avoiding the generation of out-of-manifold data caused by excessive augmentation intensity [57, 71]. Al- though various data augmentation techniques have been proposed, how to effectively augment the data under few- shot tuning settings is still an open question. The limited data diversity in few-shot data calls for techniques that can generate novel but meaningful features [62,63]. To this end, most existing few-shot data augmentation techniques adopt generative models to generate in-domain data, which, how- ever, further increase the memory and storage overhead of tuning FViTs [23, 29, 37, 43]. One potential way to alleviate the aforementioned chal- lenges is to use adversarial techniques to generate samples with beneficial features [20, 51, 59]. However, the major- ity of these works focus on improving adversarial robust- ness instead of the clean accuracy [20, 26, 51, 59, 70, 73]. In contrast, our work explores the opportunities of leverag- ing adversarial training to generate beneficial features that can boost the clean accuracy during few-shot parameter- efficient tuning. 3. The Proposed Hint-Aug Framework 3.1. Hint-Aug: Motivation We first identify that the characteristics of parameter- efficient tuning together with pretrained FViTs provide a unique opportunity for FViTs’ parameter-efficient tuning. Based on this observation, we then propose our Hint-Aug framework, which utilizes these characteristics to enhance the tuning effectiveness. We describe each of the character- istics in detail below: Characteristics of parameter-efficient tuning : As mentioned in Sec. 2.1 and Sec. 2.2, the weights of pre- trained FViTs are fixed during tuning. Therefore, the tuned FViTs behave the same as their pretrained counter- part after the added tuning modules (e.g., those adopted in Adapter [32], VPT [34], and LoRA [33]) are removed [72]. This motivates us to consider whether we can make use of this characteristic to improve the achievable few-shot tuning accuracy by leveraging the pretrained FViTs. Characteristics of pretrained FViTs : Existing works have shown that pretrained FViTs have two promising char- acteristics regarding their learned features: (1) pretrained FViTs can identify complex but meaningful features [16, 41], even on unseen datasets without tuning [6, 30, 36]; (2) the learned features in FViTs can be reversely pro- jected to the input image space using gradient-based meth- ods [22, 44, 46]. Given the aforementioned characteristics of both parameter-efficient tuning and pretrained FViTs, we hy- pothesize that these characteristics provide a unique oppor- tunity to effectively leverage the pretrained FViTs to aug- ment the few-shot tuning data. To validate our hypothesis, we aim to explore proper ways to leverage the learned fea- tures in pretrained FViTs to boost the effectiveness of few- shot tuning. Specifically, given the two commonly recog- nized major challenges of few-shot tuning, which are over- fitting [1, 58] and the lack of data diversity in the tuning data [62, 63], we set out to answer the following questions:Attention-score Maps Patch w/ largeattn. difference Patch Selection Confusion-based Adversarial Targets HDA AugmentedSample Attentive Over-fitting Detector Confusion-based Feature Infusion Pre-trained ViT ViT w/ PET Large gap Small gap AdversarialAttack Freeze Trainable PET FViT FViT Attention-score Difference PatchPerturbation PET FViT Cat: Confusion Matrix Image Dog ... 5 10 20 1 ... BirdCatHorseFish Cat ... 0 Class ... 0. 0.6 0.3 0. 0.06 DogBirdCatHorseFish Patch w/max attn.  ... Figure 2. An overview of our proposed Hint-Aug framework, which consists of two enablers: (1) an AOD to detect whether the current sample is prone to over-fitting and which patch is prone to over-fitting, and (2) a CFI to infuse easy-to-confuse features to the over-fitted patches detected by the aformentioned AOD to increase the feature diversity of the tuning data and thus alleviate the over-fitting issue. In the figure, PET represents the parameter-efficient tuning module (e.g., Adapter [32], VPT [34], and LoRA [33]) added on top of the pretrained FViTs. Q1 - Can pretrained FViTs detect the potential over-fitting issue during few-shot tuning? and Q2 - Can we leverage pretrained FViTs to generate meaningful features for en- hancing the data diversity? Our proposed Hint-Aug frame- work provides an effective solution to these two questions. 3.2. Hint-Aug: Overview We first give an overview of our proposed Hint-Aug framework, which is dedicatedly designed for few-shot parameter-efficient tuning of FViTs by leveraging the characteristic of parameter-efficient tuning that pretrained FViTs’ weights are not updated during tuning, allowing the features learned in pretrained FViTs to be utilized to aug- ment the tuning data. As shown in Fig. 2, Hint-Aug adopts a two-stage detect-then-augment pipeline. In particular, to answer the above Q1, Hint-Aug uses AOD to detect (1) whether the tuned FViT is over-fitted on this image and (2) which patch in the image is more prone to be over-fitted; To address Q2, Hint-Aug further augments the patch de- tected from AOD by infusing the easy-to-confuse features with our proposed CFI module. We introduce our AOD and CFI modules in Sec. 3.3 and Sec. 3.4, respectively. 3.3. Enabler 1: Attentive Over-fitting Detector Over-fitting is a well-known issue in few-shot tuning sce- narios [37, 62], and becomes even more severe due to the combination of larger model size and limited data size dur- ing few-shot FViT tuning. Therefore, our AOD aims to explore whether we can detect the underlying over-fitting issue for each tuning sample on-the-fly during parameter- efficient tuning of FViTs. Inspired by the various visualizations showing FViTs’ at- tention distributions in previous works [22, 30, 52, 66], we hypothesize that the evolution of attention distributions dur- ing the tuning process contains hidden traces for identifying the existence of over-fitting. To validate this hypothesis, we utilize an attention-score map to quantify the impact of each input image patch on the FViT’s attention distribution. Specifically, an attention-score map is constructed with the attention-score corresponding to each patch of the input im- age and we define the attention-score as follows: given the attention distribution [a(l,h,i) 1 , ··· , a(l,h,i) N ] for the i-th patch of the h-th head in the l-th layer, the attention-score s(l,k) j of the j-th patch for the k-th query patch is defined as: s(l,k) j = X h a(l,h,k) j (2) For the sake of simplicity, we omit the superscript l and k in the following text. By visualizing the attention-score at different stages of tuning, as shown in Fig. 3, we can draw two observations: (1) the attention-score map of the pretrained FViT itself (see Fig. 3(a)) shares a high correlation with that of the half- tuned FViT model (see Fig. 3(b)), and a relatively higher tuning accuracy (e.g., 64.37%) suggests that the over-fitting issue is not severe at the corresponding tuning stage; (2) the attention-score map at the end of tuning (see Fig. 3(c)) fo- cuses more on certain patches (marked in red) that are not focused on by the pretrained FViT, and a lower tuning accu- racy (e.g., 61.55%) indicates the existence of the over-fitting issue. Additionally, we observe that patches with a newly attracted higher attention-score (marked in red) do not con- tain human-readable information for identification. For ex- ample, some patches only consist of a black background that does not contribute to identifying the target class, e.g., a cheese plate. This finding suggests that these patches could be the reason for over-fitting. Based on the observations above, we propose an AOD module to use the attention-score map difference between the pretrained FViT and the corresponding one being tunedInput Image (a) (b) Accuracy: 64.37% (c) Accuracy: 61.59% Figure 3. Visualization of the attention-score map from the (a) pre- trained foundation model, called ViT-base, (b) parameter-efficient tuned ViT-base model with 20% of the total tuning epochs, achiev- ing an accuracy of 64.37%, and (c) parameter-efficient tuned ViT- base model with an accuracy of 61.55%. to identify both the existence of over-fitting and which patch contributes most to the over-fitting issue. Specifi- cally, given the attention-score maps SP = [sP 1 , ··· , sP N ] generated from the pretrained FViT (denoted as P) and ST = [sT 1 , ··· , sT N ] generated from the FViT model to be tuned (denoted as T), we define the over-fitting indicator as: I = \u001a 0, P i ∥sP i − sT i ∥ < λP i ∥sP i ∥ 1, otherwise (3) where λ is a hyperparameter to control the sensitivity of over-fitting detection. When over-fitting occurs (i.e., I = 1), we select the patch that significantly changes the attention-score map as the target patch to be augmented in order to alleviate the over-fitting issue. Thus, we select the patch p to augment, where p is defined by: p = arg max i (∥sP i − sT i ∥) (4) Otherwise, when there is no detected over-fitting issue, we select the patch p with the highest attention-score as the target patch to be augmented from all patches in the corre- sponding image. 3.4. Enabler 2: Confusion-based Feature Infusion With the selected over-fitted patch detected by the AOD above, the remaining question is how to augment the se- lected patch with meaningful features to (1) alleviate the over-fitting issue and (2) increase the diversity of tuning data with meaningful features. Therefore, we propose CFI that uses adversarial attack-based methods to extract the learned features from the pretrained FViT model and infuse them into the selected patch with the aim of improving the feature diversity in a meaningful way, thus alleviating the over-fitting issue. However, achieving a meaningful feature extraction and infusion that can help boost the few-shot tuning accuracy is non-trivial. Naively augmenting samples with commonly used attack objectives (e.g., perturbing the image to reduce the value of the model’s output logit on the correct class) can easily lead to out-of-manifold samples, as shown in our alation study in Sec. 4.3.2. To overcome this, the CFI mod- ule incorporates injected features to steer the model predic- tion towards a synthetic target label. This target label is determined by utilizing a confusion matrix, which quanti- fies the degree to which the model is prone to confusion between pairs of classes. Specifically, we construct a confusion matrix C ∈ RM×M ≥0 in CFI, where M is the total number of classes. As shown in a recent study on open set detection [56], a pre-softmax model output has a better ability to preserve a model’s uncertainty of samples. We thus define C as fol- lows: Ci,j = X X:y(X)=j \u0010 fi(X) − min i′ fi′ (X) \u0011 (5) where i and j are coordinates in C that represent two classes; y and f ∈ RM are the ground truth label and pre- softmax output given the input image X. The generated confusion matrix C helps to identify the class-wise similar- ity learned by the model and distinguish the class pairs that are easy to be confused by the model. To infuse the easy-to-confuse features to the patch, given input X with label y, we propose to design the attack label ˜f(X) ∈ RM ≥0 where the i-th element is computed as: ˜fi(X) = ( Ci,yP j Cj,y−Cy,y , i ̸= y 0, i = y (6) The loss function is defined as Ltar = CrossEntropy(softmax(f), softmax( ˜f)) (7) By optimizing the patch to minimize the above loss, the generated features are further shifted towards the direction where the model considers an easy-to-confuse class from the current class. This shift allows the model to learn to dif- ferentiate between the current class and the easy-to-confuse class, effectively extending the decision boundary of the current class. 4. Experimental Results 4.1. Experimental Setup Datasets, few-shot settings, models, and parameter- efficient tuning techniques. Datasets and few-shotsettings. We adopt five commonly-used datasets for few- shot tuning, including Food [5], Pet [48], Cars [35], Flow- ers [34], and Aircraft [45], and benchmark our Hint-Aug under 1/2/4/8/12/16-shot scenarios to provide a thorough evaluation of its achieved accuracy across different few-shot tuning scenarios. Models. We conduct our experiment on a widely used FViT model, i.e., ViT-Base [16]. Adopted parameter-efficient tuning methods. We consider three most widely used parameter-efficient tuning methods including Adapter [32], LoRA [33], and VPT [34]. Baselines. We benchmark our proposed Hint-Aug against two baselines, including the SOTA data augmen- tation technique for parameter-efficient FViT tuning intro- duced in [72] (denoted as NPS) and the vanilla tuning with- out augmentation (denoted as No-Aug). It is worth noting that, given the unique challenge of limited data diversity in the few-shot tuning scenarios, even the SOTA data aug- mentation technique, i.e., the aforementioned NPS [72], can lead to an inferior accuracy than that of the vanilla tuning without augmentation (as shown in Fig. 1). Thus, it is nec- essary to include No-Aug as one of the baselines. Tuning settings. In our experiments, we set l = 5 and adopt the center patch in each image as the query patch (i.e., k = 90), following [22]. We follow the widely adopted few-shot tuning settings in [72]. Specifically, we tune the model for 100 epochs using a batch size of 256, a learning rate of 0.01, and an SGD optimizer starting from the Ima- geNet [14] pretrained ViT-Base [16]. Following NPS [72], we also use data augmentation techniques including color- jitter with a factor of 0.4 and RandAugment [13] with a magnitude of 9 and a standard deviation equal to 0.5. We set λ in Eq. 3 as 0.1 and use FGSM [25] to generate the adver- sarial samples with attack radius ϵ = 0.001. Additionally, we run all experiments in the paper three times and report the average accuracy, following NPS [72]. 4.2. Benchmark on Few-shot Image Classification We first benchmark our proposed method on five com- monly used few-shot image classification datasets [5,34,35, 45, 48] with different parameter-efficient tuning techniques and few-shot settings. As shown in Fig. 4, although the SOTA augmentation baseline NPS [72] suffers from con- siderable accuracy degradation compared with the vanilla tuning method No-Aug on fine-grained image classification dataset (e.g., a 5.55% accuracy drop on [45]), our proposed Hint-Aug achieves 0.25% ∼ 6.10%, 0.10% ∼ 32.91%, and 0.04% ∼ 6.17% higher accuracies across different shot se- lections over baselines when using Adapter [32], VPT [34], and LoRA [33] tuning, respectively. In particular, we draw the following two exciting obser- vations: (1) the features generated by Hint-Aug can com- pensate for the lack of sufficient tuning data and improve accuracy under more stringent few-shot settings. Specif- Table 1. Ablation study on each enabler’s contribution to the final accuracy. AOD CFI Food Pets Cars 66.25 86.97 40.83 68.53 88.01 42.17 70.52 89.07 43.55 71.04 89.42 44.80 ically, Hint-Aug boosts the accuracy of 8-shot tuning by 2.45% ∼ 4.96% and surpasses the 12-shot tuning with NPS [72] by a 0.73% ∼ 2.22% higher accuracy when tun- ing Adapter and LoRA on the Food and Pets datasets; (2) Hint-Aug’s ability to extract features from the pretrained FViTs and infuse them into the tuning data can considerably boost accuracy in extreme few-shot scenarios (e.g., 1-shot tuning). For example, on the Pets dataset, tuning VPT with Hint-Aug under a 1-shot setting leads to a 32.91% higher accuracy than that of NPS [72]. 4.3. Ablation Studies 4.3.1 Accuracy Improvement Breakdown Setup. To better understand the contribution of each en- abler of Hint-Aug, including AOD and CFI, to the final accuracy, we conduct an ablation study where we run 8- shot tuning with Adapter [32] on three datasets, namely Food [5], Pets [48], and Cars [35]. We implement this accuracy improvement breakdown experiment as follows: (1) when using AOD only, we adopt the data augmentation method in [72] to augment the selected patch; (2) when us- ing CFI only, we generate the samples with Ltar loss and randomly select a patch to augment in each image. Observations. As shown in Tab. 1, when augment- ing a selected patch, we can observe that (1) enabling ei- ther AOD or CFI can lead to an accuracy improvement of 1.04% ∼ 2.28% and 2.10% ∼ 4.27% over the baseline (e.g., neither AOD nor CFI enabled), respectively. This in- dicates that both key challenges (i.e., the over-fitting issue and lack of feature diversity as analyzed in Sec. 3.1) in- deed hurt the achievable accuracy of few-shot tuning and our proposed enablers can effectively alleviate the challenge in over-fitting; (2) Combining both AOD and CFI can marry the merit of both, thus further boosting the achievable accu- racy by 0.35% ∼ 2.63% over that of enabling only one of AOD or CFI. 4.3.2 Ablation on Adversarial Objectives Setup. We conduct ablation studies to validate the choice of loss functions for generating the adversarial sample for fea- ture infusion. As mentioned in Sec. 3.4 and Sec. 2.3, differ- ent loss functions can have different impacts on the tuning accuracy and an improper loss function can lead to inferior clean accuracy. In Tab. 2, we validate the objective function we selected with other potential candidates when tuning on(a) Tuning Technique: VPT  (b) Tuning Technique: Adapter (c) Tuning Technique: LoRA No-Aug OursNPS Dataset: Food Dataset: Pets Dataset: Cars Dataset: Flowers Dataset: Aircraft Average Dataset: Food Dataset: Pets Dataset: Cars Dataset: Flowers Dataset: Aircraft Average Dataset: Food Dataset: Pets Dataset: Cars Dataset: Flowers Dataset: Aircraft Average Figure 4. Benchmark Hint-Aug on Food [5], Pets [48], Cars [35], Flowers [47], and Aircraft [45] with parameter-efficient tuning methods (a) VPT, (b) Adapter, and (c) LoRA on 1/2/4/8/12/16-shot setting. Table 2. Ablation study on different adversarial objectives. Target Full Untarget Random Proposed 4-shot 57.49 59.21 62.35 64.92 8-shot 66.04 67.36 69.14 71.04 16-shot 70.78 71.58 73.85 74.90 the Food dataset [5] using Adapter [32], where “Full” indi- cates generating adversarial samples with the whole image, instead of the selected patch, “Untarget” means using the conventional attack target that minimizes the value of the model’s output logit on the correct class by augmenting the selected patch, and “Random” means augmenting the se- lected patch to mislead the output of the augmented image toward another randomly selected class. Observations. As shown in Tab. 2, “Full” leads to the worst achieved accuracy which is 0.80% ∼ 1.72% lower than the second worst object “Untarget”. “Untarget” also leads to a 3.32% ∼ 5.71% lower accuracy than our pro- posed method. These two observations suggest that (1) at- tacking the image as a whole cannot effectively help with FViT tuning, and (2) naively using the “Untarget” attack can easily lead to out-of-manifold data. Furthermore , the 1.78% ∼ 3.14% accuracy improvement of “Random” over “Untarget” suggests that despite the simple method of se- lecting the direction to add features, adding features from other classes can help with tuning. However, the lack of a more precise augmentation direction still limits the achiev- able accuracy when using the “Random” adversarial objec- tive, leading to a 1.05% ∼ 2.57% lower accuracy than the adversarial objective adopted in Hint-Aug. 4.3.3 Sensitivity to Augmentation Intensity According to recent studies [52,54], augmentation intensity is a crucial factor in FViT tuning. Thus, we investigate the impact of the adversarial attack radius ϵ on the achievable accuracy of Hint-Aug. When tuning with Adapter [32] on Food [5] under an 8-shot setting, Hint-Aug achieves rela- tively stable achieved accuracy under the drastic change in attack radius. Specifically, as shown in Tab. 3, increasing or decreasing ϵ by 5 times only leads to a 0.03% ∼ 0.21%700 Input Image Pretrained Model Attention Attention Trained with   Hint-Aug Attention Trained with NPS 700 Input Image Pretrained Model Attention Attention Trained with NPSAttention Trained with   Hint-Aug Figure 5. Visualization of attention score maps of images trained with different augmentation techniques. Table 3. Impact of adversarial attack radius on the achievable ac- curacy of the Hint-Aug framework. ϵ 0.01 0.005 0.001 0.0002 0.0001 Acc. (%) 70.01 70.83 71.04 71.01 70.15 Table 4. Ablation on the number of selected patches to augment. # patches 1 2 3 8 32 All Average Acc. 65.42 65.44 65.35 65.08 64.59 63.72 accuracy change, while changing ϵ by 10 times leads to a 0.89% ∼ 1.03% accuracy change compared with a radius of 0.001 that we select in Hint-Aug, proving the robustness of Hint-Aug in different selections of hyperparameters. It is worth noting that changing ϵ by 10 times is a non-trivial change. As suggested in [22], changing ϵ by 8 times leads to an accuracy change larger than 27.94% when attacking DeiT-Tiny on ImageNet. 4.3.4 Number of Patches to Augment Motivated by the promising accuracy-data efficiency trade- off achieved by Hint-Aug, an interesting question arises whether augmenting more than one patch for each image can further push forward the accuracy-efficiency trade-off. To answer this question, we conduct an ablation study on Hint-Aug with different numbers of augmented patches and report the average achieved accuracy when tuning with an 8-shot VPT [34] across five datasets. Notably, augmenting all patches (i.e., column “All” in Tab. 4) is equivalent to augmenting the whole image without considering the patch information. Our experiments show that augmenting one to three patches in each image leads to similar average ac- curacy (less than 0.1% accuracy change). However, when augmenting more patches in the image, the average accu- racy drops by 0.34% ∼ 1.70% when augmenting more than 8 patches in each image. We suspect this is because only a few patches are prone to over-fitting in each image, as sug- gested in Fig. 3. Augmenting too many patches may ruin the attention-score map instead, leading to reduced accuracy. 4.4. Visualization of Attention Score Maps To verify Hint-Aug’s effectiveness in alleviating the over-fitting issue, we visualize the attention score maps of the pretrained FViT, FViT tuned by NPS [72], and FViT tuned with our proposed Hint-Aug. As shown in Fig. 5, we can observe that (1) after tuning with our proposed Hint-Aug, the over-fitted patches (marked in red) that are commonly observed in the attention score maps tuned by NPS [72] are successfully eliminated, and (2) the attention score map obtained from Hint-Aug features similar loca- tions of high-attention score patches to those obtained from the pretrained FViT, indicating that Hint-Aug effectively al- leviates the over-fitting issue. 4.5. Visualization of the Confusion Matrix Table 5. The averaged confu- sion matrix value of the Cats and Dogs meta-group. Cats Dogs Cats 4.94 3.96 Dogs 3.96 5.72 We visualize the confu- sion matrix using a 4-shot Adapter [32] tuning setting on Pets [48] to interpret the discovered class-wise sim- ilarity. We calculate the averaged confusion matrix value of the Cats and Dogs meta-group and visualize them in Tab. 5. We observe that the FViT is much more confused in distinguishing be- tween different classes within the Cat or Dog meta-group than distinguishing between the Cat and Dog meta-groups. This suggests that despite the simplicity of our strategy that uses the pre-softmax output, the generated confusion matrix can effectively identify the class pairs with easy-to-confuse features and thus provide correct guidance for CFI. 5. Conclusion In this paper, we propose a framework called Hint-Aug, which is dedicated to boosting the few-shot parameter- efficient tuning accuracy of FViTs. Specifically, Hint- Aug features two enablers called AOD and CFI, aiming to alleviate the over-fitting issue and the lack of diverse data in few-shot tuning, respectively. Extensive experi- ments and ablation studies validate that Hint-Aug achieves a 0.04% ∼ 32.91% higher accuracy over SOTA data augmen- tation methods, opening up a new perspective towards more effectively tuning pretrained FViTs on downstream tasks in a realistic low-data scheme. Acknowledgement The work was supported by the National Science Foun- dation (NSF) through the NSF CCF program (Award num- ber: 2211815) and supported in part by CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.References [1] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340, 2017. 3 [2] Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. Exploring visual prompts for adapting large- scale models. arXiv preprint arXiv:2203.17274, 1(3):4, 2022. 1, 3 [3] Ms Aayushi Bansal, Dr Rewa Sharma, and Dr Mamta Kathuria. A systematic review on data scarcity problem in deep learning: solution and applications. ACM Computing Surveys (CSUR), 54(10s):1–29, 2022. 3 [4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 2 [5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In European conference on computer vision, pages 446–461. Springer, 2014. 6, 7 [6] Shuhao Cao, Peng Xu, and David A Clifton. How to understand masked autoencoders. arXiv preprint arXiv:2202.03670, 2022. 3 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021. 2 [8] Haoxing Chen, Huaxiong Li, Yaohui Li, and Chunlin Chen. Sparse spatial transformers for few-shot learning. arXiv preprint arXiv:2109.12932, 2021. 3 [9] Jie-Neng Chen, Shuyang Sun, Ju He, Philip Torr, Alan Yuille, and Song Bai. Transmix: Attend to mix for vision transformers. arXiv preprint arXiv:2111.09833, 2021. 3 [10] Yuzhong Chen, Zhenxiang Xiao, Lin Zhao, Lu Zhang, Haixing Dai, David Weizhong Liu, Zihao Wu, Changhe Li, Tuo Zhang, Changying Li, et al. Mask-guided vision transformer (mg-vit) for few-shot learning. arXiv preprint arXiv:2205.09995, 2022. 3 [11] Ziyi Cheng, Xuhong Ren, Felix Juefei-Xu, Wanli Xue, Qing Guo, Lei Ma, and Jianjun Zhao. Deepmix: Online auto data augmentation for robust visual object tracking. In2021 IEEE International Conference on Multimedia and Expo (ICME), pages 1–6. IEEE, 2021. 3 [12] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. 1 [13] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 1, 6 [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 6 [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 2 [16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2, 3, 6 [17] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim- ing He. Masked autoencoders as spatiotemporal learners. arXiv preprint arXiv:2205.09113, 2022. 2 [18] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126–1135. PMLR, 2017. 3 [19] Luciano Floridi and Massimo Chiriatti. Gpt-3: Its nature, scope, limits, and consequences. Minds and Machines, 30(4):681–694, 2020. 2 [20] Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk. Adversarial examples are a natural consequence of test error in noise. arXiv preprint arXiv:1901.10513, 2019. 3 [21] Ahmed Frikha, Denis Krompaß, Hans-Georg K ¨opken, and V olker Tresp. Few-shot one-class classification via meta- learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7448–7456, 2021. 3 [22] Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, and Yingyan Lin. Patch-fool: Are vision transformers always robust against adversarial perturbations? arXiv preprint arXiv:2203.08392, 2022. 3, 4, 6, 8 [23] Hang Gao, Zheng Shou, Alireza Zareian, Hanwang Zhang, and Shih-Fu Chang. Low-shot learning via covariance- preserving adversarial augmentation networks. Advances in Neural Information Processing Systems, 31, 2018. 3 [24] Chengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Keepaugment: A simple information- preserving data augmentation approach. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1055–1064, 2021. 3 [25] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 6 [26] Shupeng Gui, Haotao Wang, Haichuan Yang, Chen Yu, Zhangyang Wang, and Ji Liu. Model compression with adversarial robustness: A unified optimization framework. Advances in Neural Information Processing Systems, 32, 2019. 3 [27] Hongyu Guo, Yongyi Mao, and Richong Zhang. Mixup as locally linear out-of-manifold regularization. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 33, pages 3714–3722, 2019. 2 [28] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In European conference on computer vision, pages 124–141. Springer, 2020. 3 [29] Bharath Hariharan and Ross Girshick. Low-shot vi- sual recognition by shrinking and hallucinating features.In Proceedings of the IEEE international conference on computer vision, pages 3018–3027, 2017. 3 [30] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi- otr Doll ´ar, and Ross B Girshick. Masked autoencoders are scalable vision learners. corr abs/2111.06377 (2021). arXiv preprint arXiv:2111.06377, 2021. 2, 3, 4 [31] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781, 2019. 1, 3 [32] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 2790–2799. PMLR, 2019. 3, 4, 6, 7, 8 [33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1, 3, 4, 6 [34] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Vi- sual prompt tuning. arXiv preprint arXiv:2203.12119, 2022. 1, 3, 4, 6, 8 [35] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei- Fei. 3d object representations for fine-grained categoriza- tion. In Proceedings of the IEEE international conference on computer vision workshops, pages 554–561, 2013. 6, 7 [36] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng Gao. Efficient self-supervised vision transformers for representation learn- ing. arXiv preprint arXiv:2106.09785, 2021. 2, 3 [37] Kai Li, Yulun Zhang, Kunpeng Li, and Yun Fu. Adver- sarial feature hallucination networks for few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13470–13479, 2020. 3, 4 [38] Lu Liu, William Hamilton, Guodong Long, Jing Jiang, and Hugo Larochelle. A universal representation trans- former layer for few-shot image classification.arXiv preprint arXiv:2006.11702, 2020. 3 [39] Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learn- ing. arXiv preprint arXiv:1805.10002, 2018. 3 [40] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and reso- lution. arXiv preprint arXiv:2111.09883, 2021. 1, 2 [41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. arXiv preprint arXiv:2103.14030, 2021. 1, 2, 3 [42] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor- dan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97–105. PMLR, 2015. 3 [43] Qinxuan Luo, Lingfeng Wang, Jingguo Lv, Shiming Xiang, and Chunhong Pan. Few-shot learning via feature hallu- cination with variational inference. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3963–3972, 2021. 3 [44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learn- ing models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 2, 3 [45] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classi- fication of aircraft. arXiv preprint arXiv:1306.5151, 2013. 2, 6, 7 [46] Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. Google Research Blog, 2015, 2015. 2, 3 [47] M-E Nilsback and Andrew Zisserman. A visual vocabulary for flower classification. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 2, pages 1447–1454. IEEE, 2006. 7 [48] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498–3505. IEEE, 2012. 6, 7, 8 [49] Yao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshmi- narayanan, Alex Beutel, and Xuezhi Wang. Under- standing and improving robustness of vision transformers through patch-based negative augmentation. arXiv preprint arXiv:2110.07858, 2021. 3 [50] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Pe- ter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. 2 [51] Evgenia Rusak, Lukas Schott, Roland S Zimmermann, Ju- lian Bitterwolf, Oliver Bringmann, Matthias Bethge, and Wieland Brendel. A simple way to make neural networks robust against diverse image corruptions. In European Conference on Computer Vision, pages 53–69. Springer, 2020. 3 [52] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. 4, 7 [53] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Lad- der side-tuning for parameter and memory efficient transfer learning. arXiv preprint arXiv:2206.06522, 2022. 3 [54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training data-efficient image transformers & distillation through at- tention. In International Conference on Machine Learning, pages 10347–10357. PMLR, 2021. 1, 2, 7 [55] Hugo Touvron, Matthieu Cord, and Herv ´e J ´egou. Deit iii: Revenge of the vit. arXiv preprint arXiv:2204.07118, 2022. 1 [56] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisser- man. Open-set recognition: A good closed-set classifier is all you need. arXiv preprint arXiv:2110.06207, 2021. 5[57] Shashanka Venkataramanan, Yannis Avrithis, Ewa Kijak, and Laurent Amsaleg. Alignmix: Improving represen- tation by interpolating aligned features. arXiv preprint arXiv:2103.15375, 2021. 3 [58] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learn- ing. Advances in neural information processing systems, 29, 2016. 3 [59] Riccardo V olpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generaliz- ing to unseen domains via adversarial data augmentation. Advances in neural information processing systems, 31, 2018. 3 [60] Haotao Wang, Chaowei Xiao, Jean Kossaifi, Zhiding Yu, An- ima Anandkumar, and Zhangyang Wang. Augmax: Adver- sarial composition of random augmentations for robust train- ing. Advances in Neural Information Processing Systems, 34, 2021. 3 [61] Xixi Wang, Xiao Wang, Bo Jiang, and Bin Luo. Few- shot learning meets transformer: Unified query-support transformers for few-shot classification. arXiv preprint arXiv:2208.12398, 2022. 3 [62] Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples: A survey on few-shot learning. ACM computing surveys (csur), 53(3):1–34, 2020. 3, 4 [63] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7278–7286, 2018. 1, 3 [64] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22–31, 2021. 1 [65] Chengming Xu, Siqian Yang, Yabiao Wang, Zhanxiong Wang, Yanwei Fu, and Xiangyang Xue. Exploring efficient few-shot adaptation for vision transformers. arXiv preprint arXiv:2301.02419, 2023. 3 [66] Zhongzhi Yu, Yonggan Fu, Sicheng Li, Chaojian Li, and Yingyan Lin. Mia-former: Efficient and robust vision trans- formers via multi-grained input-adaptation. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 36, pages 8962–8970, 2022. 4 [67] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. 1, 2 [68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Reg- ularization strategy to train strong classifiers with localiz- able features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. 1, 2, 3 [69] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu- cas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104–12113, 2022. 2 [70] Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong. You only propagate once: Accelerat- ing adversarial training via maximal principle. Advances in Neural Information Processing Systems, 32, 2019. 3 [71] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 1, 2, 3 [72] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. 1, 2, 3, 6, 8 [73] Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data augmentation for im- proved generalization and robustness. Advances in Neural Information Processing Systems, 33:14435–14447, 2020. 3 [74] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. InProceedings of the AAAI Conference on Artificial Intelligence, vol- ume 34, pages 13001–13008, 2020. 1, 3 [75] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi- aochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021. 2",
      "meta_data": {
        "arxiv_id": "2304.12520v3",
        "authors": [
          "Zhongzhi Yu",
          "Shang Wu",
          "Yonggan Fu",
          "Shunyao Zhang",
          "Yingyan Lin"
        ],
        "published_date": "2023-04-25T02:22:01Z",
        "pdf_url": "https://arxiv.org/pdf/2304.12520v3.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of effectively tuning Foundation Vision Transformers (FViTs) in few-shot scenarios due to their data-hungry nature and the limitations of existing random-based data augmentation techniques. It proposes Hint-based Data Augmentation (Hint-Aug), a framework that leverages learned features of pretrained FViTs to guide data augmentation. Hint-Aug introduces two key enablers: (1) an Attentive Over-fitting Detector (AOD) that uses attention maps to identify over-fitting patches, and (2) a Confusion-based Feature Infusion (CFI) module that adaptively infuses 'easy-to-confuse' features from pretrained FViTs into these detected patches to enhance feature diversity. Extensive experiments demonstrate Hint-Aug's effectiveness, achieving 0.04% to 32.91% higher accuracy over state-of-the-art data augmentation methods in various low-shot settings.",
        "methodology": "Hint-Aug employs a two-stage 'detect-then-augment' pipeline. The Attentive Over-fitting Detector (AOD) quantifies the impact of each input image patch using attention-score maps, defined as the sum of attention distributions across heads and layers. It detects potential over-fitting by calculating the difference between attention-score maps from the pretrained FViT and the tuned FViT, using a hyperparameter \"lambda\" for sensitivity. If over-fitting is detected, the patch with the largest attention-score difference is selected; otherwise, the patch with the highest attention-score is chosen. The Confusion-based Feature Infusion (CFI) module then uses adversarial attack-based methods to infuse meaningful features into the selected patch. CFI constructs a confusion matrix from the model's pre-softmax outputs to identify class pairs that are easily confused. It designs an adversarial attack target label based on this confusion matrix and optimizes the patch perturbation using a Cross-Entropy loss to shift features towards these easy-to-confuse classes, thereby extending decision boundaries and increasing data diversity.",
        "experimental_setup": "The experiments were conducted on five commonly used few-shot image classification datasets: Food, Pet, Cars, Flowers, and Aircraft, under 1/2/4/8/12/16-shot scenarios. The Vision Transformer Base (ViT-Base) model was used as the Foundation ViT. Three widely used parameter-efficient tuning methods were employed: Adapter, LoRA, and Visual Prompt Tuning (VPT). Baselines included the state-of-the-art data augmentation technique for parameter-efficient FViT tuning (NPS) and vanilla tuning without augmentation (No-Aug). The tuning settings involved 100 epochs, a batch size of 256, a learning rate of 0.01, and an SGD optimizer, starting from an ImageNet-pretrained ViT-Base. Additional data augmentations (color-jitter, RandAugment) were applied for baselines following NPS. Hint-Aug's specific hyperparameters included an AOD sensitivity control parameter lambda of 0.1, and the Fast Gradient Sign Method (FGSM) for adversarial attacks with an attack radius epsilon of 0.001. All experiments were run three times, and the average accuracy was reported.",
        "limitations": "The paper reveals an implicit limitation: augmenting too many patches (e.g., more than 8 or all patches) can lead to reduced accuracy, suggesting that the precise and selective augmentation guided by the Attentive Over-fitting Detector (AOD) is critical, and non-selective augmentation can be detrimental. While the framework demonstrates robustness to variations in the adversarial attack radius (epsilon), the sensitivity to other hyperparameters within AOD and Confusion-based Feature Infusion (CFI) is not extensively detailed. The approach relies on fixed pretrained FViT weights, and its generalizability might be constrained if these fixed features are not universally optimal for all diverse few-shot tasks, or if scenarios involving partial fine-tuning of FViT weights are considered.",
        "future_research_directions": "Not mentioned"
      }
    },
    {
      "title": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering",
      "abstract": "Recently, finetuning pretrained Vision-Language Models (VLMs) has been a\nprevailing paradigm for achieving state-of-the-art performance in Visual\nQuestion Answering (VQA). However, as VLMs scale, finetuning full model\nparameters for a given task in low-resource settings becomes computationally\nexpensive, storage inefficient, and prone to overfitting. Current\nparameter-efficient tuning methods dramatically reduce the number of tunable\nparameters, but there still exists a significant performance gap with full\nfinetuning. In this paper, we propose MixPHM, a redundancy-aware\nparameter-efficient tuning method that outperforms full finetuning in\nlow-resource VQA. Specifically, MixPHM is a lightweight module implemented by\nmultiple PHM-experts in a mixture-of-experts manner. To reduce parameter\nredundancy, MixPHM reparameterizes expert weights in a low-rank subspace and\nshares part of the weights inside and across experts. Moreover, based on a\nquantitative redundancy analysis for adapters, we propose Redundancy\nRegularization to reduce task-irrelevant redundancy while promoting\ntask-relevant correlation in MixPHM representations. Experiments conducted on\nVQA v2, GQA, and OK-VQA demonstrate that MixPHM outperforms state-of-the-art\nparameter-efficient methods and is the only one consistently surpassing full\nfinetuning.",
      "full_text": "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering Jingjing Jiang Nanning Zheng * Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University jingjingjiang2017@gmail.com nnzheng@mail.xjtu.edu.cn Abstract Recently, finetuning pretrained Vision-Language Mod- els (VLMs) has been a prevailing paradigm for achieving state-of-the-art performance in Visual Question Answering (VQA). However, as VLMs scale, finetuning full model pa- rameters for a given task in low-resource settings becomes computationally expensive, storage inefficient, and prone to overfitting. Current parameter-efficient tuning methods dramatically reduce the number of tunable parameters, but there still exists a significant performance gap with full fine- tuning. In this paper, we propose MixPHM, a redundancy- aware parameter-efficient tuning method that outperforms full finetuning in low-resource VQA. Specifically, MixPHM is a lightweight module implemented by multiple PHM- experts in a mixture-of-experts manner. To reduce param- eter redundancy, MixPHM reparameterizes expert weights in a low-rank subspace and shares part of the weights in- side and across experts. Moreover, based on a quantitative redundancy analysis for adapters, we proposeRedundancy Regularization to reduce task-irrelevant redundancy while promoting task-relevant correlation in MixPHM represen- tations. Experiments conducted on VQA v2, GQA, and OK- VQA demonstrate that MixPHM outperforms state-of-the- art parameter-efficient methods and is the only one consis- tently surpassing full finetuning. 1. Introduction Adapting pretrained vision-language models (VLMs) [5, 6,25,30,31,53,60] to the downstream VQA task [2] through finetuning has emerged as a dominant paradigm for achiev- ing state-of-the-art performance. However, as the scale of VLMs continues to expand, finetuning an entire pretrained model that consists of millions or billions of parameters engenders a substantial increase in computational and stor- age costs, and exposes the risk of overfitting (poor perfor- mance) in low-resource learning. Parameter-efficient tun- *Corresponding author. /uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000015/uni00000011/uni00000018 /uni00000033/uni00000048/uni00000055/uni00000046/uni00000048/uni00000051/uni00000057/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000058/uni00000051/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000017/uni00000017/uni00000011/uni00000013 /uni00000017/uni00000017/uni00000011/uni00000018 /uni00000017/uni00000018/uni00000011/uni00000013 /uni00000017/uni00000018/uni00000011/uni00000018 /uni00000017/uni00000019/uni00000011/uni00000013 /uni00000017/uni00000019/uni00000011/uni00000018 /uni00000017/uni0000001a/uni00000011/uni00000013 /uni00000017/uni0000001a/uni00000011/uni00000018 /uni00000017/uni0000001b/uni00000011/uni00000013 /uni00000017/uni0000001b/uni00000011/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000029/uni00000058/uni0000004f/uni0000004f/uni00000003/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000017/uni00000019/uni00000011/uni0000001b/uni0000001a /uni00000030/uni0000004c/uni0000005b/uni00000033/uni0000002b/uni00000030/uni00000003/uni00000017/uni0000001b/uni00000011/uni00000015/uni00000019 /uni00000024/uni00000047/uni00000044/uni00000030/uni0000004c/uni0000005b/uni00000003/uni00000017/uni00000019/uni00000011/uni0000001a/uni00000013/uni00000033/uni00000049/uni00000048/uni0000004c/uni00000049/uni00000049/uni00000048/uni00000055/uni00000003/uni00000017/uni00000018/uni00000011/uni0000001c/uni00000016 /uni0000002b/uni00000052/uni00000058/uni0000004f/uni00000056/uni00000045/uni0000005c/uni00000003/uni00000017/uni00000018/uni00000011/uni00000014/uni00000014/uni00000026/uni00000052/uni00000050/uni00000053/uni00000044/uni00000046/uni00000057/uni00000048/uni00000055/uni00000003/uni00000017/uni00000017/uni00000011/uni0000001c/uni00000014 /uni0000002f/uni00000052/uni00000035/uni00000024/uni00000003/uni00000017/uni00000018/uni00000011/uni00000016/uni00000019 /uni00000025/uni0000004c/uni00000057/uni00000029/uni0000004c/uni00000057/uni00000003/uni00000017/uni00000019/uni00000011/uni00000014/uni00000017 Figure 1. Comparison between parameter-efficient methods. In a low-resource setting (i.e., with 64 training samples), we show the average score across five seeds on VQA v2 (y-axis) and the percentage of tunable parameters w.r.t. pretrained VL-T5 (x-axis). ing methods, only updating newly-added lightweight mod- ules (e.g., Houlsby [16], Pfeiffer [41], Compacter [24], and AdaMix [54]) or updating a tiny number of parameters in pretrained models ( e.g., BitFit [59] and LoRA [17]), are thus proposed to handle these challenges. However, as illustrated in Figure 1, the aforementioned parameter-efficient tuning methods substantially reduce the number of tunable parameters, but their performance still lags behind full finetuning. Among these methods, adapter- based approaches [16,24,41,54] allow for more flexible pa- rameter sharing [46] and are more storage-efficient by stor- ing only a copy of these adapters instead of the entire pre- trained model. In particular, AdaMix [54] employs adapters as experts within a mixture-of-experts (MoE) [45] archi- tecture to enhance adapter capacity and achieves compara- ble performance to full finetuning. Nevertheless, this MoE mode leads to an increase in the number of tunable param- eters due to parameter redundancy between adapters. In this paper, we build upon adapter-based approaches to explore more parameter-efficient tuning methods that can outperform full finetuning on low-resource VQA. Specifi- cally, when adapting pretrained VLMs to a given task, we focus on two key improvements: (i) Reducing parameter redundancy while maintaining adapter capacity. Striking a 1 arXiv:2303.01239v2  [cs.CV]  7 Jun 2023balance between parameter efficiency and model capacity is crucial since an excessive reduction of tunable parameters may lead to underfitting, limiting the ability of adapters to capture sufficient task-relevant information [24]. (ii) Re- ducing task-irrelevant redundancy while promoting task- relevant correlation in representations. In practice, adapters leverage residual connections to integrate task-specific in- formation learned from a target dataset and prior knowl- edge already implied in pretrained VLMs. However, recent works [22, 35, 51] have highlighted that pretrained models inevitably contain redundant and irrelevant information for target tasks, resulting in statistically spurious correlations between representations and labels, thereby hindering per- formance and generalization [49, 52]. To enhance the ef- fectiveness of adapters, our objective thus is to maximize the acquisition of task-relevant information while discard- ing task-irrelevant information from pretrained VLMs. To this end, we propose MixPHM, a redundancy-aware parameter-efficient tuning method that can efficiently de- crease tunable parameters and reduce task-irrelevant redun- dancy while promoting task-relevant correlation in repre- sentations. MixPHM is implemented with multiple PHM- experts in a MoE manner. To reduce (i) parameter redun- dancy, MixPHM first decomposes and reparameterizes ex- pert weights into a low-rank subspace. Then, it further re- duces tunable parameters and facilitates information trans- fer via global and local weight sharing. To achieve the im- provement (ii), we first quantify the redundancy for adapters in representation spaces. The result reveals that adapter rep- resentations are redundant with pretrained VLM represen- tations, but exhibit limited correlation to the final task-used representations. Motivated by this finding, we propose Re- dundancy Regularization, which is incorporated into Mix- PHM and serves to reduce task-irrelevant redundancy by decorrelating the similarity matrix between MixPHM repre- sentations and pretrained VLM representations. Simultane- ously, it promotes task-relevant correlation by maximizing the mutual information between MixPHM representations and the final task-used representations. We conduct experiments on VQA v2 [12], GQA [20], and OK-VQA [38], and demonstrate that the proposed Mix- PHM consistently outperforms full finetuning and state-of- the-art parameter-efficient tuning methods when adapting pretrained VLMs to low-resource VQA. In summary, our contributions are as follows: (1) We propose MixPHM, a redundancy-aware parameter-efficient tuning method for adapting pretrained VLMs to downstream tasks. (2) We propose Redundancy Regularization, a novel regularizer to reduce task-irrelevant redundancy while promoting task- relevant correlation in MixPHM representations. (3) Exten- sive experiments show MixPHM achieves a better trade-off between performance and parameter efficiency, leading to a significant performance gain over existing methods. 2. Related Work Vision-Langauge Pretraining. Vision-language pretrain- ing [6,9,19,21,25,31,48,63,65] aims to learn task-agnostic multimodal representations for improving the performance of downstream tasks in a finetuning fashion. Recently, a line of research [5, 18, 30, 31, 53] has been devoted to lever- aging encoder-decoder frameworks and generative model- ing objectives to unify architectures and objectives between pretraining and finetuning. VLMs with an encoder-decoder architecture generalize better. In this paper, we explore how to better adapt them to low-resource VQA [2]. Parameter-Efficient Tuning. Finetuning large-scale pre- trained VLMs on given downstream datasets has become a mainstream paradigm for vision-language tasks. However, finetuning the full model consisting of millions of parame- ters is time-consuming and resource-intensive. Parameter- efficient tuning [13, 36, 37, 44, 58, 59, 62] vicariously tunes lightweight trainable parameters while keeping (most) pre- trained parameters frozen, which has shown great success in NLP tasks. According to whether new trainable parameters are introduced, these methods can be roughly categorized into two groups: (1) tuning partial parameters of pretrained models, such as BitFit [59] and FISH Mask [47], (2) tuning additional parameters, such as prompt (prefix)-tuning [28, 32], adapter [16, 41], and low-rank methods [17, 24]. Motivated by the success in NLP, some works [33,46,64] have begun to introduce parameter-efficient methods to tune pretrained VLMs for vision-language tasks. Specifically, Lin et al. [33] investigate action-level prompts for vision- language navigation. VL-Adapter [46] extends adapters to transfer VLMs for various vision-language tasks. Hy- perPELT [64] is a unified parameter-efficient framework for vision-language tasks, incorporating adapter and prefix- tuning. In addition, Frozen [50] and PICa [57] use prompt- tuning techniques [28] to transfer the few-shot learning ability of large-scale pretrained language models to han- dle few-shot vision-language tasks. FewVLM [23] designs hand-crafted prompts to finetune pretrained VLMs for low- resource adaptation. In contrast, low-rank methods are more parameter-efficient but are rarely explored. Mixture-of-Experts. MoE [45] aims to scale up model ca- pacities and keep computational efficiency with conditional computation. Most recent works [7, 8, 27, 29, 42, 43] in- vestigate how to construct large-scale vision or language transformer models using MoE and well-designed rout- ing mechanisms in the pretraining stage. Despite its suc- cess in pretraining, MoE has not been widely explored in parameter-efficient tuning. MPOE [11] and AdaMix [54] are two recent works that tune pretrained language models by MoE. Specifically, MPOE considers additional FFN lay- ers as experts and decomposes weight matrices of experts with MPO. AdaMix treats the added adapters as experts and increases adapter capacity by a stochastic routing strategy. 2Self-Attention Feed Forward Add & Norm Add & Norm Cross-Attention Add & Norm Feed Forward Add & Norm Add & Norm Training + Inference + + Self-Attention (d) MixPHM Updated Parameters Frozen Parameters (a) Transformer block of VLMs   (b) Adapter (c) Weight decomposition of PHM-expert  + + Figure 2. Illustration of MixPHM inserted into (a) one transformer block of VLMs. (b) The structure of standard adapter. (c) An example of the weight matrix decomposition in Eq. (13) for a PHM-expert (here n = 2, d = 10, dr = 8, dk = 2). (d) MixPHM architecture with Ne = 3PHM-experts. During training, MixPHM randomly activates one PHM-expert to learn robust representations and exploits the proposed redundancy regularization LRa to reduce task-irrelevant redundancy while promoting task-relevant correlation. 3. Preliminary Problem Definition. We follow recent work [5, 55] to for- mulate VQA as a generative modeling task, i.e., generating free-form textual answers for a given question instead of selecting a specific one from the predefined set of answers. Formally, we denote a VQA dataset with D = {(I, Q, y) ∈ I × Q × Y}, where I is an image, Q is a question, and y is an answer. Assuming that a given pretrained VLMs MΘ is parameterized by a tiny number of tunable parameters θ, the general problem of adapting pretrained VLMs for VQA is to tune Mθ in a parameter-efficient manner on D. Mixture-of-Experts. A standard MoE [45] is implemented with a set of Ne experts {Ei}Ne i=1 and a gating network G. Each expert is a sub-neural network with unique weights and can learn from a task-specific subset of inputs. The gate conditionally activates Na (1 ≤ Na ≤ Ne) experts. Formally, given an input representation x ∈ Rd, the i-th expert maps x into de-dimensional space, i.e., Ei(·) : x → Rde , the gate generates a sparseNe-dimensional vector,i.e., G(·) : x → RNe . Then, the output y ∈ Rde of the MoE can be formulated as y = NeX i=1 G(x)iEi(x), (1) where, G(x)i denotes the probability of assigning x to the i-th expert, satisfying PNe i=1 G(x)i = 1. Parameterized Hypercomplex Multiplication. The PHM layer [61] aims to generalize hypercomplex multiplications to fully-connected layer by learning multiplication rules from data. Formally, for a fully-connected layer that trans- forms an input x ∈ Rd to an output y ∈ Rde , i.e., y = WTx + b, (2) where, W ∈ Rd×de . In PHM, the weight matrix W is learned via the summation of n Kronecker products be- tween Sj ∈ Rn×n and Aj ∈ R d n ×de n : W = nX j=1 Sj ⊗ Aj, (3) where, the hyperparameter n ∈ Z>0 controls the number of the above summations, d and de are divisible by n, and ⊗ indicates the Kronecker product that generalizes the vec- tor outer products to higher dimensions in real space. For example, the Kronecker product between S ∈ Rm×k and A ∈ Rp×q is a block matrix S ⊗ A ∈ Rmp×kq, i.e., S ⊗ A =   s11A ··· s1kA ... ... ... sm1A ··· smkA  , (4) where, sij denotes the element of matrix S at the i-th row and j-th column. As a result, replacing a fully-connected layer with PHM can reduce the trainable parameters by at most 1/n of the fully-connected layer. 4. Methodology We propose MixPHM, a redundancy-aware parameter- efficient tuning method to adapt pretrained VLMs. This sec- tion first analyzes the redundancy in the adapter representa- tion space toward low-resource VQA (Sec. 4.1). Then, we sequentially detail architecture (Sec. 4.2), redundancy reg- ularization (Sec. 4.3), and inference (Sec. 4.4) of MixPHM. 34.1. Rethinking Redundancy in Adapter As shown in Figure 2 (b), adapter [16] is essentially a lightweight module, usually implemented by a two-layer feed-forward network with a bottleneck, a nonlinear func- tion, and a residual connection. When finetuning pretrained VLMs on downstream tasks, adapters are inserted between the transformer layers of VLMs, and only the parameters of the newly-added adapters are updated, while the original parameters in pretrained VLMs remain frozen. Formally, given an input representation h ∈ Rd, the down-projection layer Wdn ∈ Rd×dr maps h to a lower-dimensional space specified by the bottleneck dimension dr, i.e., hr ∈ Rdr . The up-projection layer Wup ∈ Rdr×d maps hr back to the input size, i.e., ha ∈ Rd. Considering the residual and nonlinear function f, an adapter is defined as ha = f(hWdn)Wup, (5) h ← ha + h. (6) Ideally, by incorporating task-specific information learned from a given downstream dataset ( ha) and prior knowl- edge already encoded in pretrained VLMs ( h), adapters can quickly transfer pretrained VLMs to new tasks without over-parameterization or under-parameterization. Redundancy Analysis for Adapter. However, recent in- vestigation has shown that some of the information captured by adapters is task-agnostic [14]. To get the facts, we lever- age Representational Similarity Analysis (RSA) [26] to as- sess the redundancy in representation spaces. Specifically, we first tune the pretrained VL-T5 [5] with Pfeiffer [41] on 1k samples from VQA v2 training set [12]. Then, we ran- domly sample 1k samples from VQA v2 val set and extract token-level representations ( i.e., h and ha) at each trans- former layer as well as the final output representation ˜h of transformer encoder/decoder. Finally, for each sample, we can obtain Nt token-level representations at each layer,i.e., H = {h}Nt i=1 ∈ RNt×d, Ha = {ha}Nt i=1 ∈ RNt×d and ˜H = {˜h}Nt i=1 ∈ RNt×d. In each layer, we compute RSA similarity between ha and h as well as ha and ˜h by RSA(ha, h) = fρ(fU[HaHT a ], fU[HHT]), (7) RSA(ha, ˜h) = fρ(fU[HaHT a ], fU[ ˜H˜HT]), (8) where, fU [·] denotes an operation of taking the upper trian- gular elements from a matrix, fρ is a function to compute the Pearson correlation coefficient. Figure 3 illustrates the average RSA similarity across 1k samples, which demon- strates that in transformer layers, the adapter representa- tion ha is redundant with the representation h of pretrained VLMs, but has limited correlation to the final output ˜h. Intuitively, to transfer pretrained VLMs to downstream tasks efficiently, the representation ha learned by adapter needs to contain as much information as possible from Figure 3. The average RSA similarity across 1k samples be- tween ha and h (left) as well as ha and ˜h (right) at each trans- former layer. The higher the RSA, the more similar (redundant) the representation spaces are. the task-relevant representation ˜h, while reducing task- irrelevant redundancy with the representation h of pre- trained VLMs. However, Figure 3 exhibits a counterintu- itive result. Therefore, in order to improve the effectiveness of adapters, it is crucial to encourage task-relevant corre- lation between ha and ˜h while reducing task-irrelevant re- dundancy between ha and h. 4.2. MixPHM Architecture As illustrated in Figure 2, MixPHM is also a lightweight module inserted into each transformer block of VLMs. We utilize transformer-based encoder-decoder models as un- derlying pretrained VLMs, which consists of repeated L encoder and L decoder blocks. Specifically, for the l-th (1 ≤ l ≤ L) block, we insert a MixPHM composed of a set of Ne PHM-experts {El i}Ne i=1 after the feed-forward layer to capture the knowledge and learn task-specific information. As with the adapter, each PHM-expert is implemented by a bottleneck network with down- and up-projection layers. To reduce parameter redundancy in MixPHM, we first decompose and reparameterize the projection matrices of experts in MixPHM into low-dimensional subspace. Then, we further reduce the number of parameters and transfer in- formation using a strategy of global and local expert weight sharing. Moreover, a stochastic routing [54,66] is employed for expert selection to avoid gating networks from introduc- ing additional trainable parameters. Parameter-Efficient Tuning. At each training (tuning) it- eration, we randomly select one expert from the insertedNe PHM-experts in the l-th transformer block. Once the expert El i is selected, all inputs in a given batch are processed by the same expert. Formally, in the l-th block1, for a token input representation h ∈ Rd, the randomly selected i-th ex- pert encodes and updates h by h ← f(hWdn i )Wup i + h. (9) In Eq. (9), the down-projection matrix Wdn i ∈ Rd×dr and up-projection matrix Wup i ∈ Rdr×d are firstly decomposed 1For brevity, the superscript l is omitted hereafter. 4into low-dimensional matrices using PHM, i.e., Wdn i = nX j=1 Si,j ⊗ Adn i,j, Wup i = nX j=1 Si,j ⊗ Aup i,j, (10) where, Si,j ∈ Rn×n, Adn i,j ∈ R d n ×dr n , Aup i,j ∈ R dr n × d n . To be more parameter-efficient, the matrix Adn i,j (Aup i,j) is further factorized into two low-rank matrices by Adn i,j = Tdn i,j(Udn i,j)T, Aup i,j = Tup i,j(Uup i,j)T, (11) where, Tdn i,j ∈ R d n ×dk , Udn i,j ∈ R dr n ×dk , Tup i,j ∈ R dr n ×dk , Uup i,j ∈ R d n ×dk , and dr is the rank of these matrices. Finally, we learn the weight matrices of the i-th PHM-expert by Wdn i = nX j=1 Si,j ⊗ (Tdn i,j(Udn i,j)T), (12) Wup i = nX j=1 Si,j ⊗ (Tup i,j(Uup i,j)T). (13) Information Sharing across PHM-Experts. When tuning pretrained VLMs with MixPHM on a downstream dataset, the set of n matrices {Si,j ∈ Rn×n}n j=1 of the i-th PHM- expert are globally shared among all PHM-experts across transformer blocks to capture general information for the target task. In other words, the S = {Sj}n j=1 ∈ Rn×n×n of one expert is shared between all added MixPHMs. On the contrary, {Adn i,j}n j=1 and {Aup i,j}n j=1 are expert-specific weight matrices which are unique to each PHM-expert. To better transfer information between PHM-experts of Mix- PHM and further reduce parameter redundancy, we locally share {Adn i,j}n j=1 among PHM-experts in each MixPHM. At this point, the total number of trainable parameters inserted into pretrained VLMs using MixPHM is reduced from the original4LNe(ddr) to 2Ldk(d+dr)(Ne+1)+ n3. 4.3. Redundancy Regularization Motivated by the insight discussed in Sec. 4.1, we pro- pose redundancy regularization. Specifically, for the Mix- PHM in the l-th transformer block, we ensemble its token- level output representation{ha}N i=1 and its residual {h}N i=1 of a batch to Za ∈ RN×d and Z ∈ RN×d, N = NbNt (Nb indicates batch size). For the transformer encoder/decoder, we average the final output representation{˜h}N i=1 of a batch along the token dimension and obtain a global task-relevant representation {¯h}Nb i=1. Then, the redundancy regularization can be expressed by LRa ≜ NX i NX j̸=i ZaZT ∥Za∥2 ∥Z∥2 − NbX i NtX j ˆI(hai,j; ¯hi), (14) where, ∥·∥2 denotes the L2 norm, hai,j is the output rep- resentation of the j-th token of the i-th sample in a batch, and ˆI(·; ·) means an estimation of mutual information (MI), which is used to maximize the correction between two rep- resentations and computed by the JSD MI estimator [15]. In redundancy regularization LRa, the first term is a redun- dancy reduction term, which encouragesha to discard task- irrelevant information from pretrained VLMs by encourag- ing the off-diagonal elements of the cosine similarity matrix between ha and h to zero. The second term aims to advo- cate ha contain more task-relevant information from down- stream datasets by maximizing the MI between ha and ¯h. Formulating VQA as a generative task, the training ob- jective is to minimize the negative log-likelihood of answer y tokens given input image I and question Q. Therefore, the total training loss in parameter-efficient tuning is L = − |y|X j=1 log Pθ(yj|y<j; I, Q) + αLRa, (15) where, α is a factor to balance redundancy regularization. 4.4. Inference In contrast to the stochastic routing utilized during train- ing, we adopt a weight aggregation strategy [56] to obtain a final PHM-expert for each MixPHM during inference. Specifically, one MixPHM has Ne PHM-experts. When learning weights in a low-rank subspace, each expert has2n expert-specific matrices {Tup j , Uup j }n j=1, and the Ne experts have the same 2n locally-shared matrices {Tdn j , Udn j }n j=1 as well as n globally-shared matrices {Sj}n j=1. To obtain weights of the final PHM-expert, we first merge the weights of up-projection matrices by averaging the corresponding Ne weight matrices. Mathematically, the j-th up-projection matrices can be computed with eTup j = 1 Ne NeX i=1 Tup ji , eUup j = 1 Ne NeX i=1 Uup ji . (16) Due to the global and local weight sharing, we need not perform weight aggregation on S and {Tdn j , Udn j }n j=1. Fi- nally, we employ the merged expert to compute the output representations of MixPHM at each transformer block. 5. Experiment 5.1. Experimental Setting Datasets and Metrics. We conduct experiments on three datasets, VQA v2 [12], GQA [20], and OK-VQA [38]. To simulate the low-resource setting for VQA, we follow the work [4] and consider the training data size ND for low-resource VQA to be smaller than 1,000. For more practical low-resource learning, we follow true few-shot learning [10, 40] and utilize the development set Ddev, which has the same size with the training set Dtrain (i.e., 5Dataset Method #Param #Sample (M) (%) ND=16 ND=32 ND=64 ND=100 ND=500 ND=1,000 Finetuning 224.54 100% 41.82 ±1.58 43.09 ±3.10 46.87 ±0.57 48.12 ±0.87 53.46 ±0.41 55.56 ±0.13 BitFit [59] 0.29 0.13% 40.61 ±4.15 43.86 ±2.19 46.14 ±1.00 47.53 ±0.67 51.91 ±0.40 53.18 ±0.58 LoRA [17] 0.44 0.20% 41.60 ±2.27 42.62 ±2.41 45.36 ±1.66 47.57 ±0.91 51.93 ±0.38 54.15 ±0.45 Compacter [24] 0.34 0.15% 39.28 ±1.87 42.47 ±2.76 44.91 ±1.27 46.28 ±1.37 51.21 ±0.90 53.39 ±0.54 Houlsby [16] 4.76 2.12% 41.71 ±2.16 44.01 ±2.09 45.11 ±1.40 47.71 ±0.78 52.27 ±1.05 54.31 ±0.34 Pfeiffer [41] 2.38 1.06% 41.48 ±1.86 44.18 ±2.13 45.93 ±1.11 47.42 ±1.15 52.35 ±0.52 53.98 ±0.38 AdaMix [54] 5.92 2.64% 40.59 ±2.05 43.42 ±2.08 46.70 ±1.32 47.34 ±0.91 51.72 ±1.05 54.12 ±0.63 VQA v2 [12] MixPHM 0.87 0.39% 43.13 ±1.78 45.97 ±2.01 48.26 ±0.56 49.91 ±0.76 54.30 ±0.33 56.11 ±0.40 Finetuning 224.54 100% 28.24 ±2.08 30.80 ±2.49 34.22 ±0.59 36.15 ±0.99 41.49 ±0.54 43.04 ±0.57 BitFit [59] 0.29 0.13% 26.13 ±2.83 29.00 ±4.81 34.25 ±1.16 35.91 ±1.22 40.08 ±0.42 41.84 ±0.15 LoRA [17] 0.44 0.20% 26.89 ±2.74 30.40 ±2.27 34.40 ±0.99 36.14 ±1.10 40.20 ±1.02 42.06 ±1.12 Compacter [24] 0.34 0.15% 23.70 ±2.10 27.18 ±2.61 32.70 ±1.30 35.28 ±1.45 38.68 ±0.50 41.17 ±0.95 Houlsby [16] 4.76 2.12% 25.13 ±2.32 28.34 ±1.17 33.23 ±0.94 35.88 ±1.79 40.85 ±0.48 41.90 ±0.72 Pfeiffer [41] 2.38 1.06% 25.08 ±1.81 29.18 ±1.32 32.97 ±0.84 35.08 ±1.01 40.30 ±0.40 41.39 ±0.27 AdaMix [54] 5.92 2.64% 24.62 ±2.34 28.01 ±1.33 32.74 ±0.96 35.64 ±0.94 40.14 ±0.42 41.97 ±0.86 GQA [20] MixPHM 0.87 0.39% 28.33 ±2.63 32.40 ±2.52 36.75 ±0.55 37.40 ±0.87 41.92 ±0.55 43.81 ±0.50 Finetuning 224.54 100% 11.66 ±2.08 14.20 ±0.78 16.65 ±1.02 18.28 ±0.67 24.07 ±0.40 26.66 ±0.72 BitFit [59] 0.29 0.13% 11.29 ±1.79 13.66 ±1.49 15.29 ±0.57 16.51 ±0.53 22.54 ±0.57 24.80 ±0.63 LoRA [17] 0.44 0.20% 10.26 ±1.53 12.46 ±1.82 15.95 ±0.38 17.03 ±0.82 23.02 ±0.41 25.26 ±0.53 Compacter [24] 0.34 0.15% 9.64 ±2.73 11.04 ±1.39 13.57 ±1.07 15.92 ±1.18 22.20 ±0.89 24.52 ±0.59 Houlsby [16] 4.76 2.12% 9.79 ±1.71 12.25 ±2.13 15.04 ±1.25 16.58 ±0.65 22.67 ±0.77 25.04 ±0.44 Pfeiffer [41] 2.38 1.06% 9.06 ±0.53 11.39 ±0.79 14.23 ±1.54 16.34 ±0.79 22.90 ±1.03 26.70 ±0.71 AdaMix [54] 5.92 2.64% 8.39 ±1.20 11.55 ±1.37 13.66 ±2.29 16.27 ±0.92 23.20 ±0.78 26.34 ±0.88 OK-VQA [38] MixPHM 0.87 0.39% 13.87 ±2.39 16.03 ±1.23 18.58 ±1.42 20.16 ±0.97 26.08 ±0.88 28.53 ±0.85 Table 1. Experimental results with pretrained VL-T5. The average VQA-Score with standard deviation across 5 seeds are evaluated on VQA v2 validation set, GQA test-dev, and OK-VQA test set. The best and second best parameter-efficient tuning methods are highlighted. The number of tuned parameters and the percentage of tuned parameters relative to VL-T5 (i.e., 224.54M) are reported. |Ddev| = |Dtrain| = ND), instead of using large-scale val- idation set, for best model selection and hyperparameter tuning. Specifically, we conduct experiments on ND ∈ {16, 32, 64, 100, 500, 1000}. To construct the Dtrain and Ddev of VQA v2, we randomly sample 2ND samples from its training set and divide them equally into the Dtrain and Ddev. Analogously, we construct Dtrain and Ddev for GQA and OK-VQA. VQA-Score [2] is the accuracy metric of the low-resource VQA task. Baselines. We compare our method with several state-of- the-art parameter-efficient tuning methods and finetuning. For a fair comparison, we perform hyperparameter search on their key hyperparameters (KHP) and report their best performance. Specifically, • BitFit [59] only tunes the bias weights of pretrained mod- els while keeping the rest parameters frozen. • LoRA [17] tunes additional low-rank matrices, which are used to approximate the query and value weights in each transformer self-attention and cross-attention layer. KHP is the matrix rank (r). • Compacter [24] adds adapters after each feed-forward layer of transformer blocks and reparameterizes adapter weights with low-rank PHM layers [61]. KHP are the number of summations of Kronecker product (n), the bot- tleneck dimension (dr), and r. • Houlsby [16] adds adapters after self-attention and feed- forward layers in each transformer block. KHP is dr. Method Model Size #Param (%) VQAv2 GQA OK-VQA Frozen [50] 7B - 38.2 12.6 - PICa-Base [57] 175B - 54.3 43.3 - PICa-Full [57] 175B - 56.1 48.0 - FewVLM [23] 225M 100% 48.2 32.2 15.0 MixPHM† 226M 0.39% 49.3 33.4 19.2 Table 2. Comparison with few-shot learner (ND=64). FewVLM is a prompt-based full finetuning method. MixPHM† means using MixPHM to tune FewVLM in a parameter-efficient manner. • Pfeiffer [41] is to determine the location of adapter based on pilot experiments. In this work, we place it after each transformer feed-forward layer. KHP is dr. • AdaMix [54] adds multiple adapters after each trans- former feed-forward layer in a MoE manner. KHP are the number of adapters (Ne), and dr. Implementation Details. We use four pretrained VLMs, i.e., VL-T5 [5], X-VLM [60], BLIP [30], and OFABase [53], as underlying encoder-decoder transformers, which formu- late VQA as a generation task in finetuning and thus do not introduce additional parameters from VQA heads. Since the original pretraining datasets employed by VL-T5 con- tain samples of the above VQA datasets, we instead load the weights2 released by Jinet al. [23], which is re-trained with- out the overlapped samples. All results are reported across five seeds {13, 21, 42, 87, 100}. More details and hyperpa- rameter setups are provided in Appendix D. 2https://github.com/woojeongjin/FewVLM 6Method VQA v2 GQA OK-VQA Finetuning 46.87 ±0.57 34.22 ±0.59 16.65 ±1.02 MixPHM∗ 47.30 ±0.67 34.66 ±0.78 18.05 ±1.16 + Lcs 46.70 ±0.66 34.83 ±1.35 17.37 ±1.38 + LI Ra 47.42 ±0.71 34.69 ±0.96 18.25 ±1.54 + LII Ra 47.71 ±0.85 36.10 ±0.83 18.21 ±1.08 + LRa 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 Table 3. Ablation on different regularizers with ND = 64. MixPHM∗ means the baseline without any regularizer. Lcs is a consistency regularizer [66]. LI Ra and LII Ra indicate that only using the first and the second term of LRa, respectively. 5.2. Low-Resource Visual Question Answering Table 1 shows the results with pretrained VL-T5 [5] on three datasets. Overall, our MixPHM outperforms state-of- the-art parameter-efficient tuning methods and is the only one that consistently outperforms full finetuning. Next, we elaborate on the different comparisons. Comparison with AdaMix. AdaMix and MixPHM adopt MoE to boost the capacity of adapters, but MixPHM con- siders further reducing parameter redundancy and learning more task-relevant representations. Table 1 shows that on all datasets with different ND, our MixPHM markedly out- performs AdaMix and full finetuning, while AdaMix fails to outperform full finetuning (except VQA v2 withND = 32). This result demonstrates the effectiveness of MixPHM in terms of performance and parameter efficiency, which also suggests the importance of prompting task-relevant correc- tion while reducing parameter redundancy. Comparison with Houlsby and Pfeiffer. PHM-expert in MixPHM has the same bottleneck structure with adapter. However, PHM-expert is more parameter-efficient due to the reduction of parameter redundancy and can better cap- ture task-relevant information owing to the proposed re- dundancy regularization. The result in Table 1 shows that compared to Finetuning, the performance of Houlsby and Pfeiffer falls far short of the ceiling performance in most low-resource settings. Conversely, the proposed MixPHM exhibits advantages in terms of performance and parameter efficiency under all dataset settings. Comparison with Compacter. To reduce parameter re- dundancy, Compacter and MixPHM reparameterize adapter weights with low-rank PHM. However, in reducing param- eter redundancy, MixPHM encourages task-relevant cor- rection with redundancy regularization and improves the model capacity with MoE, avoiding overfitting and under- parameterization concerns. Table 1 shows that Compacter does not perform as expected. One possible explanation is that Compacter is under-parameterized on the low-resource VQA task. Because too few trainable parameters do not guarantee that model capture enough task-relevant informa- tion in the tuning stage. This suggests that when tuning pretrained VLMs for low-resource VQA, it is necessary to WD WS #Param VQA v2 GQA OK-VQAD1 D2 S Adn Aup (M) Finetuning 224.54 46.87 ±0.57 34.22 ±0.59 16.65 ±1.02 ✓ 2.45 48.15 ±0.89 36.42 ±0.64 17.34 ±1.59 ✓ ✓ ✓ 1.55 47.79 ±1.11 36.59 ±0.64 18.77 ±0.99 ✓ ✓ ✓ ✓ 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 ✓ ✓ 1.37 47.67 ±1.13 36.22 ±0.89 17.65 ±2.38 ✓ ✓ ✓ 1.36 48.05 ±0.99 36.76 ±0.78 17.02 ±1.70 ✓ ✓ ✓ 0.83 47.30 ±0.92 36.05 ±1.05 17.27 ±0.63 ✓ ✓ ✓ ✓ 0.82 47.83 ±0.65 36.39 ±0.84 17.75 ±1.36 ✓ ✓ ✓ 0.88 47.78 ±1.20 36.57 ±0.81 18.07 ±1.73 ✓ ✓ ✓ ✓ 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 Table 4. Ablation on weight decomposition (WD) and weight sharing (WS). D1: the decomposition of expert weights in Mix- PHM with PHM. D2: the further low-rank reparameterization of the decomposed weights. balance the effectiveness and parameter efficiency. Comparison with LoRA and BitFit. LoRA and BitFit are two typical parameter-efficient methods that tune a part of parameters of original pretrained VLMs and are more parameter-efficient. The results are shown in Table 1. We observe that compared with MixPHM, the tunable parame- ters of LoRA and BitFit are relatively lightweight. How- ever, their performance trails much below MixPHM. In particular, the performance gap becomes larger as ND in- creases. Similar to the discussion on Compacter, too few trainable parameters in the tuning process may lead to over- fitting on the given dataset. Comparison with SoTA Few-Shot Learner. Few-shot VQA is a special case of low-resource VQA. The compar- isons with SoTA multimodal few-shot learner are shown in Table 2. Frozen [50] and PICa [57] are two in-context learning methods that adopt prompt-tuning to transfer large- scale language models ( i.e., GPT-2 and GPT-3 [3]) with- out parameter tuning. While FewVLM [23] is a prompt- based full finetuning method to adapt VL-T5, which inserts hand-crafted prompts into model inputs and finetunes full model parameters. We utilize MixPHM to tune FewVLM in a parameter-efficient manner. With only a tiny number of parameters tuned, MixPHM outperforms FewVLM in few- shot performance, especially on OK-VQA ( 19.2 vs. 15.0). This demonstrates the superiority of MixPHM in terms of performance and parameter efficiency. 5.3. Ablation Study We conduct all ablated experiments with pretrained VL- T5 on VQA v2, GQA, and OK-VQA with ND = 64. Effectiveness of Redundancy Regularization. To demon- strate the effectiveness of the proposed redundancy regular- ization, we first introduce a consistency regularizerLcs [66] for comparison. Moreover, to further analyze the contribu- tion of different terms in LRa, we consider two variations of LRa: ( i) LI Ra: only using the first term in Eq. (14) as the regularizer during training. ( ii) LII Ra: only using the sec- ond term in Eq. (14) during training. Table 3 shows that 7Figure 4. The average RSA similarity of MixPHM between ha and h (left) as well as ha and ˜h (right) at each transformer layer. Lcs improves MixPHM performance only on GQA and the improvement is minor. In contrast, LRa shows a significant improvement in MixPHM performance on all datasets. This observation demonstrates the effectiveness and superiority of the proposed regularizer LRa. Furthermore, analyzing the impact of LI Ra and LII Ra on MixPHM performance, we find that only reducing the redundancy between the repre- sentation of MixPHM and the representation of pretrained VLMs (i.e., LI Ra) makes limited contribution to performance gains. However, the joint effect ofLI Ra and LII Ra is better than LII Ra alone. This suggests that the trade-off between reduc- ing task-irrelevant redundancy and prompting task-relevant correlation is critical for MixPHM. Impact of Reducing Parameter Redundancy. MixPHM reduces parameter redundancy by first decomposing expert weights with PHM (D1) and then reparameterizing the de- composed weights (D2). We ablate D1 and D2 to analyze their effects on MixPHM performance ( i.e., the third col- umn in Table 4). In addition, weight sharing can further re- duce parameter redundancy in MixPHM. We thus conduct ablation on different meaningful combinations of shared weights in the fourth column of Table 4. Aside from the globally shared matrices ( S), we also locally share down- projection (Adn) or up-projection ( Aup) matrices between experts in one MixPHM. Table 4 shows that there is a trade- off between parameter efficiency and performance, i.e., ex- cessive parameter reduction may harm performance. There- fore, we advocate reducing parameter redundancy while maintaining model capacity. Impact of Hyperparameters. Results on hyperparameters (Ne, dr, dk, n, α) are available in Appendix A. 5.4. Discussion Redundancy Analysis of MixPHM. In this paper, we propose an insight that improving the effectiveness of adapters via reducing task-irrelevant redundancy and pro- moting task-relevant correlation in representations. To as- sess whether our method actually leads to performance im- provements based on this insight, we conduct the redun- dancy analysis in the MixPHM representation space under the same experimental settings as described in Sec. 4.1. Figure 4 illustrates the RSA similarity across 1k samples VLMs Method #Param #Sample (M) ND=16 ND=64 ND=500 ND=1000 X-VLM Finetuning 294 26.63 30.45 38.96 43.92 [60] MixPHM 0.66 27.54 31.80 41.05 48.06 BLIP Finetuning 385 27.01 30.05 37.00 42.22 [30] MixPHM 0.87 29.17 32.09 41.80 46.78 OFABase Finetuning 180 27.48 31.75 42.99 46.81 [53] MixPHM 0.70 28.46 33.00 45.88 50.01 Table 5. Experimental results of MixPHM on other pretrained VLMs. We report the average VQA-Score across five seeds on VQA v2 validation set under different low-resource settings. on VQA v2. Compared with the redundancy analysis re- sult of adapter shown in Figure 1, we observe that Mix- PHM markedly reduces the representation redundancy be- tween ha and h, while increasing the representation corre- lation between ha and ˜h. This finding provides a perceptive demonstration for the soundness of our motivation and the effectiveness of our method. Generalizability to Other Pretrained VLMs. To demon- strate the generalization capability of our method on other pretrained VLMs, we apply MixPHM to adapt pretrained X-VLM [60], BLIP [30], and OFA Base [53] for the low- resource VQA task. Table 5 presents a lite comparison be- tween our method and full finetuning on VQA v2. We ob- serve that MixPHM consistently outperforms full finetuning in all settings, with significant performance gains observed when ND ∈ {500, 1000}. Notably, the largest performance gaps from finetuning are achieved by X-VLM ( +4.14), BLIP (+4.80), and OFABase (+3.20) at ND=1000, ND=500, and ND=1000, respectively. These findings demonstrate the generalizability of MixPHM to various pretrained VLMs. More comparisons between parameter-efficient methods are available in Appendix C. 6. Conclusion and Limitation In this paper, we propose a redundancy-aware parameter- efficient tuning method to adapt pretrained VLMs to the low-resource VQA task. The proposed MixPHM reduces task-irrelevant redundancy while promoting task-relevant correlation through a proposed redundancy regularization. Experiments demonstrate its effectiveness and superiority in terms of performance and parameter efficiency. Redundancy is a double-edged sword. In addition to re- ducing task-irrelevant redundancy, we can also exploit task- relevant redundancy already learned by pretrained VLMs to enhance performance. Although MixPHM emphasizes re- ducing task-irrelevant redundancy, there is no explicit guar- antee that the reduced redundancy is ineffective for given tasks. As such, a potential prospect is to investigate how to explicitly delimit and minimize task-irrelevant redundancy. Acknowledgements. This work was supported by the National Science Foundation of China (Grant No. 62088102). 8Appendix In the Appendix, we provide some supplementary ma- terial for our experiments. Specifically, Appendix A ex- plores the impact of different routing mechanisms and hy- perparameters on MixPHM. Appendix B presents some vi- sualization results of our method. Appendix C provides additional comparisons between parameter-efficient tuning methods using pretrained X-VLM on VQA v2. Appendix D describes more implementation details. A. Ablation Study and Parameter Analysis In this section, using pretrained VL-T5 [5] as the under- lying pretrained VLMs, we conduct additional ablation ex- periments on routing mechanisms and hyperparameter anal- ysis on VQA v2, GQA, and OK-VQA with ND = 64. Impact of Routing Mechanisms. In MixPHM, in addition to performance, routing mechanisms also affect the train- ing speed, i.e., T/Itr (s). To analyze the impact of different routing strategies on performance and speed, we first in- troduce two random routing methods, i.e., token-level and sentence-level routing [8]. In addition, we develop a sim- ple representation-based rounding by averaging the outputs of all PHM-experts in each MixPHM. Table 6 shows that random routing mechanism is the fastest and has the best performance on both VQA v2 and OK-VQA. Impact of Hyperparameters. To investigate the impact of different hyperparameters on MixPHM, we conduct ex- periments by varying Ne, dr, dk, and n. More specifically, we consider the following settings: Ne ∈ {1, 2, 4, 8, 12}, dr ∈ {48, 64, 96, 192}, dk ∈ {1, 8, 16, 24}, and n ∈ {2, 4, 8, 16}. The results in Table 7 show that changing these hyperparameters has only a slight impact on the per- formance of MixPHM. In addition, the performance of Mix- PHM with different hyperparameters always outperforms full finetuning. This suggests that the performance improve- ment brought by MixPHM does not significantly depend on the hyperparameter selection. Impact of α. When tuning pretrained VLMs with Mix- PHM, α controls the trade-off between redundancy regu- larization and generative modeling loss. To investigate the impact of α on MixPHM, we perform experiments with dif- ferent values of α, i.e., α ∈ {0.04, 0.06, 0.08, 0.1, 0.2, 0.4}. Figure 5 illustrates the curve of VQA-Score as α increases. We observe that varyingα within a certain range [0.04, 0.4] does not hinder the advantage of MixPHM over full finetun- ing. In addition, according to the results on three datasets, we empirically set α to 0.2. B. Visualization Results We visualize some examples of the proposed MixPHM. As depicted in Figure 6, these predictions are generated by Method T/Itr (s) VQA v2 GQA OK-VQA MixPHM-Token 0.693 47.67 ±0.92 36.23 ±0.89 17.77 ±0.89 MixPHM-Sent 0.683 47.69 ±0.99 36.13 ±0.86 17.83 ±1.32 MixPHM-Rep 0.675 48.00 ±0.95 36.77 ±0.55 18.25 ±1.46 MixPHM 0.668 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 Table 6. Ablation on different routing mechanisms with ND = 64. T/Itr (s) is the average tuning time for each iteration. HP #Param VQA v2 GQA OK-VQA Finetuning 224.54 46.87 ±0.57 34.22 ±0.59 16.65 ±1.02 1 0.34 47.30 ±0.97 36.30 ±0.83 17.59 ±0.97 2 0.52 47.90 ±0.65 36.88 ±0.75 18.08 ±1.28 4 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 8 1.59 48.09 ±0.67 36.50 ±0.81 18.51 ±1.29 Ne 12 2.30 47.80 ±0.72 36.30 ±0.80 18.43 ±1.50 48 0.86 48.36 ±0.97 36.36 ±0.32 18.05 ±0.85 64 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 96 0.91 48.05 ±0.82 36.36 ±0.51 18.39 ±0.96 dr 192 1.00 47.97 ±1.17 36.37 ±0.82 18.26 ±1.20 1 0.18 47.87 ±0.73 35.74 ±0.70 17.04 ±0.81 8 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.70 16 1.67 48.35 ±1.14 36.62 ±0.35 18.22 ±1.36 dk 24 2.47 48.07 ±1.12 36.42 ±0.52 18.79 ±1.18 2 0.87 48.17 ±0.93 36.53 ±0.32 18.43 ±0.75 4 0.87 48.26 ±0.56 36.75 ±0.55 18.58 ±1.42 8 0.87 47.97 ±1.08 36.37 ±0.56 17.41 ±1.05 n 16 0.88 46.65 ±1.10 35.46 ±0.55 17.52 ±0.63 Table 7. Impact of hyperparameters (HP) on MixPHM. Ne: the number of PHM-experts, dr: bottleneck dimension, dk: rank dimension, n: the number of summations of Kronecker product. the VL-T5 tuned with MixPHM on VQA v2 withND = 64. In addition, we visualize the top-down attention [1] for im- ages and mark the top two task-relevant tokens for ques- tions. More specifically, we follow a recent work [22] to compute an attention score between task-relevant represen- tations and visual input features obtained using bottom-up Faster R-CNN [1] and visualize the top-down attention for the first and second highest scores. Analogously, we com- pute the score between task-relevant representations and linguistic embeddings of questions and mark the tokens for the first and second highest scores. Figure 6 qualita- tively shows that our MixPHM can generate consistent and question-relevant visual and textual attentions in the process of answer prediction. C. Results with Pretrained X-VLM As a supplement to the results in Table 5 of the main pa- per, we utilize pretrained X-VLM [60] as a representative and compare our methods with state-of-the-art parameter- efficient tuning methods on VQA v2 validation set. The key hyperparameter settings for these parameter-efficient methods are the same as those in Table 9. The conclu- sions that we observe in Table 8 are consistent with Ta- 9/uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013 × 10 2 /uni00000017/uni00000017/uni00000011/uni00000013 /uni00000017/uni00000017/uni00000011/uni00000018 /uni00000017/uni00000018/uni00000011/uni00000013 /uni00000017/uni00000018/uni00000011/uni00000018 /uni00000017/uni00000019/uni00000011/uni00000013 /uni00000017/uni00000019/uni00000011/uni00000018 /uni00000017/uni0000001a/uni00000011/uni00000013 /uni00000017/uni0000001a/uni00000011/uni00000018 /uni00000017/uni0000001b/uni00000011/uni00000013 /uni00000017/uni0000001b/uni00000011/uni00000018 /uni00000017/uni0000001c/uni00000011/uni00000013/uni00000039/uni00000034/uni00000024/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a /uni00000030/uni0000004c/uni0000005b/uni00000033/uni0000002b/uni00000030 (a) VQA v2 /uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013 × 10 2 /uni00000016/uni00000015/uni00000011/uni00000018 /uni00000016/uni00000016/uni00000011/uni00000013 /uni00000016/uni00000016/uni00000011/uni00000018 /uni00000016/uni00000017/uni00000011/uni00000013 /uni00000016/uni00000017/uni00000011/uni00000018 /uni00000016/uni00000018/uni00000011/uni00000013 /uni00000016/uni00000018/uni00000011/uni00000018 /uni00000016/uni00000019/uni00000011/uni00000013 /uni00000016/uni00000019/uni00000011/uni00000018 /uni00000016/uni0000001a/uni00000011/uni00000013 /uni00000016/uni0000001a/uni00000011/uni00000018/uni00000039/uni00000034/uni00000024/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a /uni00000030/uni0000004c/uni0000005b/uni00000033/uni0000002b/uni00000030 (b) GQA /uni00000017/uni00000011/uni00000013/uni00000019/uni00000011/uni00000013/uni0000001b/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000017/uni00000013/uni00000011/uni00000013 × 10 2 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni00000018/uni00000011/uni00000018 /uni00000014/uni00000019/uni00000011/uni00000013 /uni00000014/uni00000019/uni00000011/uni00000018 /uni00000014/uni0000001a/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000014/uni0000001b/uni00000011/uni00000013 /uni00000014/uni0000001b/uni00000011/uni00000018 /uni00000014/uni0000001c/uni00000011/uni00000013 /uni00000014/uni0000001c/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013/uni00000039/uni00000034/uni00000024/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c /uni00000029/uni0000004c/uni00000051/uni00000048/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a /uni00000030/uni0000004c/uni0000005b/uni00000033/uni0000002b/uni00000030 (c) OK-VQA Figure 5. The average VQA-Score with standard deviation across five seeds as α varies. Q: What is the persondoing in the air?GT: snowboarding (1.0), jumping (0.6)Prediction: snowboarding Q: Of the words onthe signs, which ones are official?GT: both (0.3), left (0.3)Prediction: both Q: Are those ducksin the water?GT: no (1.0)Prediction: no  Q: Is the personsurfing?GT: yes (1.0), surfing(0.3)Prediction: yes Q: How many peopleare in the picture?GT: 2 (1.0)Prediction: 2 Q: Who wants to play?GTdog (1.0)Prediction: dog Q: Whatbrandis that laptop?GT: apple (1.0)Prediction: apple Q: What color is the bear's coat?GT: green (1.0),greenandwhite(0.9)Prediction: green Q: How many baby teethare visible?GT: 4 (1.0),3 (0.6)Prediction: 3 Q: What kind of materialis this made of?GT: wood (1.0)Prediction: wood Q: What is the numberon the player's back?GT: 30 (1.0)Prediction: 30 Q: Who isin the photo?GT: child (1.0), baby (0.9), kid (0.3), boy (0.3)Prediction: kid Q: What are they getting ready to do?GT: surf(1.0)Prediction: surf Q: Is the man wearing a shirt?GT: yes (1.0)Prediction: yes Q: What ison top of the hot dog?GT: relish(0.6), pickle (0.6), onion (0.3)Prediction: relish Q: Will someonebe injured?GT: no (1.0), yes (0.6)Prediction: yes Q: Which color are the bananas?GT: yellow(1.0)Prediction: yellow Q: Is this woman doingsomething active?GT: no (1.0)Prediction: no Figure 6. Qualitative results on VQA v2 validation set. The Prediction is generated by the VL-T5 tuned with the proposed MixPHM. GT denotes the annotated answer and the corresponding score. We visualize the top-down attention [1] of images and mark the task- relevant tokens of questions for the first and second highest attention scores. ble 5, i.e., our method consistently outperforms existing parameter-efficient tuning methods when using other pre- trained VLMs, which further demonstrates the generaliza- tion capability of MixPHM. 10Method #Param #Sample (M) (%) ND=16 ND=32 ND=64 ND=100 ND=500 ND=1,000 Finetuning 293.48 100% 26.63 ±0.98 29.33 ±1.68 30.45 ±1.80 31.48 ±1.57 38.96 ±1.56 43.92 ±1.22 BitFit [59] 0.29 0.13% 25.48 ±3.81 28.90 ±1.14 30.73 ±1.18 31.92 ±1.14 36.77 ±1.32 40.77 ±0.79 LoRA [17] 0.37 0.13% 25.31 ±1.50 26.91 ±3.09 30.52 ±1.67 31.97 ±1.11 36.13 ±1.12 40.49 ±0.87 Compacter [24] 0.25 0.09% 25.69 ±2.34 28.04 ±1.63 28.10 ±2.06 31.35 ±0.34 35.91 ±0.65 40.44 ±0.77 Houlsby [16] 3.57 1.20% 26.54 ±2.57 29.34 ±2.25 30.74 ±1.20 31.71 ±1.43 38.48 ±0.91 41.96 ±0.72 Pfeiffer [41] 1.78 0.60% 26.57 ±2.00 28.46 ±1.74 29.22 ±2.56 31.95 ±1.34 37.39 ±0.73 40.96 ±1.09 AdaMix [54] 4.44 1.49% 26.11 ±1.58 28.91 ±1.36 30.71 ±2.05 31.15 ±1.26 38.48 ±1.53 43.26 ±0.85 MixPHM 0.66 0.22% 27.54 ±1.52 30.65 ±1.09 31.80 ±1.61 32.58 ±1.09 41.05 ±1.22 48.06 ±0.64 Table 8. Experimental results with pretrained X-VLM. The average VQA-Score with standard deviation across 5 different seeds are evaluated on VQA v2 validation set. The best and second best parameter-efficient tuning methods are highlighted. The number of tuned parameters and the percentage of tuned parameters relative to X-VLM (i.e., 293.48M) are reported. Method Learning rate Configuration Finetuning 5 × 10−5 — BitFit 5 × 10−5 — LoRA 5 × 10−5 r = 4 Compacter 5 × 10−3 dr = 64, dk = 8, n = 4 Houlsby 5 × 10−5 dr = 64 Pfeiffer 5 × 10−5 dr = 64 AdaMix 5 × 10−4 Ne = 4, dr = 64 MixPHM 5 × 10−3 Ne = 4, dr = 64, dk = 8, n = 4 Table 9. Hyperparameter settings for all parameter-efficient tuning methods. Ne: the number of experts, dr: bottleneck di- mension, dk and r: rank dimension, n: the number of summations of Kronecker product. D. Implementation Details For parameter-efficient tuning methods, we search the bottleneck dimension dr from {48, 64, 96, 192} for all adapter-based methods ( i.e., MixPHM, AdaMix, Pfeiffer, Houlsby and Compacter), the number of experts Ne from {1, 2, 4, 8, 12} for MixPHM and AdaMix, the rank dimen- sion dr (for MixPHM and Compacter), r (for LoRA) from {1, 8, 16, 24}, as well as the number of summations of Kro- necker product n from {2, 4, 8, 16} for MixPHM and Com- pacter. Table 9 presents the final configuration of the hyper- parameters used in our experiments. For MixPHM, we set the trade-off factor α to 0.2. All methods are implemented using Pytorch [39] on an NVIDIA GeForce RTX 3090Ti GPU. In addition, we also perform a grid search to select the best learning rate from {5 × 10−5, 1 × 10−4, 5 × 10−4, 1 × 10−3, 5 × 10−3}. The batch size and the number of epochs are set to 16 and 1000, respectively. We utilize AdamW optimizer [34] and the early stopping strategy with a patience of 200 non- increasing epochs, where the stopping metric is the VQA- Score on the development set Ddev of datasets. References [1] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, pages 6077–6086, 2018. 9, 10 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. InICCV, pages 2425–2433, 2015. 1, 2, 6 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, pages 1877–1901, 2020. 7 [4] Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and Shang- song Liang. Revisiting parameter-efficient tuning: Are we really there yet? In EMNLP, pages 2612–2626, 2022. 5 [5] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unify- ing vision-and-language tasks via text generation. In ICML, pages 1931–1942, 2021. 1, 2, 3, 4, 6, 7, 9 [6] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of train- ing end-to-end vision-and-language transformers. In CVPR, pages 18166–18176, 2022. 1, 2 [7] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Effi- cient scaling of language models with mixture-of-experts. In ICML, pages 5547–5569, 2022. 2 [8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with sim- ple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. 2, 9 [9] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision- and-language representation learning. In NeurIPS, 2020. 2 [10] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre- trained language models better few-shot learners. In ACL, pages 3816–3830, 2021. 5 11[11] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. Parameter-efficient mixture-of-experts ar- chitecture for pre-trained language models. In COLING, pages 3263–3273, 2022. 2 [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba- tra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answer- ing. In CVPR, pages 6904–6913, 2017. 2, 4, 5, 6 [13] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg- Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022. 2 [14] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. On the effectiveness of adapter-based tuning for pretrained lan- guage model adaptation. In ACL, pages 2208–2222, 2021. 4 [15] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual informa- tion estimation and maximization. In ICLR, 2019. 5 [16] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In ICML, pages 2790–2799, 2019. 1, 2, 4, 6, 11 [17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 1, 2, 6, 11 [18] Ronghang Hu and Amanpreet Singh. Unit: Multimodal mul- titask learning with a unified transformer. In ICCV, pages 1439–1449, 2021. 2 [19] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the box: End-to- end pre-training for vision-language representation learning. In CVPR, pages 12976–12985, 2021. 2 [20] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 6700–6709, 2019. 2, 5, 6 [21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904– 4916, 2021. 2 [22] Jingjing Jiang, Ziyi Liu, and Nanning Zheng. Correlation information bottleneck: Towards adapting pretrained multi- modal models for robust visual question answering. arXiv preprint arXiv:2209.06954, 2022. 2, 9 [23] Woojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen, and Xiang Ren. A good prompt is worth millions of pa- rameters: Low-resource prompt-based learning for vision- language models. In ACL, pages 2763–2775, 2022. 2, 6, 7 [24] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. In NeurIPS, pages 1022–1035, 2021. 1, 2, 6, 11 [25] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision- and-language transformer without convolution or region su- pervision. In ICML, pages 5583–5594, 2021. 1, 2 [26] Aarre Laakso and Garrison Cottrell. Content and cluster analysis: Assessing representational similarity in neural sys- tems. Philosophical Psychology, 13(1):47–76, 2000. 4 [27] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant mod- els with conditional computation and automatic sharding. In ICLR, 2021. 2 [28] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045–3059, 2021. 2 [29] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In ICML, pages 6265–6274, 2021. 2 [30] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, pages 12888–12900, 2022. 1, 2, 6, 8 [31] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learn- ing with momentum distillation. In NeurIPS, pages 9694– 9705, 2021. 1, 2 [32] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL, pages 4582– 4597, 2021. 2 [33] Bingqian Lin, Yi Zhu, Zicong Chen, Xiwen Liang, Jianzhuang Liu, and Xiaodan Liang. Adapt: Vision-language navigation with modality-aligned action prompts. In CVPR, pages 15396–15406, 2022. 2 [34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 11 [35] Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. Variational information bottleneck for effective low-resource fine-tuning. In ICLR, 2021. 2 [36] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa De- hghani, and James Henderson. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. In ACL, pages 565–576, 2021. 2 [37] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Alma- hairi, Hao Ma, Jiawei Han, Scott Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient lan- guage model tuning. In ACL, pages 6253–6264, 2022. 2 [38] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In CVPR, pages 3195–3204, 2019. 2, 5, 6 [39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8024–8035, 2019. 11 [40] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few- shot learning with language models. In NeurIPS, pages 11054–11070, 2021. 5 12[41] Jonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non- destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247, 2020. 1, 2, 4, 6, 11 [42] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ´e Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mix- ture of experts. In NeurIPS, pages 8583–8595, 2021. 2 [43] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. In NeurIPS, pages 17555–17566, 2021. 2 [44] Andreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers. In EMNLP, pages 7930–7946, 2021. 2 [45] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra- geously large neural networks: The sparsely-gated mixture- of-experts layer. In ICLR, 2017. 1, 2, 3 [46] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. VL-adapter: Parameter-efficient transfer learning for vision-and-language tasks. In CVPR, pages 5227–5237, 2022. 1, 2 [47] Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neu- ral networks with fixed sparse masks. In NeurIPS, pages 24193–24205, 2021. 2 [48] Hao Tan and Mohit Bansal. LXMERT: Learning cross- modality encoder representations from transformers. In EMNLP, pages 5099–5110, 2019. 2 [49] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. arXiv preprint arXiv:1503.02406, 2015. 2 [50] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Es- lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In NeurIPS, pages 200–212, 2021. 2, 6, 7 [51] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. InfoBERT: Improving robust- ness of language models from an information theoretic per- spective. In ICLR, 2021. 2 [52] Haoqing Wang, Xun Guo, Zhi-Hong Deng, and Yan Lu. Rethinking minimal sufficient representation in contrastive learning. In CVPR, pages 16041–16050, 2022. 2 [53] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. OFA: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML, pages 23318–23340, 2022. 1, 2, 6, 8 [54] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. In EMNLP, pages 5744–5760, 2022. 1, 2, 4, 6, 11 [55] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In ICLR, 2022. 3 [56] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Re- becca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Ko- rnblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing in- ference time. In ICML, pages 23965–23998, 2022. 5 [57] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yu- mao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In AAAI, pages 3081–3089, 2022. 2, 6, 7 [58] Zonghan Yang and Yang Liu. On robust prefix-tuning for text classification. In ICLR, 2022. 2 [59] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, pages 1–9, 2022. 1, 2, 6, 11 [60] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vi- sion language pre-training: Aligning texts with visual con- cepts. In ICML, pages 25994–26009, 2022. 1, 6, 8, 9 [61] Aston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, and Jie Fu. Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters. In ICLR, 2021. 3, 6 [62] Ningyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun Chen. Differen- tiable prompt makes pre-trained language models better few- shot learners. In ICLR, 2022. 2 [63] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, pages 5579–5588, 2021. 2 [64] Zhengkun Zhang, Wenya Guo, Xiaojun Meng, Yasheng Wang, Yadao Wang, Xin Jiang, Qun Liu, and Zhenglu Yang. Hyperpelt: Unified parameter-efficient language model tun- ing for both language and vision-and-language tasks. arXiv preprint arXiv:2203.03878, 2022. 2 [65] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In CVPR, pages 16793–16803, 2022. 2 [66] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. Tam- ing sparsely activated transformer with stochastic experts. In ICLR, 2022. 4, 7 13",
      "meta_data": {
        "arxiv_id": "2303.01239v2",
        "authors": [
          "Jingjing Jiang",
          "Nanning Zheng"
        ],
        "published_date": "2023-03-02T13:28:50Z",
        "pdf_url": "https://arxiv.org/pdf/2303.01239v2.pdf",
        "github_url": "https://github.com/woojeongjin/FewVLM"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes MixPHM, a redundancy-aware parameter-efficient tuning method that addresses the computational expense, storage inefficiency, and overfitting issues of finetuning large Vision-Language Models (VLMs) for low-resource Visual Question Answering (VQA). MixPHM aims to reduce parameter redundancy while maintaining adapter capacity and to reduce task-irrelevant redundancy while promoting task-relevant correlation in representations. It significantly outperforms state-of-the-art parameter-efficient tuning methods and consistently surpasses full finetuning in low-resource VQA settings.",
        "methodology": "MixPHM is a lightweight module implemented with multiple PHM-experts in a Mixture-of-Experts (MoE) manner. To reduce parameter redundancy, expert weights are reparameterized into a low-rank subspace using Parameterized Hypercomplex Multiplication (PHM) layers, and a strategy of global and local weight sharing is employed across and within experts. A stochastic routing mechanism is used during training, and a weight aggregation strategy is used during inference. The method also introduces Redundancy Regularization (LRa), which has two terms: one term decorrelates the similarity matrix between MixPHM representations and pretrained VLM representations to reduce task-irrelevant redundancy, and another term maximizes the mutual information between MixPHM representations and the final task-used representations to promote task-relevant correlation. VQA is formulated as a generative modeling task.",
        "experimental_setup": "Experiments are conducted on three VQA datasets: VQA v2, GQA, and OK-VQA. The low-resource setting is simulated by using training data sizes (ND) smaller than 1,000, specifically ND values of {16, 32, 64, 100, 500, 1000}. A development set (Ddev) of the same size as the training set is used for best model selection and hyperparameter tuning. The evaluation metric is VQA-Score accuracy. The underlying pretrained VLMs used are encoder-decoder transformers: VL-T5, X-VLM, BLIP, and OFA Base. Baselines include full finetuning and other state-of-the-art parameter-efficient tuning methods like BitFit, LoRA, Compacter, Houlsby, Pfeiffer, and AdaMix, as well as few-shot learners such as Frozen, PICa, and FewVLM. Results are reported as the average score with standard deviation across five random seeds.",
        "limitations": "The paper acknowledges that redundancy is a double-edged sword. While MixPHM aims to reduce task-irrelevant redundancy, there is no explicit guarantee that the reduced redundancy is completely ineffective or non-beneficial for all given tasks.",
        "future_research_directions": "A potential future research direction is to investigate how to explicitly delimit and minimize task-irrelevant redundancy more precisely, building upon the insight that task-relevant redundancy from pretrained VLMs can also be exploited to enhance performance.",
        "experimental_code": "import torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom transformers.models.t5.modeling_t5 import T5Stack, T5LayerNorm, T5ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, ModelOutput\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nimport copy\n\n\nclass VisualEmbedding(nn.Module):\n    def __init__(self, config, obj_order_embedding):\n        super().__init__()\n        self.config = config\n        feat_dim = config.feat_dim\n        pos_dim = config.pos_dim\n        n_images = config.n_images\n\n        if self.config.individual_vis_layer_norm:\n            feat_embedding = [nn.Linear(feat_dim, config.d_model)]\n            if self.config.use_vis_layer_norm:\n                feat_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n            self.feat_embedding = nn.Sequential(*feat_embedding)\n\n            absolute_vis_pos_embedding = [nn.Linear(pos_dim + 1, config.d_model)]\n            if self.config.use_vis_layer_norm:\n                absolute_vis_pos_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n            self.absolute_vis_pos_embedding = nn.Sequential(*absolute_vis_pos_embedding)\n\n            if self.config.use_vis_order_embedding:\n                self.obj_order_embedding = obj_order_embedding\n                self.img_order_embedding = nn.Embedding(n_images, config.d_model)\n        else:\n            feat_embedding = [nn.Linear(feat_dim, config.d_model)]\n            self.feat_embedding = nn.Sequential(*feat_embedding)\n\n            absolute_vis_pos_embedding = [nn.Linear(pos_dim + 1, config.d_model)]\n            self.absolute_vis_pos_embedding = nn.Sequential(*absolute_vis_pos_embedding)\n\n            if self.config.use_vis_order_embedding:\n                self.obj_order_embedding = obj_order_embedding\n                self.img_order_embedding = nn.Embedding(n_images, config.d_model)\n\n            if self.config.use_vis_layer_norm:\n                self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n\n    def get_area(self, pos):\n        height = pos[:, :, 3] - pos[:, :, 2]\n        width = pos[:, :, 1] - pos[:, :, 0]\n        area = height * width\n        return area\n\n\n    def forward(self, feats, pos, img_order_ids=None, obj_order_ids=None):\n        B, N, _ = feats.size()\n        assert pos.size() == (B, N, 4)\n\n        feat_embedding = self.feat_embedding(feats)\n\n        device = feats.device\n\n        area = self.get_area(pos).unsqueeze(2)\n        pos = torch.cat([pos, area], dim=2)\n\n        absolute_vis_pos_embedding = self.absolute_vis_pos_embedding(pos)\n\n\n        if self.config.use_vis_order_embedding:\n            if img_order_ids is None:\n                img_order_ids = torch.zeros(N, dtype=torch.long, device=device)\n                img_order_ids = img_order_ids.unsqueeze(0)\n            img_order_embedding = self.img_order_embedding(img_order_ids)\n\n            if obj_order_ids is None:\n                obj_order_ids = torch.arange(N, dtype=torch.long, device=device)\n                obj_order_ids = obj_order_ids.unsqueeze(0)\n            obj_order_ids = self.obj_order_embedding.num_embeddings - obj_order_ids - 1\n            obj_order_embedding = self.obj_order_embedding(obj_order_ids)\n\n            vis_embedding = feat_embedding + absolute_vis_pos_embedding + \\\n                img_order_embedding + obj_order_embedding\n\n        else:\n            vis_embedding = feat_embedding + absolute_vis_pos_embedding\n\n        if not self.config.individual_vis_layer_norm:\n            if self.config.use_vis_layer_norm:\n                vis_embedding = self.layer_norm(vis_embedding)\n\n        return vis_embedding\n\n\nclass JointEncoder(T5Stack):\n    def __init__(self, config, embed_tokens=None):\n        super(T5Stack, self).__init__(config)\n        self.config = config\n\n        self.embed_tokens = embed_tokens\n        self.is_decoder = self.config.is_decoder\n        assert self.config.is_decoder is False\n\n        self.visual_embedding = VisualEmbedding(self.config, embed_tokens)\n\n        self.block = nn.ModuleList(\n            [T5Block(config, has_relative_attention_bias=(i == 0))\n                for i in range(config.num_layers)]\n        )\n        self.final_layer_norm = T5LayerNorm(\n            config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n        self.init_weights()\n\n    def set_input_embeddings(self, new_embeddings):\n        self.embed_tokens = new_embeddings\n        self.visual_embedding.obj_order_embedding = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n\n        vis_inputs=None,\n        vis_attention_mask=None,\n\n        inputs_embeds=None,\n        head_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n\n        if inputs_embeds is None:\n            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        B, L = inputs_embeds.size()[:-1]\n\n        vis_feats = vis_inputs[0]\n        boxes = vis_inputs[1]\n        img_order_ids = None\n        obj_order_ids = None\n        if len(vis_inputs) >= 3:\n            img_order_ids = vis_inputs[2]\n        if len(vis_inputs) == 4:\n            obj_order_ids = vis_inputs[3]\n\n        vis_embeds = self.visual_embedding(\n            vis_feats, boxes, img_order_ids, obj_order_ids)\n\n        V_L = vis_embeds.size(1)\n\n        inputs_embeds = torch.cat([inputs_embeds, vis_embeds], dim=1)\n\n        if attention_mask is None:\n            attention_mask = input_ids.ne(self.config.pad_token_id).to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n\n        if vis_attention_mask is None:\n            vis_attention_mask = attention_mask.new_ones(B, V_L)\n\n        attention_mask = torch.cat([attention_mask, vis_attention_mask], dim=1)\n\n        extended_attention_mask = self.get_extended_attention_mask(\n            attention_mask,\n            (B, L+V_L),\n            inputs_embeds.device)\n\n        if past_key_values is None:\n            past_key_values = [None] * len(self.block)\n\n        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n        present_key_value_states = () if use_cache else None\n        all_hidden_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n\n        hidden_states = self.dropout(inputs_embeds)\n\n        if self.config.num_layers > 0:\n            assert self.block[0].layer[0].SelfAttention.has_relative_attention_bias\n\n            seq_length = L + V_L\n            text_position_bias = self.block[0].layer[0].SelfAttention.compute_bias(\n                L, L)\n            num_heads = text_position_bias.size(1)\n            position_bias = text_position_bias.new_zeros(\n                1, num_heads, seq_length, seq_length)\n            position_bias[:, :, :L, :L] = text_position_bias\n\n            position_bias = position_bias + extended_attention_mask\n\n            for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask=extended_attention_mask,\n                    position_bias=position_bias,\n                    encoder_hidden_states=None,\n                    encoder_attention_mask=None,\n                    encoder_decoder_position_bias=None,\n                    head_mask=head_mask[i],\n                    past_key_value=past_key_value,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                )\n                hidden_states, present_key_value_state = layer_outputs[:2]\n\n                position_bias = layer_outputs[2]\n\n                if use_cache:\n                    present_key_value_states = present_key_value_states + \\\n                        (present_key_value_state,)\n\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    present_key_value_states,\n                    all_hidden_states,\n                    all_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=present_key_value_states,\n            hidden_states=all_hidden_states,\n            attentions=all_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass FewVLM(T5ForConditionalGeneration):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder\\.embed_tokens\\.weight\",\n        r\"decoder\\.embed_tokens\\.weight\",\n        r\"lm_head\\.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n    ]\n\n    def __init__(self, config):\n        super(T5ForConditionalGeneration, self).__init__(config)\n\n        self.config = config\n        self.model_dim = config.d_model\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n\n        self.encoder = JointEncoder(encoder_config, self.shared)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n        self.init_weights()\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        encoder_outputs=None,\n\n        vis_inputs=None,\n        vis_attention_mask=None,\n\n        decoder_input_ids=None,\n        decoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        labels=None,\n        inputs_embeds=None,\n        decoder_inputs_embeds=None,\n        head_mask=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        reduce_loss=False,\n\n        return_hidden_state=False,\n\n        **kwargs,\n    ):\n\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n\n                vis_inputs=vis_inputs,\n                vis_attention_mask=vis_attention_mask,\n\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(\n                    encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(\n                    encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n            decoder_input_ids = self._shift_right(labels)\n\n        if past_key_values is not None:\n            assert labels is None, \"Decoder should not use cached key value states when training.\"\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids[:, -1:]\n            if decoder_inputs_embeds is not None:\n                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n\n        if attention_mask is None:\n            attention_mask = input_ids.ne(self.config.pad_token_id).to(dtype=hidden_states.dtype, device=hidden_states.device)\n        if vis_attention_mask is None:\n            B, L = attention_mask.size()\n            V_L = encoder_outputs[0].size(1) - L\n            vis_attention_mask = attention_mask.new_ones(B, V_L)\n        encoder_attention_mask = torch.cat([attention_mask, vis_attention_mask], dim=1)\n\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n\n            head_mask=head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        assert self.config.tie_word_embeddings is True\n\n        if self.config.tie_word_embeddings:\n            sequence_output = sequence_output * (self.model_dim ** -0.5)\n\n        if return_hidden_state:\n            return sequence_output\n\n        lm_logits = self.lm_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            if reduce_loss:\n                loss_fct = CrossEntropyLoss(ignore_index=-100)\n            else:\n                loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='none')\n            loss = loss_fct(\n                lm_logits.view(-1, lm_logits.size(-1)),\n                labels.view(-1))\n\n        return VLSeq2SeqLMOutput(\n            loss=loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_last_hidden_state=decoder_outputs.last_hidden_state,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past=None, attention_mask=None, use_cache=None,\n        encoder_outputs=None,\n        **kwargs):\n\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        output = {\n            \"decoder_input_ids\": input_ids,\n            \"past_key_values\": past,\n            \"encoder_outputs\": encoder_outputs,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n        if 'vis_attention_mask' in kwargs:\n            output['vis_attention_mask'] = kwargs['vis_attention_mask']\n\n        return output\n\n    @staticmethod\n    def _expand_inputs_for_generation(\n        input_ids: torch.LongTensor,\n        expand_size: int = 1,\n        is_encoder_decoder: bool = False,\n        attention_mask: torch.LongTensor = None,\n        encoder_outputs: ModelOutput = None,\n        **model_kwargs\n    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n        expanded_return_idx = (\n            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1,\n                                                                expand_size).view(-1).to(input_ids.device)\n        )\n        input_ids = input_ids.index_select(0, expanded_return_idx)\n\n        if \"token_type_ids\" in model_kwargs:\n            token_type_ids = model_kwargs[\"token_type_ids\"]\n            model_kwargs[\"token_type_ids\"] = token_type_ids.index_select(\n                0, expanded_return_idx)\n\n        if attention_mask is not None:\n            model_kwargs[\"attention_mask\"] = attention_mask.index_select(\n                0, expanded_return_idx)\n\n        if model_kwargs.get(\"vis_attention_mask\", None) is not None:\n            model_kwargs['vis_attention_mask'] = model_kwargs['vis_attention_mask'].index_select(\n                0, expanded_return_idx)\n\n        if is_encoder_decoder:\n            assert encoder_outputs is not None\n            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n                0, expanded_return_idx\n            )\n            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n\n        return input_ids, model_kwargs\n\n\n@dataclass\nclass VLSeq2SeqLMOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    past_key_values: Optional[List[torch.FloatTensor]] = None\n    decoder_last_hidden_state: Optional[Tuple[torch.FloatTensor]] = None\n    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n    vis_encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n    vis_encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    vis_encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n\n\nclass FewVLMVQA(FewVLM):\n    def __init__(self, config, num_answers=None, label2ans=None):\n        super().__init__(config)\n\n        self.num_answers = num_answers\n        self.label2ans = label2ans\n\n    def train_step(self, batch):\n\n        device = next(self.parameters()).device\n        vis_feats = batch['vis_feats'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        vis_pos = batch['boxes'].to(device)\n\n        lm_labels = batch[\"target_ids\"].to(device)\n        output = self(\n            input_ids=input_ids,\n            vis_inputs=(vis_feats, vis_pos),\n            labels=lm_labels,\n            return_dict=True\n        )\n        assert 'loss' in output\n\n        lm_mask = (lm_labels != -100).float()\n        B, L = lm_labels.size()\n\n        loss = output['loss']\n\n        loss = loss.view(B, L) * lm_mask\n\n        loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1) # B\n\n        loss = loss * batch['scores'].to(device=device)\n\n        loss = loss.mean()\n\n        result = {\n            'loss': loss\n        }\n\n        return result\n\n    @torch.no_grad()\n    def test_step(self, batch, **kwargs):\n        self.eval()\n        device = next(self.parameters()).device\n        vis_feats = batch['vis_feats'].to(device)\n        input_ids = batch['input_ids'].to(device)\n        vis_pos = batch['boxes'].to(device)\n\n        result = {}\n        output = self.generate(\n            input_ids=input_ids,\n            vis_inputs=(vis_feats, vis_pos),\n            **kwargs\n        )\n        generated_sents = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n        result['token_ids'] = output\n        result['pred_ans'] = generated_sents\n\n        return result\n",
        "experimental_info": "The model, named FewVLM, is based on the T5 architecture and is adapted for Vision-Language tasks, specifically VQA formulated as a generative modeling task.\n\n**Architecture Details:**\n*   **Base Model:** FewVLM extends `T5ForConditionalGeneration`, employing a T5-based encoder-decoder structure.\n*   **Joint Encoder:** The standard T5 encoder is replaced with a `JointEncoder` that fuses visual and textual information. This encoder integrates `VisualEmbedding`.\n*   **Visual Embedding:** Processes visual features (`vis_feats`, typically 2048-dimensional) and normalized bounding box coordinates (`boxes`, 4 values) augmented with area information (`pos_dim + 1`). It supports optional visual layer normalization and uses object order embeddings (`obj_order_embedding`) and image order embeddings (`img_order_embedding`). The `n_boxes` parameter (e.g., 36) defines the number of visual regions.\n*   **Text and Vision Fusion:** Text embeddings from `self.shared` (T5's token embeddings) and visual embeddings from `VisualEmbedding` are concatenated before being passed through the T5 blocks in the `JointEncoder`.\n*   **Relative Position Bias:** Only applied between text tokens in the encoder; no relative position bias between text and vision or vision and vision tokens.\n\n**VQA Task Formulation & Training:**\n*   **Generative Task:** VQA is treated as a sequence-to-sequence generation problem, where the model generates the answer token by token.\n*   **Input Tokenization:** Questions are tokenized using `FewVLMTokenizerFast` (a T5TokenizerFast variant) with a maximum input length (`max_text_length`, typically 20). Prompting schemes, such as `question: {sent} answer: <extra_id_0>`, are configurable (`args.prompt`).\n*   **Target Generation:** Answers are tokenized into `target_ids` with a maximum generation length (`gen_max_length`, typically 10 for VQA). The `pad_token_id` is replaced with -100 in labels for loss calculation.\n*   **Loss Function:** `CrossEntropyLoss` (with `ignore_index=-100` and `reduction='none'`) is used for language modeling on the generated answer sequence. The per-example loss is then weighted by `batch['scores']` (representing answer scores from the dataset) before averaging.\n*   **Optimizer and Scheduler:** `AdamW` is used as the optimizer, typically with a learning rate of `1e-4`, `adam_eps=1e-6`, `adam_beta1=0.9`, `adam_beta2=0.999`, and `weight_decay=0.01`. A linear learning rate schedule with a warmup period (`warmup_ratio=0.05`) is employed.\n*   **Training Dynamics:** Gradient accumulation (`gradient_accumulation_steps`) and gradient clipping (`clip_grad_norm`) are supported. Mixed-precision training (`fp16`) can be enabled.\n\n**Inference and Evaluation:**\n*   **Prediction:** During inference, `model.generate()` is used, typically with beam search (`num_beams`, default 1; configurable for >1 beams) and a maximum generation length (`gen_max_length`).\n*   **Evaluation Metrics:** VQA accuracy is computed using `VQAEvaluator`, which handles answer normalization (punctuation, digits, articles) for fair comparison against ground truth. The evaluation reports overall accuracy, and accuracies per question type and answer type."
      }
    },
    {
      "title": "Variational Information Bottleneck for Effective Low-Resource Fine-Tuning",
      "abstract": "While large-scale pretrained language models have obtained impressive results\nwhen fine-tuned on a wide variety of tasks, they still often suffer from\noverfitting in low-resource scenarios. Since such models are general-purpose\nfeature extractors, many of these features are inevitably irrelevant for a\ngiven target task. We propose to use Variational Information Bottleneck (VIB)\nto suppress irrelevant features when fine-tuning on low-resource target tasks,\nand show that our method successfully reduces overfitting. Moreover, we show\nthat our VIB model finds sentence representations that are more robust to\nbiases in natural language inference datasets, and thereby obtains better\ngeneralization to out-of-domain datasets. Evaluation on seven low-resource\ndatasets in different tasks shows that our method significantly improves\ntransfer learning in low-resource scenarios, surpassing prior work. Moreover,\nit improves generalization on 13 out of 15 out-of-domain natural language\ninference benchmarks. Our code is publicly available in\nhttps://github.com/rabeehk/vibert.",
      "full_text": "Published as a conference paper at ICLR 2021 VARIATIONAL INFORMATION BOTTLENECK FOR EFFEC- TIVE LOW-RESOURCE FINE-TUNING Rabeeh Karimi Mahabadi♣♥ Y onatan Belinkov♦∗ James Henderson♣ ♥EPFL, Switzerland ♣Idiap Research Institute, Switzerland ♦Technion – Israel Institute of Technology {rabeeh.karimi,james.henderson}@idiap.ch belinkov@technion.ac.il ABSTRACT While large-scale pretrained language models have obtained impressive results when fine-tuned on a wide variety of tasks, they still often suffer from overfitting in low-resource scenarios. Since such models are general-purpose feature extractors, many of these features are inevitably irrelevant for a given target task. We propose to use V ariational Information Bottleneck (VIB) to suppress irrelevant features when fine-tuning on low-resource target tasks, and show that our method successfully reduces overfitting. Moreover, we show that our VIB model finds sentence representations that are more robust to biases in natural language inference datasets, and thereby obtains better generalization to out-of-domain datasets. Evaluation on seven low-resource datasets in different tasks shows that our method significantly improves transfer learning in low-resource scenarios, surpassing prior work. Moreover, it improves generalization on 13 out of 15 out-of-domain natural language inference benchmarks. Our code is publicly available inhttps://github.com/rabeehk/vibert. 1 I NTRODUCTION Transfer learning has emerged as the de facto standard technique in natural language processing (NLP), where large-scale language models are pretrained on an immense amount of text to learn a general-purpose representation, which is then transferred to the target domain with fine-tuning on target task data. This method has exhibited state-of-the-art results on a wide range of NLP benchmarks (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019). However, such pretrained models have a huge number of parameters, potentially making fine-tuning susceptible to overfitting. In particular, the task-universal nature of large-scale pretrained sentence representations means that much of the information in these representations is irrelevant to a given target task. If the amount of target task data is small, it can be hard for fine-tuning to distinguish relevant from irrelevant information, leading to overfitting on statistically spurious correlations between the irrelevant information and target labels. Learning low-resource tasks is an important topic in NLP (Cherry et al., 2019) because annotating more data can be very costly and time-consuming, and because in several tasks access to data is limited. In this paper, we propose to use the Information Bottleneck (IB) principle (Tishby et al., 1999) to address this problem of overfitting. More specifically, we propose a fine-tuning method that uses V ariational Information Bottleneck (VIB; Alemi et al. 2017) to improve transfer learning in low-resource scenarios. VIB addresses the problem of overfitting by adding a regularization term to the training loss that directly suppresses irrelevant information. As illustrated in Figure 1, the VIB component maps the sentence embedding from the pretrained model to a latent representationz, which is the only input to the task- specific classifier. The information that is represented inzis chosen based on the IB principle, namely that all the information about the input that is represented inzshould be necessary for the task. In particular, VIB directly tries to remove the irrelevant information, making it easier for the task classifier to avoid overfitting when trained on a small amount of data. We find that in low-resource scenarios, using VIB to suppress irrelevant features in pretrained sentence representations substantially improves accuracy on the target task. ∗Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. 1 arXiv:2106.05469v1  [cs.CL]  10 Jun 2021Published as a conference paper at ICLR 2021 Figure 1: VIBERT compresses the encoder’s sentence representationfϕ(x) into representationzwith mean µ(x) and eliminates irrelevant and redundant information through the Gaussian noise with varianceΣ(x). Removing unnecessary information from the sentence representation also implies removing redundant information. VIB tries to find the most concise representation which can still solve the task, so even if a fea- ture is useful alone, it may be removed if it isn’t useful when added to other features because it is redundant. We hypothesize that this provides a useful inductive bias for some tasks, resulting in better generalization to out-of-domain data. In particular, it has recently been demonstrated that annotation biases and artifacts in several natural language understanding benchmarks (Kaushik & Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019) allow models to exploit superficial shortcuts during training to per- form surprisingly well without learning the underlying task. However, models that rely on such superficial features do not generalize well to out-of-domain datasets, which do not share the same shortcuts (Belinkov et al., 2019a). We investigate whether using VIB to suppress redundant features in pretrained sentence embeddings has the effect of removing these superficial shortcuts and keeping the deep semantic features that are truly useful for learning the underlying task. We find that using VIB does reduce the model’s dependence on shortcut features and substantially improves generalization to out-of-domain datasets. We evaluate the effectiveness of our method on fine-tuning BERT (Devlin et al., 2019), which we call the VIBERT model (V ariational Information Bottleneck for Effective Low-Resource Fine-Tuning). On seven different datasets for text classification, natural language inference, similarity, and paraphrase tasks, VIBERT shows greater robustness to overfitting than conventional fine-tuning and other regularization techniques, improving accuracies on low-resource datasets. Moreover, on NLI datasets, VIBERT shows robustness to dataset biases, obtaining substantially better generalization to out-of-domain NLI datasets. Further analysis demonstrates that VIB regularization results in less biased representations. Our approach is highly effective and simple to implement, involving a small additional MLP classifier on top of the sentence embeddings. It is model agnostic and end-to-end trainable. In summary, we make the following contributions: 1) Proposing VIB for low-resource fine-tuning of large pretrained language models. 2) Showing empirically that VIB reduces overfitting, resulting in substantially improved accuracies on seven low-resource benchmark datasets against conventional fine-tuning and prior regularization techniques. 3) Showing empirically that training with VIB is more robust to dataset biases in NLI, resulting in significantly improved generalization to out-of-domain NLI datasets. To facilitate future work, we will release our code. 2 F INE-TUNING IN LOW-RESOURCE SETTINGS The standard fine-tuning paradigm starts with a large-scale pretrained model such as BERT, adds a task-specific output component which uses the pretrained model’s sentence representation, and trains this model end-to-end on the task data, fine-tuning the parameters of the pretrained model. As depicted in Figure 1, we propose to add a VIB component that controls the flow of information from the representations of the pretrained model to the output component. The goal is to address overfitting in resource-limited scenarios by removing irrelevant and redundant information from the pretrained representation. Problem Formulation We consider a general multi-class classification problem with a low-resource datasetD= {xi,yi}N i=1 consisting of inputsxi∈X, and labelsyi∈Y. We assume we are also given a large-scale pretrained encoderfϕ(.) parameterized byϕthat computes sentence embeddings for the input xi. Our goal is to fine-tunefϕ(.) on Dto maximize generalization. Information BottleneckTo specifically optimize for the removal of irrelevant and redundant information from the input representations, we adopt the Information Bottleneck principle. The objective of IB is to find a maximally compressed representationZof the input representationX(compression loss) that 2Published as a conference paper at ICLR 2021 maximally preserves information about the outputY (prediction loss),1 by minimizing: LIB = βI(X,Z)   Compression Loss −I(Z,Y )   Prediction Loss , (1) whereβ≥0 controls the balance between compression and prediction, andI(.,.) is the mutual information. Variational Information BottleneckAlemi et al. (2017) derive an efficient variational estimate of (1): LVIB =βE x [KL[pθ(z|x),r(z)]]+ E z∼pθ(z|x) [−logqφ(y|z)], (2) whereqφ(y|z) is a parametric approximation ofp(y|z), r(z) is an estimate of the prior probabilityp(z) of z, and pθ(z|x) is an estimate of the posterior probability ofz. During training, the compressed sentence representationzis sampled from the distributionpθ(z|x), meaning that a specific pattern of noise is added to the input of the output classifierqφ(y|z). Increasing this noise decreases the information conveyed by z. In this way, the VIB module can block the output classifierqφ(y|z) from learning to use specific information. At test time, the expected value ofzis used for predicting labels withqφ(y|z). We refer to the dimensionality ofzas K, which specifies the bottleneck size. Note that there is an interaction between decreasingKand increasing the compression by increasingβ(Shamir et al., 2010; Harremo¨es & Tishby, 2007). Kand βare hyper-parameters (Alemi et al., 2017). We consider parametric Gaussian distributions for priorr(z) and pθ(z|x) to allow an analytic computation for their Kullback-Leibler divergence,2 namely r(z) = N(z|µ0,Σ0) and pθ(z|x) = N(z|µ(x),Σ(x)), where µand µ0 are K−dimensional mean vectors, andΣ and Σ0 are diagonal covariance matrices. We use the reparameterization trick (Kingma & Welling, 2013) to estimate the gradients, namely z= µ(x)+Σ(x)⊙ϵ, where ϵ∼N(0,I). To compute the compressed sentence representationspθ(z|x), as shown in Figure 1, we first feed sentence embeddingsfϕ(x) through a shallow MLP . It is then followed by two linear layers, each withKhidden units to computeµ(x) and Σ(x) (after a softplus transform to ensure non-negativity). We also use another linear layer to approximateqφ(y|z). 3 E XPERIMENTS Datasets We evaluate the performance on seven different benchmarks for multiple tasks, in particular text classification, natural language inference, similarity, and paraphrase detection. For NLI, we experiment with two well-known NLI benchmarks, namely SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018). For text classification, we evaluate on two sentiment analysis datasets, namely IMDB (Maas et al., 2011) and Y elp2013 (YELP) (Zhang et al., 2015). We additionally evaluate on three low-resource datasets in the GLUE benchmark (Wang et al., 2019):3 paraphrase detection using MRPC (Dolan & Brockett, 2005), semantic textual similarity using STS-B (Cer et al., 2017), and textual entailment using RTE (Dagan et al., 2006). For the GLUE benchmark, SNLI, and Y elp, we evaluate on the standard validation and test splits. For MNLI, since the test sets are not available, we tune on the matched dev set and evaluate on the mismatched dev set (MNLI-M) or vice versa. See Appendix A for datasets statistics and Appendix B for hyper-parameters of all methods. Base Model We use the BERTBase (12 layers, 110M parameters) and BERTLarge (24 layers, 340M parameters) uncased (Devlin et al., 2019) implementation of Wolf et al. (2019) as our base models,4 known to work well for these tasks. We use the default hyper-parameters of BERT, i.e., we use a sequence length of 128, with batch size32. We use the stable variant of the Adam optimizer (Zhang et al., 2021; Mosbach et al., 2021) with the default learning rate of2e−5 through all experiments. We do not use warm-up or weight decay. Baselines We compare against prior regularization techniques, including previous state-of-the-art, Mixout: 1In this work,Z, X, and Y are random variables, andz, xand yare instances of these random variables. 2KL(N(µ0,Σ0)∥N(µ1,Σ1))= 1 2(tr(Σ−1 1 Σ0)+(µ1−µ0)TΣ−1 1 (µ1−µ0)−K+log(det(Σ1) det(Σ0))). 3We did not evaluate on WNLI and CoLA due to the irregularities in these datasets and the reported instability during the fine-tuninghttps://gluebenchmark.com/faq. 4To have a controlled comparison, all results are computed with this PyTorch implementation, which might slightly differ from the TensorFlow variant (Devlin et al., 2019). 3Published as a conference paper at ICLR 2021 • Dropout(Srivastava et al., 2014), a widely used stochastic regularization techniques used in multiple large-scale language models (Devlin et al., 2019; Y ang et al., 2019; V aswani et al., 2017) to mitigate overfitting. Following Devlin et al. (2019), we apply dropout on all layers of BERT. • Mixout(Lee et al., 2019) is a stochastic regularization technique inspired by Dropout with the goal of preventing catastrophic forgetting during fine-tuning. Mixout regularizes the learning to minimize the deviation of a fine-tuned model from the pretrained initialization. It replaces the model parameters with the corresponding value from the pretrained model with probabilityp. • Weight Decay (WD)is a common regularization technique to improve generalization (Krogh & Hertz, 1992). It regularizes the large weightswby adding a penalization termλ 2 ∥w∥to the loss, whereλis a hyperparameter specifying the strength of regularization. Chelba & Acero (2004) and Daum´e III (2007) adapt WD for fine-tuning of the pretrained models, and propose to replace this regularization term withλ∥w−w0∥, where w0 are the weights of the pretrained models. Recently, Lee et al. (2019) demonstrated that the latter formulation of WD works better for fine- tuning of BERT than conventional WD and can improve generalization on small training sets. 3.1 R ESULTS ON THE GLUE BENCHMARK Table 1 shows results on the low-resource datasets in GLUE.5 We find that a) Our VIBERT model substantially outperforms the baselines on all the datasets, demonstrating the effectiveness of the proposed method. b) Dropout decreases the performance on low-resource datasets. We conjecture that regularization techniques relying on stochasticity without considering the relevance to the output, in contrast to VIB, can make it more difficult for learning to extract relevant information from a small amount of data. Igl et al. (2019) observe similar effects in another application. c) Similar to the results of Zhang et al. (2021), we find less pronounced benefits of the previously suggested methods than the results originally published. This can be explained by using a more stable version of Adam (Zhang et al., 2021) suggested by the very recent work in our experiments, which decreases the added benefits of previously suggested regularization techniques on top of a stable optimizer. In contrast, our VIBERT model still substantially improves the results and surpasses the prior work in all settings for both BERTBase and BERTLarge models. Due to the computational overhead of BERTLarge, for the rest of this work, we stick to BERTBase. Table 1: Average results and standard deviation in parentheses over 3 runs on low-resource data in GLUE. ∆ shows the absolute difference between the results of the VIBERT model with BERT. MRPC STS-B RTE Model Accuracy F1 Pearson Spearman Accuracy BERTBase 87.80 (0.5) 83.20 (0.6) 84.93 (0.1) 83.53 (0.0) 67.93 (1.5) +Dropout (Srivastava et al., 2014) 87.33 (0.2) 81.90 (0.7) 84.33 (0.9) 82.73(1.0) 65.80 (1.5) +Mixout (Lee et al., 2019) 87.03 (0.2) 82.63 (0.3) 85.23 (0.4) 83.80(0.4) 67.70 (0.9) +WD (Lee et al., 2019) 87.57(0.2) 82.83(0.3) 85.0(0.3) 83.6(0.2) 68.63(1.3) VIBERTBase 89.23 (0.1) 85.23 (0.2) 87.63 (0.3) 86.50 (0.4) 70.53 (0.5) ∆ +1.43 +2.03 +2.7 +2.97 +2.6 BERTLarge 88.47 (0.7) 84.20 (1.3) 86.87 (0.2) 85.70 (0.1) 68.67 (0.8) +Dropout (Srivastava et al., 2014) 87.77 (0.4) 82.97 (0.2) 86.47 (0.1) 85.33 (0.2) 65.77 (0.6) +Mixout (Lee et al., 2019) 88.57 (0.7) 84.10 (1.1) 86.70 (0.2) 85.43 (0.3) 70.03 (1.0) +WD (Lee et al., 2019) 88.97(0.5) 84.87(0.4) 86.9(0.1) 85.67(0.1) 69.27(0.9) VIBERTLarge 89.10 (0.4) 85.13 (0.6) 87.53 (0.8) 86.40 (0.9) 71.37 (0.8) ∆ +0.63 +0.93 +0.66 +0.7 +2.7 Impact of Random Seeds:Following Dodge et al. (2020), we examine the choice of random seed and evaluate the performance of VIBERT and BERT by fine-tuning them across 50 random seeds on GLUE. To comply with the limited access to the GLUE benchmark online system, we split the original validation sets into half and consider one half as the validation set and use the other half as the test set. We first perform model selection on the validation set to fix the hyper-parameters and then fine-tune the selected models for 50 different seeds. Figure 2 shows the expected test performance (Dodge et al., 2019) as the function 5Note that the test sets are not publicly available and the prior work reports the results on the validation set of the GLUE benchmark (Lee et al., 2019; Dodge et al., 2020). We, however, report the results of their methods and ours on the original test sets by submitting to an online system. 4Published as a conference paper at ICLR 2021 1 10 20 30 40 50 # of Random Seeds 83 84 85 86 87 88 89Exp. Test Acc. MRPC VIBERT BERT 1 10 20 30 40 50 # of Random Seeds 88.2 88.4 88.6 88.8 89.0 89.2 89.4 89.6 Exp. Test PCC. STS-B VIBERT BERT 1 10 20 30 40 50 # of Random Seeds 66 68 70 72 74 76Exp. Test Acc. RTE VIBERT BERT Figure 2: Expected test performance (solid lines) with standard deviation (shaded region) over the number of random seeds allocated for fine-tuning. Our VIBERT model consistently outperforms BERT. We report the accuracy for RTE and MRPC and the Pearson correlation coefficient for STS-B. of random trials. The results demonstrate that our VIBERT model consistently obtains better performance than BERT on all datasets. As anticipated, the expected test performance monotonically increases with more random trials (Dodge et al., 2020) till it reaches a plateau, such as after 30 trials on STS-B. 3.2 V ARYING-RESOURCE RESULTS To analyze the performance of our method as a function of dataset size, we use four large-resource NLI and sentiment analysis datasets, namely SNLI, MNLI, IMDB, and YELP to be able to subsample the training data with varying sizes. Table 2 shows the obtained results. VIBERT consistently outperforms all the baselines on low-resource scenarios, but the advantages are reduced or eliminated as we approach a medium-resource scenario. Also, the improvements are generally larger when the datasets are smaller, showing that our method successfully addresses low-resource scenarios. Table 2: Test accuracies in the low-resource setting on text classification and NLI datasets under varying sizes of training data (200, 500, 800, 1000, 3000, and 6000 samples). We report the average and standard deviation in parentheses across three runs. We show the highest average result in each setting in bold. ∆ shows the absolute difference between the results of VIBERT with BERT. Data Model 200 500 800 1000 3000 6000 SNLI BERT 58.70 (1.3) 68.12 (1.5) 73.29 (0.9) 74.69 (1.1) 79.57 (0.4) 80.85 (0.4) +Dropout 58.95 (0.4) 69.33 (1.1) 73.22 (1.2) 74.20 (0.5) 79.48 (0.7) 81.71(0.6) +Mixout 58.52 (1.3) 68.26 (1.7) 72.81 (1.0) 74.09 (0.5) 78.7 (0.3) 80.61 (0.5) +WD 59.23 (1.5) 68.54 (1.9) 73.72 (1.0) 74.78 (0.8) 79.83(0.5) 81.32 (0.5) VIBERT 61.42(1.3) 70.75(0.6) 74.71(0.5) 75.84(0.1) 79.56 (0.3) 81.29 (0.4) ∆ +2.72 +2.63 +1.42 +1.15 -0.01 +0.44 MNLI BERT 49.93 (1.4) 59.76 (2.0) 63.63 (1.6) 65.21 (1.4) 70.67 (0.7) 73.11 (0.9) +Dropout 50.74 (2.1) 59.58 (2.1) 62.82 (0.8) 65.71 (1.4) 71.11 (0.8) 72.88 (1.1) +Mixout 50.05 (1.8) 58.69 (2.8) 63.31 (1.7) 64.58 (1.5) 70.60 (0.8) 72.56 (0.7) +WD 49.92 (1.4) 60.36 (2.0) 64.41 (1.5) 65.3 (1.0) 71.47 (0.8) 72.94 (0.7) VIBERT 53.58(0.9) 63.04(1.1) 64.87(0.6) 66.41(1.2) 71.86(0.9) 74.22(0.3) ∆ +3.65 +3.28 +1.24 +1.2 +1.19 +1.11 IMDB BERT 78.96 (1.9) 83.68 (0.2) 84.04 (0.9) 84.80 (0.0) 86.17 (0.2) 86.98 (0.4) +Dropout 81.19 (1.6) 83.30 (0.2) 84.52 (0.3) 85.01 (0.3) 86.20 (0.2) 87.31(0.2) +Mixout 79.17 (4.2) 83.55 (0.3) 84.37 (0.3) 84.50 (0.1) 86.15 (0.1) 86.97 (0.1) +WD 79.78 (2.2) 83.95 (0.2) 84.29 (0.6) 84.97 (0.2) 86.13 (0.3) 87.2 (0.1) VIBERT 83.05(0.3) 84.46(0.4) 84.83(0.4) 85.03(0.4) 86.27(0.4) 87.15 (0.3) ∆ +4.09 +0.78 +0.79 +0.23 +0.1 +0.17 YELP BERT 41.60 (0.9) 44.12 (1.4) 45.67 (1.6) 46.77 (0.5) 50.14 (0.7) 51.86 (0.4) +Dropout 41.30 (0.3) 44.37 (0.6) 46.49 (0.8) 46.21 (1.5) 51.09(0.2) 52.39(0.5) +Mixout 41.52 (0.9) 43.60 (1.1) 45.65 (1.9) 46.98 (1.1) 50.68 (0.5) 51.51 (0.3) +WD 41.66 (0.6) 44.43 (1.2) 46.26 (1.4) 47.37 (0.6) 50.7 (0.5) 51.9 (0.6) VIBERT 42.30(0.2) 46.65(0.5) 46.60(0.1) 48.03(0.6) 50.37 (0.4) 51.34 (0.4) ∆ +0.7 +2.53 +0.93 +1.26 +0.23 -0.52 5Published as a conference paper at ICLR 2021 3.3 O UT-OF-DOMAIN GENERALIZATION Besides improving fine-tuning on low-resource data by removing irrelevant features, we expect VIB to improve on out-of-domain data because it removes redundant features. In particular, annotation artifacts create shortcut features, which are superficial cues correlated with a label (Gururangan et al., 2018; Poliak et al., 2018) that do not generalize well to out-of-domain datasets (Belinkov et al., 2019a). Since solving the real underlying task can be done without these superficial shortcuts, they must be redundant with the deep semantic features that are truly needed. We hypothesize that many more superficial shortcut features are needed to reach the same level of performance as a few deep semantic features. If so, then VIB should prefer to keep the concise deep features and remove the abundant superficial features, thus encouraging the classifier to rely on the deep semantic features, and therefore resulting in better generalization to out- of-domain data. To evaluate out-of-domain generalization, we take NLI models trained on medium-sized 6K subsampled SNLI and MNLI in Section 3.2 and evaluate their generalization on several NLI datasets. Datasets: We consider a total of 15 different NLI datasets used in Mahabadi et al. (2020), including SICK (Marelli et al., 2014), ADD1 (Pavlick & Callison-Burch, 2016), JOCI (Zhang et al., 2017), MPE (Lai et al., 2017), MNLI, SNLI, SciTail (Khot et al., 2018), and three datasets from White et al. (2017) namely DPR (Rahman & Ng, 2012), FN+ (Pavlick et al., 2015), SPR (Reisinger et al., 2015), and Quora Question Pairs (QQP) interpreted as an NLI task as by Gong et al. (2017). We use the same split used in Wang et al. (2017). We also consider SNLI hard and MNLI(-M) Hard sets (Gururangan et al., 2018), a subset of SNLI/MNLI(-M) where a hypothesis-only model cannot correctly predict the labels and the known biases are avoided. Since the target datasets have different label spaces, during the evaluation, we map predictions to each target dataset’s space (Appendix C). Following prior work (Belinkov et al., 2019a; Mahabadi et al., 2020), we select hyper-parameters based on the development set of each target dataset and report the results on the test set. Results: Table 3 shows the results of VIBERT and BERT. We additionally include WD, the baseline that performed the best on average on SNLI and MNLI in Table 2. On models trained on SNLI, VIBERT improves the transfer on 13 out of 15 datasets, obtaining a substantial average improvement of 5.51 points. The amount of improvement on different datasets varies, with the largest improvement on SPR and SciTail with +15.5, and +12.5 points respectively, while WD on average obtains only 0.99 points improvement. On models trained on MNLI, VIBERT improves the transfer on 13 datasets, obtaining an average improvement of 3.83 points. The improvement varies across the datasets, with the largest on ADD1 and JOCI with 16.8 and 8.3 points respectively, substantially surpassing WD. Interestingly, VIBERT improves the results on the SNLI and MNLI(-M) hard sets, resulting in models that are more robust to known biases. These results support our claim that VIBERT motivates learning more general features, rather than redundant superficial features, leading to an improved generalization to datasets without these superficial biases. In the next section, we analyze this phenomenon more. Table 3: Test accuracy of models transferring to new target datasets. All models are trained on SNLI or MNLI and tested on the target datasets.∆ are absolute differences with BERT. SNLI MNLI Data BERT VIBERT ∆ WD ∆ BERT VIBERT ∆ WD ∆ SICK 48.47 54.68 +6.2 48.37 -0.1 59.16 69.17 +10.0 63.87 +4.7 ADD1 78.81 84.75 +5.9 80.62 +1.8 66.15 82.95 +16.8 67.18 +1.0 DPR 50.78 50.14 -0.6 50.41 -0.4 49.95 49.95 0.0 49.95 0. SPR 50.21 65.68 +15.5 51.90 +1.7 59.16 65.61 +6.5 57.21 -1.9 FN+ 50.78 53.44 +2.7 50.58 -0.2 46.28 49.94 +3.7 46.34 +0.1 JOCI 42.03 50.66 +8.6 43.91 +1.9 45.60 53.94 +8.3 46.49 +0.9 MPE 58.30 58.10 -0.2 58.10 -0.2 55.10 50.30 -4.8 58.2 +3.1 SCITAIL 62.32 74.84 +12.5 65.10 +2.8 72.58 75.68 +3.1 75.73 +3.2 QQP 65.19 70.67 +5.5 65.90 +0.7 67.88 70.50 +2.6 68.75 +0.9 SNLI Hard 65.72 68.35 +2.6 66.82 +1.1 56.98 60.29 +3.3 57.8 +0.8 MNLI Hard 46.31 53.17 +6.9 47.42 +1.1 59.74 61.19 +1.4 60.08 +0.3 MNLI-M Hard 46.12 52.38 +6.3 46.82 +0.7 60.55 61.03 +0.5 59.77 -0.8 SNLI 80.54 81.81 +1.3 81.26 +0.7 64.32 67.87 +3.6 65.44 +1.1 MNLI-M 60.51 64.88 +4.4 62.11 +1.6 72.42 73.06 +0.6 72.76 +0.3 MNLI 61.79 66.76 +5.0 63.42 +1.6 72.73 74.67 +1.9 72.89 +0.2 Average — — +5.51 — +0.99 — — +3.83 — +0.93 6Published as a conference paper at ICLR 2021 4 A NALYSIS Analysis of the Removed FeaturesElazar & Goldberg (2018) propose a challenging framework to evaluate if debiasing methods have succeeded in removing biases from the sentence representation. After debiasing, the trained encoder is frozen and the classifier is retrained to try to extract the biases. If the classifier reaches high accuracy given only bias features, then the encoder’s representation has not been successfully debiased. We follow the framework of Elazar & Goldberg (2018) to analyze whether known biases in NLI data have been removed in the trained sentence representations. In particular, following Belinkov et al. (2019b), we train a classifier which only sees the representation of the hypothesis sentence and see if it can predict the class of the sentence pair, which is an established criterion to measure known biases in NLI datasets (Gururangan et al., 2018). Thus, we freeze the trained encoders from our model and the BERT baseline and retrain a hypothesis-only classifier on hypotheses from the SNLI and MNLI datasets.6 For reference, we compare to a hypothesis-only model with a BERT encoder trained end-to-end. Table 4 shows the results. With the baseline (BERT), the retrained classifier is not able to recapture all the biases (H-only), but it captures much more than with our method (VIBERT). VIBERT is so successful at reducing biases that performance of the hypothesis-only classifier is close to chance (33%). Table 4: Hypothesis-only accuracy when freezing the encoder from models trained on SNLI/MNLI in Table 2 and retraining a hypothesis-only classifier (BERT, VIBERT), and baseline results when the encoder is not frozen (H-only). Lower results show more successful debiasing. Model SNLI MNLI Train Dev Test Train Dev Test H-only 81.3 61.89 62.17 87.15 53.46 53.63 BERT 66.40 53.73 53.17 58.5 44.68 44.03 VIBERT 38.20 36.65 37.10 42.03 36.43 35.75 Impact of VIB on OverfittingTo analyze the effect of VIB on reducing overfitting, we analyze the effect of theβparameter on training and validation error sinceβcontrols the trade-off between removing information from the sentence embedding (highβ) and keeping information that is predictive of the output (lowβ). We fix the bottleneck size (K) based on the models selected in Section 3.1, and we train VIBERT on the GLUE benchmark for varying values ofβand plot the validation and training loss in Figure 3. For small values ofβ, where VIB has little effect, the validation loss is substantially higher than the training loss, indicating overfitting. This is because the network learns to be more deterministic (Σ≈0), thereby retaining too much irrelevant information. As we increaseβ, where VIB has an effect, we observe better gen- eralization performance with less overfitting. Asβbecomes too large, both the training and validation losses shoot up because the amount of preserved information is insufficient to differentiate between the classes. This pattern is observable in the MRPC and RTE datasets, with a similar pattern in the STS-B dataset. 10 6  10 3  100 103 0.0 0.5 1.0CE Loss MRPC Dev Train 10 6  10 3  100 103 0 1 2CE Loss RTE Dev Train 10 6  10 3  100 103 0 1 2MSE Loss STS-B Dev Train Figure 3: V alidation and training losses of VIBERT for varyingβand a fixed bottleneck size on GLUE. Efficiency Evaluation Table 5 presents the efficiency evaluation in terms of memory, number of parameters, and time for all the methods measured on RTE. Our approach has several attractive properties. First, while our method is slightly larger in terms of parameters compared to the other standard regularization approaches due to an additional MLP layer (Figure 1), the difference is still marginal, and for BERTBase model with 109.48M trainable parameters, that is less than 1.22% more parameters. Second, 6Note that with VIBERT, the frozen encoderpθ(z|x) outputs a distribution, and the hypothesis-only classifier is trained on samples from this distribution. 7Published as a conference paper at ICLR 2021 Table 5: Performance evaluation for all methods.∆% are relative differences with BERT. Model Memory ∆% #Parameters ∆% Time ∆% BERT 290.91 GB — 109.48 M — 4.50 min — +Mixout 407.65 GB 40.13 % 109.48 M 0% 5.15 min 14.44% +WD 331.78 GB 14.05% 109.48 M 0% 4.91 min 9.11% +Dropout 290.91 GB 0% 109.48 M 0% 4.68 min 4% VIBERT 292.57 GB 0.57 % 110.83 M 1.22% 4.67 min 3.77% our approach presents a much better memory usage with low-overhead, close to Dropout, while WD and especially Mixout cause substantial memory overhead. In dealing with large-scale transformer models like BERT, efficient memory usage is of paramount importance. Third, in terms of training time, our method is similar to Dropout and much faster than the other two baselines. Relative to BERT, VIBERT increases the training time by 3.77%, while WD and Mixout cause the substantial training overhead of 9.11% and 14.44%. Note that our method and other baselines require hyper-parameter tuning. Ablation Study As an ablation, Table 6 shows results for our model without the compression loss (VIBERT (β=0)), in which case there is no incentive to introduce noise, and the VIB layer reduces to deterministic dimensionality reduction with an MLP . We optimize the dimensionality of the MLP layer (K) as a hyper-parameter for both methods. This ablation does reduce performance on all considered datasets, demonstrating the added benefit of the compression loss of VIBERT. Table 6: Average ablation results over 3 runs with std in parentheses on GLUE. BERT and VIBERT’s results are from Table 1. MRPC STS-B RTE Model Accuracy F1 Pearson Spearman Accuracy BERT 87.80 (0.5) 83.20 (0.6) 84.93 (0.1) 83.53 (0.0) 67.93 (1.5) VIBERT (β=0) 88.57 (0.6) 84.27 (0.7) 87.10 (0.4) 86.00 (0.5) 69.63 (1.3) VIBERT 89.23(0.1) 85.23(0.2) 87.63(0.3) 86.50(0.4) 70.53(0.5) 5 R ELATED WORK Low-resource Setting Recently, developing methods for low-resource NLP has gained attention (Cherry et al., 2019). Prior work has investigated improving on low-resource datasets by injecting large unlabeled in-domain data and pretraining a unigram document model using a variational autoencoder and use its internal representations as features for downstream tasks (Gururangan et al., 2019). Other approaches propose injecting a million-scale previously collected phrasal paraphrase relations (Arase & Tsujii, 2019) and data augmentation for translation task (Fadaee et al., 2017). Due to relying on the additional source and in-domain corpus, such techniques are not directly comparable to our model. Information Bottleneck IB has recently been adopted in NLP in applications such as parsing (Li & Eisner, 2019), and summarization (West et al., 2019). V oita et al. (2019) use the mutual information to study how token representations evolve across layers of a Transformer model (V aswani et al., 2017). This paper – to the best of our knowledge – is the first attempt to study VIB as a regularization technique to improve the fine-tuning of large-scale language models on low-resource scenarios. Regularization Techniques for Fine-tuning Language modelsIn addition to references given throughout, Phang et al. (2018) proposed to perform an extra data-rich intermediate supervised task pretraining followed by fine-tuning on the target task. They showed that their method leads to improved fine-tuning performance on the GLUE benchmark. However, their method requires pretraining with a large intermediate task. In contrast, our goal is to use only the provided low-resource target datasets. 8Published as a conference paper at ICLR 2021 6 C ONCLUSION AND FUTURE DIRECTIONS We propose VIBERT, an effective model to reduce overfitting when fine-tuning large-scale pretrained lan- guage models on low-resource datasets. By leveraging a VIB objective, VIBERT finds the simplest sentence embedding, predictive of the target labels, while removing task-irrelevant and redundant information. Our approach is model agnostic, simple to implement, and highly effective. Extensive experiments and analyses show that our method substantially improves transfer performance in low-resource scenarios. We demon- strate our obtained sentence embeddings are robust to biases and our model results in a substantially better generalization to out-of-domain NLI datasets. Future work includes exploring incorporating VIB on mul- tiple layers of pretrained language models and using it to jointly learn relevant features and relevant layers. ACKNOWLEDGEMENTS We would like to thank Maksym Andriushchenko for his helpful comments. Rabeeh Karimi was supported by the Swiss National Science Foundation under the project Learning Representations of Abstraction for Opinion Summarisation (LAOS), grant number “FNS-30216”. Y onatan Belinkov was supported by the ISRAEL SCIENCE FOUNDA TION (grant No. 448/20). REFERENCES Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017. Y uki Arase and Jun’ichi Tsujii. Transfer fine-tuning: A bert case study. InEMNLP, 2019. Y onatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin V an Durme, and Alexander Rush. Don’t take the premise for granted: Mitigating artifacts in natural language inference. InACL, 2019a. Y onatan Belinkov, Adam Poliak, Stuart M Shieber, Benjamin V an Durme, and Alexander M Rush. On adversarial removal of hypothesis-only bias in natural language inference. InSEM, 2019b. Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. InCoNLL, 2016. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. InEMNLP, 2015. Daniel Cer, Mona Diab, Eneko Agirre, I˜nigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. InSemEval, 2017. Ciprian Chelba and Alex Acero. Adaptation of maximum entropy capitalizer: Little data can help a lot. In EMNLP, 2004. Colin Cherry, Greg Durrett, George Foster, Reza Haffari, Shahram Khadivi, Nanyun Peng, Xiang Ren, and Swabha Swayamdipta (eds.).DeepLo, 2019. URL https://www.aclweb.org/anthology/ D19-6100. Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge. In MLCW, 2006. Hal Daum´e III. Frustratingly easy domain adaptation. InACL, 2007. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. InNAACL, 2019. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A Smith. Show your work: Improved reporting of experimental results. InEMNLP-IJCNLP, 2019. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. arXiv:2002.06305, 2020. 9Published as a conference paper at ICLR 2021 William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In IWP, 2005. Y anai Elazar and Y oav Goldberg. Adversarial removal of demographic attributes from text data. In EMNLP, 2018. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. Data augmentation for low-resource neural machine translation. InACL, 2017. Yichen Gong, Heng Luo, and Jian Zhang. Natural language inference over interaction space. InICLR, 2017. Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. InNAACL, 2018. Suchin Gururangan, Tam Dang, Dallas Card, and Noah A Smith. V ariational pretraining for semi-supervised text classification. InACL, 2019. Peter Harremo¨es and Naftali Tishby. The information bottleneck revisited or how to choose a good distortion measure. InISIT, 2007. Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, and Katja Hofmann. Generalization in reinforcement learning with selective noise injection and information bottleneck. InNeurIPS, 2019. Divyansh Kaushik and Zachary C Lipton. How much reading does reading comprehension require? a critical investigation of popular benchmarks. InEMNLP, 2018. Tushar Khot, Ashish Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science question answering. InAAAI, 2018. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. InICLR, 2013. Anders Krogh and John A Hertz. A simple weight decay can improve generalization. InNeurIPS, 1992. Alice Lai, Y onatan Bisk, and Julia Hockenmaier. Natural language inference from multiple premises. In IJCNLP, 2017. Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to finetune large-scale pretrained language models. InICLR, 2019. Xiang Lisa Li and Jason Eisner. Specializing word embeddings (for parsing) by information bottleneck. In EMNLP, 2019. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and V eselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692, 2019. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher Potts. Learning word vectors for sentiment analysis. InACL, 2011. Karimi Rabeeh Mahabadi, Y onatan Belinkov, and James Henderson. End-to-end bias mitigation by modelling biases in corpora. InACL, 2020. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zam- parelli. A sick cure for the evaluation of compositional distributional semantic models. InLREC, 2014. Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines.ICLR, 2021. Ellie Pavlick and Chris Callison-Burch. Most “babies” are “little” and most “problems” are “huge”: Compositional entailment in adjective-nouns. InACL, 2016. Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi, Chris Callison-Burch, Mark Dredze, and Benjamin V an Durme. Framenet+: Fast paraphrastic tripling of framenet. InACL, 2015. 10Published as a conference paper at ICLR 2021 Jason Phang, Thibault F´evry, and Samuel R Bowman. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks.arXiv:1811.01088, 2018. Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin V an Durme. Hypothesis only baselines in natural language inference. InSEMEVAL, 2018. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Altaf Rahman and Vincent Ng. Resolving complex cases of definite pronouns: the winograd schema challenge. InEMNPL, 2012. Drew Reisinger, Rachel Rudinger, Francis Ferraro, Craig Harman, Kyle Rawlins, and Benjamin V an Durme. Semantic proto-roles. InTACL, 2015. Tal Schuster, Darsh J Shah, Y un Jie Serene Y eo, Daniel Filizzola, Enrico Santus, and Regina Barzilay. Towards debiasing fact verification models. InEMNLP, 2019. Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information bottleneck. InTCS, 2010. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. InJMLR, 2014. Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. InAllerton, 1999. Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. InNeurIPS, 2017. Elena V oita, Rico Sennrich, and Ivan Titov. The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives. InEMNLP-IJCNLP, 2019. Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. InICLR, 2019. Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language sentences. InIJCAI, 2017. Peter West, Ari Holtzman, Jan Buys, and Y ejin Choi. Bottlesum: Unsupervised and self-supervised sentence summarization using the information bottleneck principle. InEMNLP, 2019. Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin V an Durme. Inference is everything: Recasting semantic resources into a unified evaluation framework. InIJCNLP, 2017. Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. InNAACL, 2018. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface’s transformers: State-of-the-art natural language processing.arXiv:1910.03771, 2019. Zhilin Y ang, Zihang Dai, Yiming Y ang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. InNeurIPS, 2019. Sheng Zhang, Rachel Rudinger, Kevin Duh, and Benjamin V an Durme. Ordinal common-sense inference. In TACL, 2017. Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Y oav Artzi. Revisiting few-sample bert fine-tuning.ICLR, 2021. Xiang Zhang, Junbo Zhao, and Y ann LeCun. Character-level convolutional networks for text classification. In NeurIPS, 2015. 11Published as a conference paper at ICLR 2021 A E XPERIMENTAL DETAILS Datasets Statistics Table 7 shows the statistics of the datasets used in our experiments. Table 7: Datasets used in our experiments. Dataset #Labels Train Val. Test Single-Sentence Tasks IMDB 2 20K 5K 25K YELP 5 62.5K 7.8K 8.7K Inference Tasks SNLI 3 550K 10K 10K MNLI 3 393K 9.8K 9.8K RTE 2 2.5K 0.08K 3K Similarity and Paraphrase Tasks MRPC 2 3.7K 0.4K 1.7K STS-B 1 (Similarity score) 5.8K 1.5K 1.4K Computing InfrastructureWe run all experiments on one GTX1080Ti GPU with 11 GB of RAM. VIBERT Architecture The MLP module used to compute the compressed sentence representations (Figure 1) is a shallow MLP with768, 2304+K 4 , 768+K 2 hidden units with a ReLU non-linearity, where K is the bottleneck size. Following Alemi et al. (2017), we average over 5 posterior samples, i.e., we computep(y|x)= 1 5 Σ5 i=1qφ(y|zi), wherezi∼pθ(z|x). Similar to Bowman et al. (2016), we use a linear annealing schedule forβand set it asmin(1,epoch×β0) in each epoch, whereβ0 is the initial value. B H YPER-PARAMETERS The GLUE Benchmark ExperimentResults on GLUE benchmark are reported in Table 1. We fine-tune all the models for 6 epochs to allow them to converge. We use early stopping for all models by choosing the model performing the best on the validation set with the evaluation criterion of average F1 and accuracy for MRPC, accuracy for RTE, and average Pearson and Spearman correlations for STS-B. For VIBERT, we sweep β over {10−4,10−5,10−6}and K over {144,192,288,384}. For dropout, we use dropping probabilities of {0.25,0.45,0.65,0.85}. For Mixout, we consider mixout probability of{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}. For WD, we consider weight decay of {10−6,10−5,10−4,10−3,10−2,10−1,1}. Varying-resource ExperimentResults on varying sizes of training data are reported in Table 2. We fine-tune all models for 25 epochs to allow them to converge. We use early stopping for all models based on the performance on the validation set. We also perform hyper-parameter tuning on the validation set. Since we consider datasets of a different number of training samples, we need to account for a suitable range of bottleneck size and we sweepK over {12,18,24,36,48,72,96,144,192,288,384}and β over {10−4,10−5}. For dropout, we consider dropping probabilities of{0.25,0.45,0.65,0.85}. For Mixout, we consider mixout probability of{0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9}. For WD, we consider weight decay of{10−6,10−5,10−4,10−3,10−2,10−1,1}. Ablation Experiment Ablation results are shown in Table 6. For VIBERT (β=0), we sweepKover the same range of values as VIBERT, i.e.,{144,192,288,384} C M APPING We train all models on SNLI or MNLI datasets and evaluate their performance on other target datasets. The SNLI and MNLI datasets contain three labels of contradiction, neutral, and entailment. However, 12Published as a conference paper at ICLR 2021 some of the considered target datasets have only two labels, such as DPR or SciTail. When the target dataset has two labels ofentailedand not-entailed, as in DPR, we consider the predicted contradiction and neutral labels as the not-entailed label. In the case the target dataset has two labels ofentailmentand neutral, as in SciTail, we consider the predicted contradiction label as neutral. 13",
      "meta_data": {
        "arxiv_id": "2106.05469v1",
        "authors": [
          "Rabeeh Karimi Mahabadi",
          "Yonatan Belinkov",
          "James Henderson"
        ],
        "published_date": "2021-06-10T03:08:13Z",
        "pdf_url": "https://arxiv.org/pdf/2106.05469v1.pdf",
        "github_url": "https://github.com/rabeehk/vibert"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the problem of overfitting in large-scale pretrained language models when fine-tuned on low-resource scenarios. It proposes a Variational Information Bottleneck (VIB) based method, VIBERT, to suppress irrelevant and redundant features in sentence representations. Key findings include VIBERT significantly improving transfer learning accuracy on seven low-resource benchmark datasets (text classification, NLI, similarity, paraphrase tasks) compared to conventional fine-tuning and prior regularization techniques. Additionally, it enhances generalization to out-of-domain natural language inference (NLI) datasets by fostering representations more robust to annotation biases, demonstrating improved performance on 13 out of 15 NLI benchmarks.",
        "methodology": "The proposed method integrates the Variational Information Bottleneck (VIB) principle into the fine-tuning process of pretrained language models like BERT. VIB minimizes an objective function that balances compression of the input representation (X) with preservation of information about the output (Y), effectively suppressing irrelevant and redundant features. Specifically, VIBERT maps the pretrained model's sentence embedding (fϕ(x)) to a latent representation (z) through a shallow Multi-Layer Perceptron (MLP). This MLP is followed by two linear layers that compute the mean (µ(x)) and diagonal covariance (Σ(x)) of a Gaussian distribution pθ(z|x), from which 'z' is sampled using the reparameterization trick. A task-specific classifier then uses 'z' as its only input. During training, noise is introduced through the sampling process to block the classifier from learning specific, potentially irrelevant information; at test time, the expected value of 'z' is used. The method is model-agnostic, end-to-end trainable, and adds a marginal increase in parameters and training time.",
        "experimental_setup": "The effectiveness of VIBERT was evaluated on BERTBase and BERTLarge models. Experiments covered seven low-resource datasets from multiple tasks: text classification (IMDB, Yelp2013), natural language inference (SNLI, MNLI, RTE), similarity (STS-B), and paraphrase detection (MRPC). For out-of-domain generalization, 15 NLI datasets were used, including SICK, ADD1, JOCI, MPE, SciTail, DPR, FN+, SPR, QQP, SNLI Hard, and MNLI(-M) Hard. The base model was BERT uncased (Devlin et al., 2019) implemented by Wolf et al. (2019), with a sequence length of 128 and batch size of 32. A stable variant of the Adam optimizer with a learning rate of 2e-5 was used. Baselines included Dropout, Mixout, and a modified Weight Decay. Hyperparameters, including the VIB bottleneck size (K) and beta (β) for compression, were tuned on validation sets. The models were fine-tuned for 6 or 25 epochs with early stopping. Efficiency was evaluated on memory, number of parameters, and training time on RTE. All experiments were conducted on a single GTX1080Ti GPU.",
        "limitations": "The primary limitation identified is that while VIBERT consistently outperforms baselines in low-resource scenarios, its advantages are reduced or eliminated as the training data size approaches a medium-resource scenario. Although VIBERT introduces a small additional MLP layer, it results in a marginal increase in the number of parameters (less than 1.22% for BERTBase) and a slight increase in training time (3.77% relative to BERT) compared to not using the method, though it is more memory efficient than some baselines.",
        "future_research_directions": "Future research could explore incorporating the Variational Information Bottleneck (VIB) on multiple layers of pretrained language models. Another promising direction is to use VIB to jointly learn both relevant features and relevant layers within the language models.",
        "experimental_code": "import torch\\nfrom torch import nn\\nimport torch.nn.functional as F\\nfrom torch.nn import CrossEntropyLoss, MSELoss\\nfrom transformers.modeling_bert import BertPreTrainedModel, BertModel\\n\\nclass BertForSequenceClassification(BertPreTrainedModel):\\n    def __init__(self, config):\\n        super().__init__(config)\\n        self.num_labels = config.num_labels\\n        self.bert = BertModel(config)\\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\\n        self.deterministic = config.deterministic\\n        self.ib_dim = config.ib_dim\\n        self.ib = config.ib\\n        self.activation = config.activation\\n        self.activations = {'tanh': nn.Tanh(), 'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid()}\\n        if self.ib or self.deterministic:\\n            self.kl_annealing = config.kl_annealing\\n            self.hidden_dim = config.hidden_dim\\n            intermediate_dim = (self.hidden_dim+config.hidden_size)//2\\n            self.mlp = nn.Sequential(\\n                nn.Linear(config.hidden_size, intermediate_dim),\\n                self.activations[self.activation],\\n                nn.Linear(intermediate_dim, self.hidden_dim),\\n                self.activations[self.activation])\\n            self.beta = config.beta\\n            self.sample_size = config.sample_size\\n            self.emb2mu = nn.Linear(self.hidden_dim, self.ib_dim)\\n            self.emb2std = nn.Linear(self.hidden_dim, self.ib_dim)\\n            self.mu_p = nn.Parameter(torch.randn(self.ib_dim))\\n            self.std_p = nn.Parameter(torch.randn(self.ib_dim))\\n            self.classifier = nn.Linear(self.ib_dim, self.config.num_labels)\\n        else:\\n            self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\\n\\n        self.init_weights()\\n\\n    def estimate(self, emb, emb2mu, emb2std):\\n        \"\"\"Estimates mu and std from the given input embeddings.\"\"\"\\n        mean = emb2mu(emb)\\n        std = torch.nn.functional.softplus(emb2std(emb))\\n        return mean, std\\n\\n    def kl_div(self, mu_q, std_q, mu_p, std_p):\\n        \"\"\"Computes the KL divergence between the two given variational distribution.\\n           This computes KL(q||p), which is not symmetric. It quantifies how far is\\n           The estimated distribution q from the true distribution of p.\"\"\"\\n        k = mu_q.size(1)\\n        mu_diff = mu_p - mu_q\\n        mu_diff_sq = torch.mul(mu_diff, mu_diff)\\n        logdet_std_q = torch.sum(2 * torch.log(torch.clamp(std_q, min=1e-8)), dim=1)\\n        logdet_std_p = torch.sum(2 * torch.log(torch.clamp(std_p, min=1e-8)), dim=1)\\n        fs = torch.sum(torch.div(std_q ** 2, std_p ** 2), dim=1) + torch.sum(torch.div(mu_diff_sq, std_p ** 2), dim=1)\\n        kl_divergence = (fs - k + logdet_std_p - logdet_std_q)*0.5\\n        return kl_divergence.mean()\\n\\n    def reparameterize(self, mu, std):\\n        batch_size = mu.shape[0]\\n        z = torch.randn(self.sample_size, batch_size, mu.shape[1]).cuda()\\n        return mu + std * z\\n\\n    def get_logits(self, z, mu, sampling_type):\\n        if sampling_type == \"iid\":\\n            logits = self.classifier(z)\\n            mean_logits = logits.mean(dim=0)\\n            logits = logits.permute(1, 2, 0)\\n        else:\\n            mean_logits = self.classifier(mu)\\n            logits = mean_logits\\n        return logits, mean_logits\\n\\n\\n    def sampled_loss(self, logits, mean_logits, labels, sampling_type):\\n        if sampling_type == \"iid\":\\n            # During the training, computes the loss with the sampled embeddings.\\n            if self.num_labels == 1:\\n                #  We are doing regression\\n                loss_fct = MSELoss()\\n                loss = loss_fct(logits.view(-1, self.sample_size), labels[:, None].float().expand(-1, self.sample_size))\\n                loss = torch.mean(loss, dim=-1)\\n                loss = torch.mean(loss, dim=0)\\n            else:\\n                loss_fct = CrossEntropyLoss(reduce=False)\\n                loss = loss_fct(logits, labels[:, None].expand(-1, self.sample_size))\\n                loss = torch.mean(loss, dim=-1)\\n                loss = torch.mean(loss, dim=0)\\n        else:\\n            # During test time, uses the average value for prediction.\\n            if self.num_labels == 1:\\n                loss_fct = MSELoss()\\n                loss = loss_fct(mean_logits.view(-1), labels.float().view(-1))\\n            else:\\n                loss_fct = CrossEntropyLoss()\\n                loss = loss_fct(mean_logits, labels)\\n        return loss\\n\\n    def forward(\\n        self,\\n        input_ids=None,\\n        attention_mask=None,\\n        token_type_ids=None,\\n        position_ids=None,\\n        head_mask=None,\\n        inputs_embeds=None,\\n        labels=None,\\n        sampling_type=\"iid\",\\n        epoch=1,\\n    ):\\n        r\"\"\"\\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\\n            Labels for computing the sequence classification/regression loss.\\n            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\\n            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\\n    Returns:\\n        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\\n        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\\n            Classification (or regression if config.num_labels==1) loss.\\n        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\\n        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\\n            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\\n            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\\n        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\\n            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\\n            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\\n            heads.\\n    Examples::\\n        from transformers import BertTokenizer, BertForSequenceClassification\\n        import torch\\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\\n        outputs = model(input_ids, labels=labels)\\n        loss, logits = outputs[:2]\\n        \"\"\"\\n\\n        final_outputs = {}\\n        outputs = self.bert(\\n            input_ids,\\n            attention_mask=attention_mask,\\n            token_type_ids=token_type_ids,\\n            position_ids=position_ids,\\n            head_mask=head_mask,\\n            inputs_embeds=inputs_embeds,\\n        )\\n        pooled_output = outputs[1]\\n        pooled_output = self.dropout(pooled_output)\\n        loss = {}\\n\\n        if self.deterministic:\\n            pooled_output = self.mlp(pooled_output)\\n            mu, std = self.estimate(pooled_output, self.emb2mu, self.emb2std)\\n            final_outputs[\"z\"] = mu\\n            sampled_logits, logits = self.get_logits(mu, mu, sampling_type='argmax') # always deterministic\\n            if labels is not None:\\n                loss[\"loss\"] = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type='argmax')\\n\n        elif self.ib:\\n            pooled_output = self.mlp(pooled_output)\\n            batch_size = pooled_output.shape[0]\\n            mu, std = self.estimate(pooled_output, self.emb2mu, self.emb2std)\\n            mu_p = self.mu_p.view(1, -1).expand(batch_size, -1)\\n            std_p = torch.nn.functional.softplus(self.std_p.view(1, -1).expand(batch_size, -1))\\n            kl_loss = self.kl_div(mu, std, mu_p, std_p)\\n            z = self.reparameterize(mu, std)\\n            final_outputs[\"z\"] = mu\\n\\n            if self.kl_annealing == \"linear\":\\n                beta = min(1.0, epoch*self.beta)\\n                 \\n            sampled_logits, logits = self.get_logits(z, mu, sampling_type)\\n            if labels is not None:\\n                ce_loss = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type)\\n                loss[\"loss\"] = ce_loss + (beta if self.kl_annealing == \"linear\" else self.beta) * kl_loss\\n        else:\\n            final_outputs[\"z\"] = pooled_output\\n            logits = self.classifier(pooled_output)\\n            if labels is not None:\\n                if self.num_labels == 1:\\n                    #  We are doing regression\\n                    loss_fct = MSELoss()\\n                    loss[\"loss\"] = loss_fct(logits.view(-1), labels.float().view(-1))\\n                else:\\n                    loss_fct = CrossEntropyLoss()\\n                    loss[\"loss\"] = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\\n\\n        final_outputs.update({\"logits\": logits, \"loss\": loss, \"hidden_attention\": outputs[2:]})\\n        return final_outputs\\n",
        "experimental_info": "The experimental setup for the Variational Information Bottleneck (VIB) method is configured via command-line arguments in `run_glue.py`. Key parameters related to VIBERT and its training include:\\n- `--kl_annealing`: Specifies the KL annealing schedule (choices: None, \"linear\").\\n- `--deterministic`: A boolean flag; if specified, learns the reduced dimensions through an MLP in a deterministic manner, serving as a baseline related to the VIB approach.\\n- `--activation`: Sets the activation function for the Multi-Layer Perceptron (MLP) used to map sentence embeddings to latent representation (choices: \"tanh\", \"sigmoid\", \"relu\"; default: \"relu\").\\n- `--beta`: Defines the weight for the information bottleneck loss (default: 1.0).\\n- `--ib`: A boolean flag; if specified, enables the information bottleneck to reduce dimensions.\\n- `--sample_size`: Defines the number of samples for the reparameterization trick in the VIB method (default: 5).\\n- `--ib_dim`: Specifies the dimension of the information bottleneck latent space 'z' (default: 128).\\n- `--model_type`: Type of the pretrained language model (e.g., \"bert\").\\n- `--model_name_or_path`: Path or name of the pre-trained model.\\n- `--task_name`: The name of the GLUE task to train on (e.g., \"mnli\", \"mrpc\").\\n- `--output_dir`: Directory to save model predictions and checkpoints.\\n- `--max_seq_length`: Maximum total input sequence length (default: 128).\\n- `--learning_rate`: Initial learning rate for the AdamW optimizer (default: 2e-5).\\n- `--num_train_epochs`: Total number of training epochs (default: 3.0).\\n- `--per_gpu_train_batch_size`: Batch size per GPU/CPU for training (default: 8).\\n- `--seed`: Random seed for initialization (default: 42).\\n- `--data_seed`: Random seed for data sampling/initialization (default: 66).\\n- `--evaluate_after_each_epoch`: A boolean flag to evaluate the model after each epoch and save the best model.\\n- `--eval_types`: A list specifying the types of datasets to evaluate on (choices: \"train\", \"test\", \"dev\"; default: [\"train\", \"test\"]).\\n- `--binarize_eval`: A boolean flag; if specified, binarizes predictions and labels during evaluation for binary-class datasets.\\n\\nThe `main` function in `run_glue.py` retrieves these arguments and uses them to configure the `BertForSequenceClassification` model (e.g., `config.ib = args.ib`, `config.ib_dim = args.ib_dim`) and set up the training and evaluation loop."
      }
    },
    {
      "title": "LiDAR-in-the-Loop Hyperparameter Optimization"
    },
    {
      "title": "Efficient Hyperparameter Optimization with Adaptive Fidelity Identification",
      "abstract": "Hyperparameter optimization (HPO) and neural architecture search (NAS) are\npowerful in attaining state-of-the-art machine learning models, with Bayesian\noptimization (BO) standing out as a mainstream method. Extending BO into the\nmulti-fidelity setting has been an emerging research topic, but faces the\nchallenge of determining an appropriate fidelity for each hyperparameter\nconfiguration to fit the surrogate model. To tackle the challenge, we propose a\nmulti-fidelity BO method named FastBO, which adaptively decides the fidelity\nfor each configuration and efficiently offers strong performance. The\nadvantages are achieved based on the novel concepts of efficient point and\nsaturation point for each configuration.We also show that our adaptive fidelity\nidentification strategy provides a way to extend any single-fidelity method to\nthe multi-fidelity setting, highlighting its generality and applicability.",
      "full_text": "FastBO: Fast HPO and NAS with Adaptive Fidelity Identification Jiantong Jiang and Ajmal Mian The University of Western Australia, Perth WA 6009, Australia {jiantong.jiang@research.,ajmal.mian@}uwa.edu.au Abstract. Hyperparameter optimization (HPO) and neural architec- ture search (NAS) are powerful in attaining state-of-the-art machine learning models, with Bayesian optimization (BO) standing out as a mainstream method. Extending BO into the multi-fidelity setting has been an emerging research topic, but faces the challenge of determining an appropriate fidelity for each hyperparameter configuration to fit the surrogate model. To tackle the challenge, we propose a multi-fidelity BO method named FastBO, which adaptively decides the fidelity for each configuration and efficiently offers strong performance. The advantages are achieved based on the novel concepts ofefficient pointand saturation point for each configuration. We also show that our adaptive fidelity iden- tification strategy provides a way to extend any single-fidelity method to the multi-fidelity setting, highlighting its generality and applicability. Keywords: HPO · NAS · Multi-fidelity 1 Introduction HPO [9] and NAS [7] aim to find the hyperparameter configuration or archi- tecture λ∗ that minimizesf(λ), the performance obtained by configurationλ. BO [2,10,30] is an effective model-based method for HPO and NAS. It maintains asurrogate modelof the performancebasedon past evaluations ofconfigurations, which guides the choice of promising configurations to evaluate. Recent studies onBOhaveexploredexpertpriors[11,20,26,29],derivativeinformation[1,27,35], and enhancing the interpretability [5,36–39] of HPO and NAS [3,24,25]. However, standard BO requires full evaluations of configurations, which in- curssignificantcosts,especiallyconsideringtheescalatingmodelevaluationover- head. Despite efforts to accelerate model evaluation [13,15–17], smart strategies are required to widely adopt HPO and NAS. Thus, multi-fidelity methods have been proposed [4,12,21,22], where the fidelities mean the performance levels obtained under various resource levels. They follow the idea of successive halv- ing (SHA) [12]: initially, they evaluate many random configurations using few resources; then, based on the low-fidelity performances, only the well-performing ones successively continue to be evaluated with increasing resources. Follow-up studies [8,19,23,28,33] propose model-based multi-fidelity meth- ods, replacing random sampling with more informed models to improve samplearXiv:2409.00584v1  [cs.LG]  1 Sep 20242 J. Jiang and A. Mian efficiency. However, they are based on SHA, which assumes that learning curves of different configurations rarely intersect - a condition that often fails in prac- tice [32], i.e., early performance observations cannot always indicate the final fidelity performance. This raises a key challenge:What is the appropriate fidelity for each configuration to fit the surrogate model?In other words, which fidelity can offer observations that reliably indicate the final fidelity performance? Cur- rent methods struggle with this. Hyper-Tune [23] and BOHB [8] fit separate models for different fidelities, missing inter-fidelity correlations. FTBO [31] and A-BOHB [19] fit joint models but require strong assumptions. Salinas et al. [28] use the last observed fidelity performance, which may get inaccurate surrogate models as it widens the gap between poorly- and well-performing configurations. This paper is an extended abstract of our conference paper [14], highlighting key ideas and the main experimental results, while omitting finer details. 2 Key Idea of FastBO We propose a multi-fidelity extension of BO, namely FastBO, which tackles the challenge of deciding the appropriate fidelity for each configuration to fit the surrogate model. Here, we first propose the key concepts ofefficient point and saturation point, which are crucial in the optimization process. Then, we briefly describe the process of FastBO and highlight its generality. 2.1 Efficient Point and Saturation Point We first formally define the efficient point as follows. Definition 1 (Efficient point). For a given learning curveCi(r) of hyper- parameter configuration or architectureλi, wherer represents the resource level (also referred to as fidelity), the efficient pointei of λi is defined as:ei = min{r | Ci(r) − Ci(2r) < δ1}, whereδ1 is a predefined small threshold. The semantic of Definition 1 is that starting from the efficient point onwards, whentheresourcesaredoubled,theperformanceimprovementfallsbelowasmall threshold. Consequently, this point signifies a fidelity of performance achieved with comparably efficient resource usage. Thus, we make the following remark. Remark 1. The efficient points of the configurations can serve as their ap- propriate fidelities used for fitting the surrogate model. This is due to their (i) optimal resource-to-performance balance, (ii) ability to capture valuable learning curve trends, and (iii) customization for different hyperparameter configurations. We elaborate on the reasons as follows. Firstly, efficient points balance the trade- off between computational cost and result quality. Beyond the efficient point, allocating additional resources becomes less efficient. Secondly, efficient points capture valuable behaviors within the learning curves, enabling more informed decision-making. Thirdly, the ability to customize the fidelity for each specific configuration is an advantage. This adaptive approach is more reasonable than previous studies that use a fixed fidelity for all the configurations.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 3 1 FastBO Methodology•Process overviewWarm-up Learning curve modelingEfficient & saturation pointsConfig !! •Evaluate !! to the warm-up point to get early observation set.•Stop some bad configs. •Estimate !!’s learning curve from its observation set. Post-processing……•Adaptively extract the points from the learning curve. •Stop evaluating at efficient point.•Fit surrogate model. •Resume some best-performing configs to saturation point.Optimal config Fig. 1:Main process of FastBO. FastBO involves estimating efficient and saturation points, modeling learning curves, and auxiliary stages of warm-up and post-processing. Besides efficient points, we identify saturation points for all configurations as well. We provide the definition of the saturation point as follows. Definition 2 (Saturation point).For a given learning curveCi(r) of config- uration λi, wherer represents the resource level (also referred to as fidelity), the saturation pointsi of λi is defined as:si = min{r | ∀r′ > r,|Ci(r′)−Ci(r)| < δ2}, where δ2 is a predefined small threshold. The semantic of Definition 2 is that beyond the saturation point, the observed performance no longer exhibits notable variations with more resources. Thus, this point characterizes the fidelity at which the performance of a configuration stabilizes. Building on the above definition, we make the following remark. Remark 2. The saturation points of the configurations can serve as their ap- proximate final fidelities, as they provide performance results that meet predefined quality thresholds while reducing resource wastage. 2.2 FastBO Process and Generalization With the two crucial points, we show the main process of FastBO in Fig. 1. Each configurationλi first enters a warm-up stage to get its early observation set. Some configurations are terminated here if they are detected consecutive performance deterioration. Then, FastBO estimates the learning curve ofλi from its observation set. Thus, the efficient point and saturation points are adaptively extracted. After that, λi continues to be evaluated to its efficient point; the result is used to update the surrogate model. Finally, the post-processing stage let a small set of promising configurations resume evaluating to their saturation points, and the optimal configurations can be obtained. Generalizing FastBO to single-fidelity methods.The inefficiency of single- fidelity methods like BO stems from their reliance on expensive final fidelity evaluations. Notably, low-fidelity evaluations provide informative insights but are computationally cheaper. Therefore, we can extend single-fidelity methods to the multi-fidelity setting by acquiring the low-fidelity performance for each configuration to fit the surrogate model. To do this, we need to determine the fidelity used to fit the surrogate model. FastBO adaptively determines this fi- delity for each configuration by identifying its efficient point. While this adaptive identification strategy is described in the context of model-based methods, it can4 J. Jiang and A. Mian Fashion-MNIST Airlines Albert CovertypeChristine Fig. 2:Anytime performance on the LCBench benchmark. Slice CIFAR-10 CIFAR-100 ProteinImageNet16-120 (a) NAS-Bench-201 benchmark (b) FCNet benchmark Fig. 3:Anytime performance on(a) NAS-Bench-201 and(b) FCNet. be generalized to various single-fidelity methods. For example, when evaluating configurations within the population for an evolutionary algorithm-based HPO method, we can similarly evaluate the efficient point performances instead of the final performances and integrate them in the subsequent processes, such as selection and variation. To conclude, the proposed strategy provides a simple but effective way to bridge the gap between single- and multi-fidelity methods. 3 Experimental Evaluation We compare the performance of FastBO with random search (RS), standard BO [30], ASHA [21], Hyperband [22], PASHA [4], A-BOHB [19], A-CQR [28], BOHB [8], DyHPO [34], and Hyper-Tune [23]. The results on the LCBench [40], NAS-Bench-201 [6], and FCNet [18] benchmarks are shown in Figs. 2 and 3. Overall, FastBO can handle various performance metrics and shows strong any- time performance. We can observe that FastBO gains an advantage earlier than other methods, rapidly converging to the global optimum after the initial phase. 4 Conclusion and Discussion We propose FastBO, a model-based multi-fidelity HPO method, which excels in adaptively identifying the fidelity for each configuration to fit the surrogate modelandefficientlyprovidinghigh-qualityperformance.Theproposed adaptive fidelity identification strategy also provides a simple way to extend any single- fidelity method to the multi-fidelity setting. While this paper provides a strong foundation on HPO and NAS, we see challenges that demand future improve- ments. Future work could refine and expand Fast-BO to larger search spaces and distributed computing systems to improve its applicability and scalablity.FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 5 References 1. Ament, S.E., Gomes, C.P.: Scalable first-order Bayesian Optimization via struc- tured automatic differentiation. In: International Conference on Machine Learning. pp. 500–516. PMLR (2022) 1 2. Bergstra, J., Bardenet, R., Bengio, Y., Kégl, B.: Algorithms for hyper-parameter optimization. Advances in Neural Information Processing Systems24 (2011) 1 3. Bischl, B., Binder, M., Lang, M., Pielok, T., Richter, J., Coors, S., Thomas, J., Ullmann, T., Becker, M., Boulesteix, A.L., et al.: Hyperparameter optimization: Foundations, algorithms, best practices, and open challenges. Wiley Interdisci- plinary Reviews: Data Mining and Knowledge Discovery13(2), e1484 (2023) 1 4. Bohdal, O., Balles, L., Wistuba, M., Ermis, B., Archambeau, C., Zappella, G.: PASHA: efficient HPO and NAS with progressive resource allocation. In: Interna- tional Conference on Learning Representations. OpenReview.net (2023) 1, 4 5. Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., Su, J.K.: This looks like that: deep learning for interpretable image recognition. Advances in Neural Information Processing Systems32 (2019) 1 6. Dong, X., Yang, Y.: NAS-Bench-201: Extending the scope of reproducible neu- ral architecture search. In: International Conference on Learning Representations (2020) 4 7. Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: A survey. The Journal of Machine Learning Research20(1), 1997–2017 (2019) 1 8. Falkner, S., Klein, A., Hutter, F.: BOHB: Robust and efficient hyperparameter optimization at scale. In: International Conference on Machine Learning. pp. 1437– 1446. PMLR (2018) 1, 2, 4 9. Feurer, M., Hutter, F.: Hyperparameter optimization. Automated Machine Learn- ing: Methods, Systems, Challenges pp. 3–33 (2019) 1 10. Hutter, F., Hoos, H.H., Leyton-Brown, K.: Sequential model-based optimization for general algorithm configuration. In: Learning and Intelligent Optimization. pp. 507–523. Springer (2011) 1 11. Hvarfner,C.,Stoll,D.,Souza,A.L.F.,Lindauer,M.,Hutter,F.,Nardi,L.:$\\pi$BO: Augmenting acquisition functions with user beliefs for bayesian optimization. In: International Conference on Learning Representations. OpenReview.net (2022) 1 12. Jamieson, K., Talwalkar, A.: Non-stochastic best arm identification and hyperpa- rameter optimization. In: Artificial Intelligence and Statistics. pp. 240–248. PMLR (2016) 1 13. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast parallel exact inference on Bayesian networks. In: ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. pp. 425–426 (2023) 1 14. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Efficient hyperparameter optimization with adaptive fidelity identification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 26181–26190 (2024) 2 15. Jiang, J., Wen, Z., Mansoor, A., Mian, A.: Fast inference for probabilistic graphical models. In: 2024 USENIX Annual Technical Conference (USENIX ATC 24) (2024) 1 16. Jiang, J., Wen, Z., Mian, A.: Fast parallel bayesian network structure learning. In: IEEE International Parallel and Distributed Processing Symposium. pp. 617–627. IEEE (2022) 1 17. Jiang, J., Wen, Z., Yang, P., Mansoor, A., Mian, A.: Fast-pgm: Fast probabilistic graphical model learning and inference. arXiv preprint arXiv:2405.15605 (2024) 16 J. Jiang and A. Mian 18. Klein, A., Hutter, F.: Tabular benchmarks for joint architecture and hyperparam- eter optimization. arXiv preprint arXiv:1905.04970 (2019) 4 19. Klein, A., Tiao, L.C., Lienart, T., Archambeau, C., Seeger, M.: Model-based asynchronous hyperparameter and neural architecture search. arXiv preprint arXiv:2003.10865 (2020) 1, 2, 4 20. Li, C., Rana, S., Gupta, S., Nguyen, V., Venkatesh, S., Sutti, A., de Celis Leal, D.R.,Slezak,T.,Height,M.,Mohammed,M.,Gibson,I.:Acceleratingexperimental design by incorporating experimenter hunches. In: International Conference on Data Mining. pp. 257–266. IEEE Computer Society (2018) 1 21. Li,L.,Jamieson,K.,Rostamizadeh,A.,Gonina,E.,Ben-Tzur,J.,Hardt,M.,Recht, B., Talwalkar, A.: A system for massively parallel hyperparameter tuning. Proceed- ings of Machine Learning and Systems2, 230–246 (2020) 1, 4 22. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research18(1), 6765–6816 (2017) 1, 4 23. Li, Y., Shen, Y., Jiang, H., Zhang, W., Li, J., Liu, J., Zhang, C., Cui, B.: Hyper- tune: Towards efficient hyper-parameter tuning at scale. Proceedings of the VLDB Endowment 15(6), 1256–1265 (2022) 1, 2, 4 24. Moosbauer, J., Casalicchio, G., Lindauer, M., Bischl, B.: Improving accuracy of interpretability measures in hyperparameter optimization via Bayesian algorithm execution. arXiv preprint arXiv:2206.05447 (2022) 1 25. Moosbauer, J., Herbinger, J., Casalicchio, G., Lindauer, M., Bischl, B.: Explaining hyperparameter optimization via partial dependence plots. Advances in Neural Information Processing Systems34, 2280–2291 (2021) 1 26. Oh, C., Gavves, E., Welling, M.: BOCK: Bayesian optimization with cylindrical kernels. In: International Conference on Machine Learning. pp. 3868–3877. PMLR (2018) 1 27. Padidar, M., Zhu, X., Huang, L., Gardner, J., Bindel, D.: Scaling gaussian pro- cesses with derivative information using variational inference. Advances in Neural Information Processing Systems34, 6442–6453 (2021) 1 28. Salinas, D., Golebiowski, J., Klein, A., Seeger, M.W., Archambeau, C.: Optimizing hyperparameters with conformal quantile regression. In: International Conference on Machine Learning. vol. 202, pp. 29876–29893. PMLR (2023) 1, 2, 4 29. Shahriari, B., Bouchard-Côté, A., Freitas, N.: Unbounded Bayesian Optimization via regularization. In: Artificial intelligence and statistics. pp. 1168–1176. PMLR (2016) 1 30. Snoek, J., Larochelle, H., Adams, R.P.: Practical Bayesian optimization of machine learning algorithms. Advances in Neural Information Processing Systems25 (2012) 1, 4 31. Swersky, K., Snoek, J., Adams, R.P.: Freeze-thaw Bayesian optimization. arXiv preprint arXiv:1406.3896 (2014) 2 32. Viering, T., Loog, M.: The shape of learning curves: a review. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) 2 33. Wang, J., Xu, J., Wang, X.: Combination of hyperband and bayesian optimization for hyperparameter optimization in deep learning. arXiv preprint arXiv:1801.01596 (2018) 1 34. Wistuba, M., Kadra, A., Grabocka, J.: Supervising the multi-fidelity race of hy- perparameter configurations. Advances in Neural Information Processing Systems 35, 13470–13484 (2022) 4 35. Wu, J., Poloczek, M., Wilson, A.G., Frazier, P.: Bayesian optimization with gradi- ents. Advances in neural information processing systems30 (2017) 1FastBO: Fast HPO and NAS with Adaptive Fidelity Identification 7 36. Yang,P.,Akhtar,N.,Jiang,J.,Mian,A.:Backdoor-basedexplainableaibenchmark forhighfidelityevaluationofattributionmethods.arXivpreprintarXiv:2405.02344 (2024) 1 37. Yang, P., Akhtar, N., Shah, M., Mian, A.: Regulating model reliance on non-robust features by smoothing input marginal density. arXiv preprint arXiv:2407.04370 (2024) 1 38. Yang, P., Akhtar, N., Wen, Z., Mian, A.: Local path integration for attribution. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 3173–3180 (2023) 1 39. Yang, P., Akhtar, N., Wen, Z., Shah, M., Mian, A.: Re-calibrating feature attri- butions for model interpretation. In: International Conference on Learning Repre- sentations (2022) 1 40. Zimmer, L., Lindauer, M.T., Hutter, F.: Auto-Pytorch: Multi-fidelity metalearn- ing for efficient and robust AutoDL. IEEE Transactions on Pattern Analysis and Machine Intelligence43, 3079–3090 (2021) 4",
      "meta_data": {
        "arxiv_id": "2409.00584v1",
        "authors": [
          "Jiantong Jiang",
          "Ajmal Mian"
        ],
        "published_date": "2024-09-01T02:40:04Z",
        "pdf_url": "https://arxiv.org/pdf/2409.00584v1.pdf"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper proposes FastBO, a multi-fidelity Bayesian Optimization (BO) method for Hyperparameter Optimization (HPO) and Neural Architecture Search (NAS). FastBO addresses the challenge of adaptively determining the appropriate fidelity for each hyperparameter configuration to efficiently fit the surrogate model. It introduces novel concepts of \"efficient point\" and \"saturation point\" for each configuration. The adaptive fidelity identification strategy is also shown to generalize, allowing any single-fidelity method to be extended to a multi-fidelity setting.",
        "methodology": "FastBO extends Bayesian Optimization (BO) by introducing two key concepts: the efficient point and the saturation point. The efficient point for a configuration λ_i at resource level r is defined as the minimum r where C_i(r) - C_i(2r) < δ1, signifying a balance between computational cost and performance. The saturation point is defined as the minimum r where for all r' > r, |C_i(r') - C_i(r)| < δ2, indicating performance stabilization. The FastBO process involves a warm-up stage to gather initial observations, learning curve estimation, adaptive extraction of efficient and saturation points, evaluating configurations up to their efficient points to update the surrogate model, and a post-processing stage where promising configurations are evaluated up to their saturation points.",
        "experimental_setup": "FastBO's performance was evaluated against several multi-fidelity HPO methods, including Random Search (RS), standard BO, ASHA, Hyperband, PASHA, A-BOHB, A-CQR, BOHB, DyHPO, and Hyper-Tune. The experiments were conducted on three benchmark datasets: LCBench, NAS-Bench-201, and FCNet. The results were presented as anytime performance, demonstrating FastBO's rapid convergence and strong performance across various metrics and benchmarks.",
        "limitations": "The paper acknowledges that while FastBO provides a strong foundation, there are challenges that require future improvements. Specifically, the current method's applicability and scalability may be limited in larger search spaces and distributed computing systems.",
        "future_research_directions": "Future research could focus on refining and expanding FastBO to effectively handle larger search spaces and integrate it into distributed computing systems. This would enhance its applicability and scalability for more complex HPO and NAS problems."
      }
    }
  ],
  "research_session": {
    "hypothesis": {
      "open_problems": "VIBERT uses a fixed-width latent bottleneck to suppress task-irrelevant information. All K latent dimensions are treated equally, so: (1) some dimensions still carry redundant signal, and (2) when more data are available, an unnecessarily strong, uniform compression can discard useful information, explaining why VIBERT’s advantage shrinks in medium-resource settings. A minimal way to allow the model to keep only the truly useful portions of z is needed.",
      "method": "Sparse-VIBERT (S-VIB)\nAdd a simple L1 sparsity regulariser on the mean vector µ(x) of the variational posterior pθ(z|x):\n    L = L_CE + β·KL( q(z|x) || p(z) ) + α · ||µ(x)||₁\nwhere L_CE is the original task loss and β is identical to VIBERT.  α > 0 is a small coefficient (e.g. 1e-4).\nMotivation:\n•  Because each coordinate of µ represents expected information content of that latent dimension, penalising its absolute value encourages many dimensions to collapse toward zero, i.e. to carry no information.  \n•  The model can now adaptively retain only a sparse subset of informative dimensions, reducing redundancy when data are scarce yet automatically freeing capacity when data grow (dimensions that become useful will pay the tiny α cost and remain active).  \nThis keeps the core VIBERT architecture unchanged—only one term is added to the objective.",
      "experimental_setup": "Base model: BERT-Base uncased.\nDatasets:  • RTE (2.5 K examples) with 32-shot and full-train settings.  • MRPC (3.7 K) with 32-shot and full-train.  \nProtocol: identical hyper-parameters to VIBERT paper (sequence length 128, AdamW 2e-5, 6 epochs, β=1, KL linear annealing).  Compare:\n1. Vanilla fine-tuning\n2. VIBERT (β=1, K=128)\n3. Sparse-VIBERT (β=1, K=128, α∈{1e-5,1e-4,5e-4}).\nMetric: accuracy for RTE, F1 for MRPC (averaged over 5 random seeds).",
      "experimental_code": "# Only the 3 extra lines relative to VIBERT are marked *****\nclass SparseVIBERT(BertForSequenceClassification):\n    def forward(self, *args, **kwargs):\n        out = super().forward(*args, **kwargs)   # obtains dict with 'loss' and saved 'mu'\n        if self.training and self.config.ib:              # VIB active\n            mu_batch = out['z']                          # (B, K) saved by base class\n            l1_penalty = mu_batch.abs().mean()           # *****\n            out['loss'][\"loss\"] = out['loss'][\"loss\"] + self.config.alpha*l1_penalty  # *****\n        return out\n\n# usage\nmodel = SparseVIBERT.from_pretrained(\"bert-base-uncased\", ib=True, ib_dim=128, alpha=1e-4)\n",
      "expected_result": "In 32-shot settings S-VIB is expected to outperform VIBERT by ≈1–2 accuracy/F1 points and vanilla fine-tuning by 3–5 points, because redundant dimensions are switched off, giving stronger regularisation.  In full-data settings the gap between S-VIB and vanilla fine-tuning is small (±0.2 pts) and S-VIB matches or slightly exceeds VIBERT, demonstrating that active dimensions can be re-enabled when useful.",
      "expected_conclusion": "A single L1 term on the bottleneck mean lets VIBERT automatically choose how many latent dimensions to use.  This tiny change (three lines of code) improves low-resource performance and avoids hurting medium-resource training, providing a practical, drop-in enhancement for fine-tuning large language models."
    },
    "iterations": [
      {
        "method": "Sparse-VIBERT (S-VIB)\nAdd a simple L1 sparsity regulariser on the mean vector µ(x) of the variational posterior pθ(z|x):\n    L = L_CE + β·KL( q(z|x) || p(z) ) + α · ||µ(x)||₁\nwhere L_CE is the original task loss and β is identical to VIBERT.  α > 0 is a small coefficient (e.g. 1e-4).\nMotivation:\n•  Because each coordinate of µ represents expected information content of that latent dimension, penalising its absolute value encourages many dimensions to collapse toward zero, i.e. to carry no information.  \n•  The model can now adaptively retain only a sparse subset of informative dimensions, reducing redundancy when data are scarce yet automatically freeing capacity when data grow (dimensions that become useful will pay the tiny α cost and remain active).  \nThis keeps the core VIBERT architecture unchanged—only one term is added to the objective.",
        "experimental_design": {
          "experiment_summary": "We will verify that adding an L1–sparsity term to the Information Bottleneck of VIBERT (Sparse-VIBERT, S-VIB) improves low-resource natural-language understanding.\n\n1. Model: start from the public checkpoint \"bert-base-uncased\" (≈110 M parameters).  Replace the standard classifier with the VIBERT bottleneck (K = 128) and add the α·||µ(x)||₁ penalty inside the training loop.\n2. Data: use the GLUE RTE textual entailment set (2.5 K examples).  Prepare two regimes: a 32-shot subset and the full training split.\n3. Training: sequence length 128, AdamW, 6 epochs, KL linear annealing, β = 1.  Perform grid search over α and learning-rate; keep other VIBERT hyper-parameters fixed for fairness.\n4. Evaluation: report test accuracy averaged over 5 random seeds for each setting.\n5. Comparison: run the original VIBERT implementation (no L1 term) under identical settings.  Superiority of S-VIB in the 32-shot regime will support the hypothesis that sparsifying µ eliminates redundant latent dimensions and yields stronger regularisation.",
          "evaluation_metrics": [
            "Accuracy"
          ],
          "proposed_method": "Sparse-VIBERT (S-VIB)\nObjective:  L = L_CE + β · KL(q(z|x)||p(z)) + α · ||µ(x)||₁\n• Start from the VIBERT variational bottleneck that inserts a K-dimensional Gaussian latent z between encoder and classifier.\n• Penalise the mean vector µ(x) with an L1 term weighted by α>0.  Because |µ_k| ≈ mutual information carried by dimension k, the penalty encourages many coordinates to collapse towards zero, thus switching off unneeded capacity.\n• As training data grow, dimensions that become useful can reactivate by paying the small α cost, so the model adapts its effective width automatically.\nAlgorithmic steps per batch:\n1. Encode input with BERT to hidden h.\n2. Compute µ(x), σ(x) via two linear heads; sample z∼N(µ, σ²) with re-parameterisation.\n3. Classify using z.\n4. Compute cross-entropy loss, KL term, and the new L1 penalty; back-propagate.\nImplementation (PyTorch):\nloss = ce_loss + beta*kl + alpha*mu.abs().mean()",
          "comparative_methods": [
            "VIBERT"
          ],
          "models_to_use": [
            "bert-base-uncased (110M)"
          ],
          "datasets_to_use": [
            "GLUE RTE"
          ],
          "hyperparameters_to_search": {
            "alpha": "1e-5,1e-4,5e-4",
            "learning_rate": "1e-5-5e-5",
            "batch_size": "16,32"
          },
          "external_resources": {
            "hugging_face": {
              "models": [
                {
                  "id": "google-bert/bert-base-uncased",
                  "author": "google-bert",
                  "sha": "86b5e0934494bd15c9632b12f734a8a67f723594",
                  "created_at": "2022-03-02T23:29:04+00:00",
                  "last_modified": "2024-02-19T11:06:12+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 56472580,
                  "likes": 2461,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "LICENSE"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "config.json"
                    },
                    {
                      "rfilename": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/model.mlmodel"
                    },
                    {
                      "rfilename": "coreml/fill-mask/float32_model.mlpackage/Data/com.apple.CoreML/weights/weight.bin"
                    },
                    {
                      "rfilename": "coreml/fill-mask/float32_model.mlpackage/Manifest.json"
                    },
                    {
                      "rfilename": "flax_model.msgpack"
                    },
                    {
                      "rfilename": "model.onnx"
                    },
                    {
                      "rfilename": "model.safetensors"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": "en",
                    "tags": [
                      "exbert"
                    ],
                    "datasets": [
                      "bookcorpus",
                      "wikipedia"
                    ],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "transformers",
                    "pytorch",
                    "tf",
                    "jax",
                    "rust",
                    "coreml",
                    "onnx",
                    "safetensors",
                    "bert",
                    "fill-mask",
                    "exbert",
                    "en",
                    "dataset:bookcorpus",
                    "dataset:wikipedia",
                    "arxiv:1810.04805",
                    "license:apache-2.0",
                    "autotrain_compatible",
                    "endpoints_compatible",
                    "region:us"
                  ],
                  "pipeline_tag": "fill-mask",
                  "library_name": "transformers",
                  "readme": "---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# BERT base model (uncased)\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is uncased: it does not make a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it\nwas pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally masks the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Model variations\n\nBERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.  \nChinese and multilingual uncased and cased versions followed shortly after.  \nModified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.  \nOther 24 smaller models are released afterward.  \n\nThe detailed release history can be found on the [google-research/bert readme](https://github.com/google-research/bert/blob/master/README.md) on github.\n\n| Model | #params | Language |\n|------------------------|--------------------------------|-------|\n| [`bert-base-uncased`](https://huggingface.co/bert-base-uncased) | 110M   | English |\n| [`bert-large-uncased`](https://huggingface.co/bert-large-uncased)              | 340M    | English | sub \n| [`bert-base-cased`](https://huggingface.co/bert-base-cased)        | 110M    | English |\n| [`bert-large-cased`](https://huggingface.co/bert-large-cased) | 340M    |  English |\n| [`bert-base-chinese`](https://huggingface.co/bert-base-chinese) | 110M    | Chinese |\n| [`bert-base-multilingual-cased`](https://huggingface.co/bert-base-multilingual-cased) | 110M | Multiple |\n| [`bert-large-uncased-whole-word-masking`](https://huggingface.co/bert-large-uncased-whole-word-masking) | 340M | English |\n| [`bert-large-cased-whole-word-masking`](https://huggingface.co/bert-large-cased-whole-word-masking) | 340M | English |\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions of a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n\n>>> unmasker(\"The woman worked as a [MASK].\")\n\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe BERT model was pretrained on [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038\nunpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and\nheaders).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus, and in\nthe other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n\"sentences\" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 4 cloud TPUs in Pod configuration (16 TPU chips total) for one million steps with a batch size\nof 256. The sequence length was limited to 128 tokens for 90% of the steps and 512 for the remaining 10%. The optimizer\nused is Adam with a learning rate of 1e-4, \\\\(\\beta_{1} = 0.9\\\\) and \\\\(\\beta_{2} = 0.999\\\\), a weight decay of 0.01,\nlearning rate warmup for 10,000 steps and linear decay of the learning rate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI-(m/mm) | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  | Average |\n|:----:|:-----------:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|:-------:|\n|      | 84.6/83.4   | 71.2 | 90.5 | 93.5  | 52.1 | 85.8  | 88.9 | 66.4 | 79.6    |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href=\"https://huggingface.co/exbert/?model=bert-base-uncased\">\n\t<img width=\"300px\" src=\"https://cdn-media.huggingface.co/exbert/button.png\">\n</a>\n",
                  "extracted_code": "from transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\nunmasker(\"Hello I'm a [MASK] model.\")\n\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n\nfrom transformers import pipeline\nunmasker = pipeline('fill-mask', model='bert-base-uncased')\nunmasker(\"The man worked as a [MASK].\")\nunmasker(\"The woman worked as a [MASK].\")"
                }
              ],
              "datasets": [
                {
                  "id": "gokuls/glue_augmented_rte",
                  "author": "gokuls",
                  "sha": "05e151c0051aeac95f1d93f1e69a9f824d3d6189",
                  "created_at": "2023-01-29T23:55:49+00:00",
                  "last_modified": "2023-01-30T14:26:00+00:00",
                  "private": false,
                  "gated": false,
                  "disabled": false,
                  "downloads": 32,
                  "likes": 0,
                  "siblings": [
                    {
                      "rfilename": ".gitattributes"
                    },
                    {
                      "rfilename": "README.md"
                    },
                    {
                      "rfilename": "data/test-00000-of-00001-f2e9874b7d8577f8.parquet"
                    },
                    {
                      "rfilename": "data/train-00000-of-00001-4614a6d45b95b237.parquet"
                    },
                    {
                      "rfilename": "data/validation-00000-of-00001-84e055cf3d9d89f3.parquet"
                    },
                    {
                      "rfilename": "dataset_infos.json"
                    }
                  ],
                  "card_data": {
                    "license": "apache-2.0",
                    "language": [],
                    "tags": [],
                    "datasets": [],
                    "task_categories": [],
                    "size_categories": [],
                    "metrics": [],
                    "widget": []
                  },
                  "tags": [
                    "license:apache-2.0",
                    "size_categories:100K<n<1M",
                    "format:parquet",
                    "modality:tabular",
                    "modality:text",
                    "library:datasets",
                    "library:pandas",
                    "library:mlcroissant",
                    "library:polars",
                    "region:us"
                  ],
                  "readme": "---\nlicense: apache-2.0\n---\n\n# Dataset Card for glue_augmented_rte\n\n## Dataset Description\n\nAugmented RTE dataset\n\n**Reference:** https://huggingface.co/datasets/glue"
                }
              ]
            }
          },
          "experiment_code": {
            "train_py": "\"\"\"src/train.py\nComplete training script.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport hydra\nimport numpy as np\nimport torch\nimport wandb\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# -------------------------------------------------------------------------------------\n# Make the project root importable no matter where Hydra changes cwd to ----------------\n# -------------------------------------------------------------------------------------\nimport sys\nROOT = Path(__file__).resolve().parent.parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\nfrom src.model import build_model  # noqa: E402  pylint: disable=wrong-import-position\nfrom src.preprocess import build_dataloaders, compute_accuracy  # noqa: E402  pylint: disable=wrong-import-position\n\n\n# -------------------------------------------------------------------------------------\n# Utilities ---------------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_device(cfg: DictConfig) -> torch.device:\n    if torch.cuda.is_available() and cfg.hardware.device == \"cuda\":\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\n\n# -------------------------------------------------------------------------------------\n# Epoch runner ------------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef run_epoch(\n    *,\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer | None,\n    scheduler,\n    scaler: torch.cuda.amp.GradScaler | None,\n    device: torch.device,\n    is_train: bool,\n    cfg: DictConfig,\n    epoch_idx: int,\n    enable_wandb: bool,\n    global_step: int,\n) -> Tuple[Dict[str, float], int]:\n    \"\"\"Runs one epoch and returns aggregated metrics and the updated global_step.\"\"\"\n    mode = \"train\" if is_train else \"val\"\n    model.train() if is_train else model.eval()\n\n    total_loss = 0.0\n    total_kl = 0.0\n    preds: List[int] = []\n    labels: List[int] = []\n\n    step_limit = 2 if cfg.mode == \"trial\" else None\n\n    loop = tqdm(enumerate(dataloader), total=len(dataloader), disable=not enable_wandb)\n    for step, batch in loop:\n        if step_limit is not None and step >= step_limit:\n            break\n        batch = {k: v.to(device) for k, v in batch.items()}\n        labels_batch = batch.pop(\"labels\")\n\n        with torch.cuda.amp.autocast(enabled=cfg.hardware.fp16):\n            outputs = model(**batch, labels=labels_batch)\n            loss = outputs[\"loss\"]\n            logits = outputs[\"logits\"]\n            kl_value = outputs.get(\"kl\", torch.tensor(0.0, device=device))\n\n        if is_train:\n            if scaler:\n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.gradient_clip)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.training.gradient_clip)\n                optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            if scheduler is not None:\n                scheduler.step()\n\n        batch_size = labels_batch.size(0)\n        total_loss += loss.item() * batch_size\n        total_kl += kl_value.item() * batch_size\n        preds.extend(logits.argmax(dim=-1).detach().cpu().tolist())\n        labels.extend(labels_batch.detach().cpu().tolist())\n\n        if enable_wandb:\n            wandb.log({f\"{mode}_loss_step\": loss.item()}, step=global_step)\n        global_step += 1\n\n    n_samples = len(preds)\n    metrics = {\n        f\"{mode}_loss\": total_loss / n_samples,\n        f\"{mode}_acc\": compute_accuracy(preds, labels),\n        f\"{mode}_kl\": total_kl / n_samples,\n    }\n    return metrics, global_step\n\n\n# -------------------------------------------------------------------------------------\n# Evaluation helper -------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef evaluate_model(\n    *,\n    model: torch.nn.Module,\n    dataloader: DataLoader,\n    device: torch.device,\n    cfg: DictConfig,\n) -> Tuple[Dict[str, float], List[int], List[int]]:\n    model.eval()\n    preds: List[int] = []\n    labels_list: List[int] = []\n    total_loss = 0.0\n    total_kl = 0.0\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            labels_batch = batch.pop(\"labels\")\n            with torch.cuda.amp.autocast(enabled=cfg.hardware.fp16):\n                outputs = model(**batch, labels=labels_batch)\n                loss = outputs[\"loss\"]\n                logits = outputs[\"logits\"]\n                kl_value = outputs.get(\"kl\", torch.tensor(0.0, device=device))\n            batch_size = labels_batch.size(0)\n            total_loss += loss.item() * batch_size\n            total_kl += kl_value.item() * batch_size\n            preds.extend(logits.argmax(dim=-1).detach().cpu().tolist())\n            labels_list.extend(labels_batch.detach().cpu().tolist())\n\n    metrics = {\n        \"test_loss\": total_loss / len(preds),\n        \"test_acc\": compute_accuracy(preds, labels_list),\n        \"test_kl\": total_kl / len(preds),\n    }\n    return metrics, preds, labels_list\n\n\n# -------------------------------------------------------------------------------------\n# Training routine --------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef train_once(cfg: DictConfig, *, enable_wandb: bool = True) -> Tuple[float, Dict[str, float]]:\n    \"\"\"Trains the model once using the hyper-parameters inside cfg.\n\n    Returns\n    -------\n    Tuple[float, Dict[str, float]]\n        (best validation accuracy, dictionary with best metrics across train/val/test)\n    \"\"\"\n\n    device = get_device(cfg)\n    set_seed(cfg.seed)\n\n    # ----------------------------------------- Data\n    train_loader, val_loader, test_loader = build_dataloaders(cfg)\n\n    # ----------------------------------------- Model\n    model = build_model(cfg)\n    model.to(device)\n\n    # ----------------------------------------- Optimiser & Scheduler\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        weight_decay=cfg.training.weight_decay,\n        betas=tuple(cfg.training.betas),\n    )\n    total_train_steps = max(1, len(train_loader) * cfg.training.num_epochs)\n    scheduler = torch.optim.lr_scheduler.LinearLR(\n        optimizer,\n        start_factor=1.0,\n        end_factor=0.0,\n        total_iters=total_train_steps,\n    )\n    scaler = torch.cuda.amp.GradScaler(enabled=cfg.hardware.fp16)\n\n    # ----------------------------------------- WandB initialisation\n    wandb_run = None\n    if enable_wandb and cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=cfg.run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n            mode=cfg.wandb.mode,\n        )\n        print(f\"WandB URL: {wandb_run.url}\")\n\n    # ----------------------------------------- Training loop\n    best_val_acc = -math.inf\n    best_metrics: Dict[str, float] = {}\n    global_step = 0\n    for epoch in range(cfg.training.num_epochs):\n        train_metrics, global_step = run_epoch(\n            model=model,\n            dataloader=train_loader,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            scaler=scaler,\n            device=device,\n            is_train=True,\n            cfg=cfg,\n            epoch_idx=epoch,\n            enable_wandb=enable_wandb and wandb_run is not None,\n            global_step=global_step,\n        )\n\n        with torch.no_grad():\n            val_metrics, global_step = run_epoch(\n                model=model,\n                dataloader=val_loader,\n                optimizer=None,\n                scheduler=None,\n                scaler=None,\n                device=device,\n                is_train=False,\n                cfg=cfg,\n                epoch_idx=epoch,\n                enable_wandb=enable_wandb and wandb_run is not None,\n                global_step=global_step,\n            )\n\n        epoch_metrics = {\"epoch\": epoch, **train_metrics, **val_metrics}\n        if wandb_run is not None:\n            wandb.log(epoch_metrics, step=global_step)\n\n        if val_metrics[\"val_acc\"] > best_val_acc:\n            best_val_acc = val_metrics[\"val_acc\"]\n            best_metrics = epoch_metrics\n\n    # ----------------------------------------- Final evaluation on test-set\n    test_metrics, test_preds, test_labels = evaluate_model(\n        model=model,\n        dataloader=test_loader,\n        device=device,\n        cfg=cfg,\n    )\n\n    best_metrics.update(test_metrics)\n\n    # Confusion matrix for later visualisation\n    cm = confusion_matrix(test_labels, test_preds).tolist()\n    best_metrics[\"test_confusion_matrix\"] = cm\n\n    # ----------------------------------------- Write to WandB summary\n    if wandb_run is not None:\n        for k, v in best_metrics.items():\n            wandb_run.summary[k] = v\n        wandb_run.finish()\n\n    return best_val_acc, best_metrics\n\n\n# -------------------------------------------------------------------------------------\n# Optuna objective --------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef objective(trial, *, base_cfg: DictConfig) -> float:\n    # Deep-copy cfg via OmegaConf container --------------------------------\n    cfg = OmegaConf.create(OmegaConf.to_container(base_cfg, resolve=True))\n\n    # Sample hyper-parameters ----------------------------------------------\n    for hp_name, hp_cfg in cfg.optuna.search_space.items():\n        if hp_cfg[\"type\"] == \"loguniform\":\n            sampled = trial.suggest_float(hp_name, hp_cfg[\"low\"], hp_cfg[\"high\"], log=True)\n        elif hp_cfg[\"type\"] == \"categorical\":\n            sampled = trial.suggest_categorical(hp_name, hp_cfg[\"choices\"])\n        else:\n            raise ValueError(f\"Unknown hp type: {hp_cfg['type']}\")\n\n        # Write back to corresponding section ------------------------------\n        if hp_name in cfg.training:\n            cfg.training[hp_name] = sampled\n        elif hp_name in cfg.model:\n            cfg.model[hp_name] = sampled\n        else:\n            cfg[hp_name] = sampled\n\n    # Disable WandB inside Optuna trials -----------------------------------\n    cfg.wandb.mode = \"disabled\"\n\n    val_acc, _ = train_once(cfg, enable_wandb=False)\n    return val_acc\n\n\n# -------------------------------------------------------------------------------------\n# Hydra entry-point -------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):  # noqa: C901, pylint: disable=too-many-branches\n    # Restore original working directory to keep paths consistent ----------\n    original_cwd = Path(get_original_cwd())\n\n    # Ensure HF caches go to .cache/ ---------------------------------------\n    os.environ[\"TRANSFORMERS_CACHE\"] = str(original_cwd / \".cache\")\n    os.environ[\"HF_DATASETS_CACHE\"] = str(original_cwd / \".cache\")\n\n    # --------------------------- Mode adjustments -------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.num_epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # --------------------------- Optuna search (if any) -------------------\n    best_cfg = cfg\n    if cfg.optuna.n_trials > 0:\n        import optuna\n\n        study = optuna.create_study(direction=cfg.optuna.direction)\n        study.optimize(lambda t: objective(t, base_cfg=cfg), n_trials=cfg.optuna.n_trials)\n\n        # Merge best params back into cfg ----------------------------------\n        for k, v in study.best_params.items():\n            if k in best_cfg.training:\n                best_cfg.training[k] = v\n            elif k in best_cfg.model:\n                best_cfg.model[k] = v\n            else:\n                best_cfg[k] = v\n        print(\"Optuna finished. Best val_acc =\", study.best_value)\n        print(\"Best params:\", study.best_params)\n\n    # --------------------------- Final training ---------------------------\n    _, best_metrics = train_once(best_cfg, enable_wandb=(best_cfg.wandb.mode != \"disabled\"))\n\n    # --------------------------- Save metrics locally ---------------------\n    results_dir = original_cwd / Path(cfg.results_dir) / cfg.run_id\n    results_dir.mkdir(parents=True, exist_ok=True)\n    with open(results_dir / \"metrics.json\", \"w\") as fp:\n        json.dump(best_metrics, fp, indent=2)\n\n    print(\"Training finished. Metrics written to\", results_dir / \"metrics.json\")\n\n\nif __name__ == \"__main__\":\n    main()",
            "evaluate_py": "\"\"\"src/evaluate.py\nIndependent evaluation and visualisation script.\n\nCLI (MANDATORY):\n    uv run python -m src.evaluate results_dir=PATH run_ids='[\"run-1\", \"run-2\"]'\n\nThis script purposefully does NOT use Hydra; instead it parses the\n`key=value` style arguments required by the specification.\n\"\"\"\nfrom __future__ import annotations\n\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom scipy import stats\n\n# -------------------------------------------------------------------------------------\n# Make repository importable regardless of CWD ----------------------------------------\n# -------------------------------------------------------------------------------------\nROOT = Path(__file__).resolve().parent.parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\n\n# -------------------------------------------------------------------------------------\n# CLI Parsing (key=value style) --------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef parse_cli(argv: List[str]) -> dict[str, str]:\n    \"\"\"Parses `key=value` arguments into a dictionary.\n\n    Parameters\n    ----------\n    argv : List[str]\n        sys.argv list (including program name).\n\n    Returns\n    -------\n    dict[str, str]\n        Mapping from argument names (str) to their raw string values.\n    \"\"\"\n    if len(argv) <= 1:\n        raise SystemExit(\n            \"No arguments supplied. Expected usage: python -m src.evaluate \"\n            \"results_dir=PATH run_ids='[\\\"run-1\\\"]'\"\n        )\n\n    parsed: dict[str, str] = {}\n    for arg in argv[1:]:\n        if \"=\" not in arg:\n            raise SystemExit(\n                f\"Invalid argument '{arg}'. Expected key=value style with '=' present.\"\n            )\n        key, value = arg.split(\"=\", 1)\n        if not key:\n            raise SystemExit(f\"Malformed argument '{arg}' (empty key before '=').\")\n        parsed[key] = value\n    return parsed\n\n\n# -------------------------------------------------------------------------------------\n# Helper utilities --------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef load_wandb_entity_project() -> tuple[str, str]:\n    \"\"\"Reads entity/project from config/config.yaml (root of repo).\"\"\"\n    import yaml\n\n    cfg_path = ROOT / \"config\" / \"config.yaml\"\n    if not cfg_path.exists():\n        raise FileNotFoundError(f\"Cannot locate {cfg_path}\")\n    with open(cfg_path, \"r\", encoding=\"utf-8\") as fp:\n        cfg = yaml.safe_load(fp)\n    return cfg[\"wandb\"][\"entity\"], cfg[\"wandb\"][\"project\"]\n\n\ndef plot_learning_curve(history_df: pd.DataFrame, metric: str, out_path: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    sns.lineplot(data=history_df, x=history_df.index, y=metric)\n    best_idx = history_df[metric].idxmax()\n    best_val = history_df.loc[best_idx, metric]\n    plt.scatter([best_idx], [best_val], color=\"red\")\n    plt.annotate(f\"{best_val:.3f}\", (best_idx, best_val))\n    plt.title(metric)\n    plt.xlabel(\"step\")\n    plt.ylabel(metric)\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef save_confusion_matrix(cm: list[list[int]], out_path: Path) -> None:\n    cm_arr = np.array(cm)\n    plt.figure(figsize=(4, 4))\n    sns.heatmap(cm_arr, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef bar_chart(df: pd.DataFrame, metric: str, out_path: Path) -> None:\n    plt.figure(figsize=(8, 4))\n    sns.barplot(x=\"run_id\", y=metric, data=df)\n    plt.xticks(rotation=45, ha=\"right\")\n    for p in plt.gca().patches:\n        height = p.get_height()\n        plt.text(\n            p.get_x() + p.get_width() / 2,\n            height,\n            f\"{height:.3f}\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n    plt.ylabel(metric)\n    plt.title(f\"Comparison – {metric}\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\ndef box_plot(df: pd.DataFrame, metric: str, out_path: Path) -> None:\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(y=metric, data=df)\n    sns.stripplot(y=metric, data=df, color=\"red\", jitter=0.2, size=4)\n    plt.title(f\"Distribution of {metric}\")\n    plt.tight_layout()\n    plt.savefig(out_path)\n    plt.close()\n\n\n# -------------------------------------------------------------------------------------\n# Main --------------------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef main() -> None:  # noqa: C901\n    # ------------------------------ Parse CLI ----------------------------------------\n    cli_args = parse_cli(sys.argv)\n    if \"results_dir\" not in cli_args or \"run_ids\" not in cli_args:\n        raise SystemExit(\n            \"Both 'results_dir' and 'run_ids' must be specified. Example: \"\n            \"python -m src.evaluate results_dir=PATH run_ids='[\\\"run-1\\\"]'\"\n        )\n\n    results_dir = Path(cli_args[\"results_dir\"]).expanduser()\n    run_ids: List[str] = json.loads(cli_args[\"run_ids\"])\n\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    # ------------------------------ WandB API ----------------------------------------\n    entity, project = load_wandb_entity_project()\n    api = wandb.Api()\n\n    per_run_records: list[dict] = []\n    generated_files: list[Path] = []\n\n    # ------------------------------ Per-run processing ------------------------------\n    for run_id in run_ids:\n        run_out_dir = results_dir / run_id\n        run_out_dir.mkdir(parents=True, exist_ok=True)\n\n        run = api.run(f\"{entity}/{project}/{run_id}\")\n        history_df = run.history()  # pandas DataFrame with step index\n        summary = dict(run.summary._json_dict)\n        config = dict(run.config)\n\n        # --------- Save metrics.json -------------------------------------------------\n        metrics_path = run_out_dir / \"metrics.json\"\n        with open(metrics_path, \"w\", encoding=\"utf-8\") as fp:\n            json.dump({\"summary\": summary, \"config\": config}, fp, indent=2)\n        generated_files.append(metrics_path)\n\n        # --------- Learning curves ---------------------------------------------------\n        for metric in [m for m in [\"train_acc\", \"val_acc\"] if m in history_df.columns]:\n            fig_path = run_out_dir / f\"{run_id}_learning_curve_{metric}.pdf\"\n            plot_learning_curve(history_df, metric, fig_path)\n            generated_files.append(fig_path)\n\n        # --------- Confusion matrix (if present) -------------------------------------\n        if \"test_confusion_matrix\" in summary:\n            cm_fig_path = run_out_dir / f\"{run_id}_confusion_matrix.pdf\"\n            save_confusion_matrix(summary[\"test_confusion_matrix\"], cm_fig_path)\n            generated_files.append(cm_fig_path)\n\n        per_run_record = {\"run_id\": run_id}\n        per_run_record.update({k: v for k, v in summary.items() if isinstance(v, (int, float))})\n        per_run_records.append(per_run_record)\n\n    # ------------------------------ Aggregated analysis -----------------------------\n    comp_dir = results_dir / \"comparison\"\n    comp_dir.mkdir(parents=True, exist_ok=True)\n\n    comp_df = pd.DataFrame(per_run_records)\n    comp_json_path = comp_dir / \"aggregated_metrics.json\"\n    comp_df.to_json(comp_json_path, orient=\"records\", indent=2)\n    generated_files.append(comp_json_path)\n\n    # ------------------------------ Improvement rates -------------------------------\n    if len(comp_df) >= 2 and \"val_acc\" in comp_df.columns:\n        baseline_val = comp_df.iloc[0][\"val_acc\"]\n        comp_df[\"improvement_rate\"] = (comp_df[\"val_acc\"] - baseline_val) / baseline_val\n\n    # ------------------------------ Figures -----------------------------------------\n    metric_for_bar = \"val_acc\" if \"val_acc\" in comp_df.columns else comp_df.columns[1]\n    bar_path = comp_dir / f\"comparison_{metric_for_bar}_bar_chart.pdf\"\n    bar_chart(comp_df, metric_for_bar, bar_path)\n    generated_files.append(bar_path)\n\n    box_path = comp_dir / f\"comparison_{metric_for_bar}_box_plot.pdf\"\n    box_plot(comp_df, metric_for_bar, box_path)\n    generated_files.append(box_path)\n\n    # ------------------------------ Statistical test --------------------------------\n    if len(comp_df) >= 2 and metric_for_bar in comp_df.columns:\n        a, b = comp_df.iloc[0][metric_for_bar], comp_df.iloc[1][metric_for_bar]\n        t_stat, p_val = stats.ttest_ind([a], [b], equal_var=False)\n        stats_path = comp_dir / \"significance_tests.json\"\n        with open(stats_path, \"w\", encoding=\"utf-8\") as fp:\n            json.dump({\"metric\": metric_for_bar, \"t_stat\": t_stat, \"p_value\": p_val}, fp, indent=2)\n        generated_files.append(stats_path)\n\n    # ------------------------------ Print generated files ---------------------------\n    for path in generated_files:\n        print(path)\n\n\nif __name__ == \"__main__\":\n    # Respect WANDB_API_KEY if provided in environment; if not, log-in-less.\n    os.environ.setdefault(\"WANDB_SILENT\", \"true\")\n    main()",
            "preprocess_py": "\"\"\"src/preprocess.py\nData loading & preprocessing utilities.\n\"\"\"\nfrom __future__ import annotations\n\nimport random\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom datasets import DatasetDict, load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\n\n# -------------------------------------------------------------------------------------\n# Generic helpers ---------------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef compute_accuracy(preds, labels):\n    preds_arr = np.array(preds)\n    labels_arr = np.array(labels)\n    return float((preds_arr == labels_arr).mean())\n\n\n# -------------------------------------------------------------------------------------\n# GLUE-specific processing -------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef _glue_tokenise(examples, tokenizer, max_length):\n    return tokenizer(\n        examples[\"sentence1\"],\n        examples[\"sentence2\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length,\n    )\n\n\n# -------------------------------------------------------------------------------------\n# Main dataloader builder --------------------------------------------------------------\n# -------------------------------------------------------------------------------------\n\ndef build_dataloaders(cfg) -> Tuple[DataLoader, DataLoader, DataLoader]:\n    \"\"\"Returns train_loader, val_loader, test_loader\"\"\"\n\n    # ------------------------- Dataset loading ----------------------------------------\n    if cfg.dataset.name != \"glue\":\n        raise ValueError(\"Only GLUE datasets are supported in this template\")\n    ds: DatasetDict = load_dataset(\n        \"glue\",\n        cfg.dataset.subset,\n        cache_dir=\".cache/\",\n    )\n\n    # ------------------------- Few-shot / full setup ----------------------------------\n    rng = random.Random(cfg.seed)\n    if cfg.dataset.setup == \"32-shot\":\n        shots = 32\n        label_list = list(set(ds[\"train\"][\"label\"]))\n        per_label = shots // len(label_list)\n        sel_indices = []\n        for lab in label_list:\n            idxs = [i for i, y in enumerate(ds[\"train\"][\"label\"]) if y == lab]\n            sel_indices.extend(rng.sample(idxs, per_label))\n        ds[\"train\"] = ds[\"train\"].select(sel_indices)\n    # For trial mode, down-sample heavily to make CI fast ------------------------------\n    if cfg.mode == \"trial\":\n        ds[\"train\"] = ds[\"train\"].select(range(min(64, len(ds[\"train\"]))))\n        ds[\"validation\"] = ds[\"validation\"].select(range(min(64, len(ds[\"validation\"]))))\n        ds[\"test\"] = ds[\"validation\"]  # GLUE has no test labels; use validation\n\n    # ------------------------- Train/val split ----------------------------------------\n    train_valid = ds[\"train\"].train_test_split(test_size=0.1, seed=cfg.seed)\n    ds = DatasetDict(\n        {\n            \"train\": train_valid[\"train\"],\n            \"validation\": train_valid[\"test\"],\n            \"test\": ds[\"validation\"],  # GLUE's official validation split used as test here\n        }\n    )\n\n    # ------------------------- Tokenisation -------------------------------------------\n    tokenizer = AutoTokenizer.from_pretrained(cfg.model.name, cache_dir=\".cache/\")\n\n    def tokenise_fn(examples):\n        out = _glue_tokenise(examples, tokenizer, cfg.dataset.max_length)\n        out[\"labels\"] = examples[\"label\"]\n        return out\n\n    ds = ds.map(tokenise_fn, batched=True, remove_columns=ds[\"train\"].column_names)\n    ds.set_format(type=\"torch\")\n\n    # ------------------------- DataLoaders -------------------------------------------\n    def collate_fn(batch):\n        keys = batch[0].keys()\n        return {k: torch.stack([b[k] for b in batch]) for k in keys}\n\n    train_loader = DataLoader(ds[\"train\"], batch_size=cfg.training.batch_size, shuffle=cfg.dataset.shuffle, collate_fn=collate_fn)\n    val_loader = DataLoader(ds[\"validation\"], batch_size=cfg.training.batch_size, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(ds[\"test\"], batch_size=cfg.training.batch_size, shuffle=False, collate_fn=collate_fn)\n\n    return train_loader, val_loader, test_loader",
            "model_py": "\"\"\"src/model.py\nModel architectures for VIBERT and Sparse-VIBERT classifier.\n\"\"\"\nfrom __future__ import annotations\n\nfrom typing import Dict\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModel\n\n\nclass VIBClassifier(nn.Module):\n    \"\"\"Variational Information Bottleneck classifier with optional L1 sparsity.\"\"\"\n\n    def __init__(\n        self,\n        base_model_name: str,\n        num_labels: int = 2,\n        ib: bool = True,\n        ib_dim: int = 128,\n        beta: float = 1.0,\n        alpha: float = 0.0,\n    ) -> None:\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_name, cache_dir=\".cache/\")\n        self.hidden_size = self.encoder.config.hidden_size\n\n        self.ib = ib\n        self.ib_dim = ib_dim\n        self.beta = beta\n        self.alpha = alpha\n        self.num_labels = num_labels\n\n        if self.ib:\n            self.fc_mu = nn.Linear(self.hidden_size, ib_dim)\n            self.fc_logvar = nn.Linear(self.hidden_size, ib_dim)\n            classifier_in = ib_dim\n        else:\n            classifier_in = self.hidden_size\n        self.classifier = nn.Linear(classifier_in, num_labels)\n        self.dropout = nn.Dropout(0.1)\n        self.criterion = nn.CrossEntropyLoss()\n\n    @staticmethod\n    def _reparameterise(mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):  # noqa: D401\n        enc_out = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        pooled = enc_out.pooler_output if hasattr(enc_out, \"pooler_output\") else enc_out.last_hidden_state[:, 0]\n        pooled = self.dropout(pooled)\n\n        kl_val = torch.tensor(0.0, device=pooled.device)\n        l1_pen = torch.tensor(0.0, device=pooled.device)\n\n        if self.ib:\n            mu = self.fc_mu(pooled)\n            logvar = self.fc_logvar(pooled)\n            z = self._reparameterise(mu, logvar)\n            kl_val = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - 1.0 - logvar, dim=1).mean()\n            l1_pen = mu.abs().mean()\n        else:\n            z = pooled\n\n        logits = self.classifier(z)\n        output: Dict[str, torch.Tensor] = {\"logits\": logits, \"kl\": kl_val}\n\n        if labels is not None:\n            ce = self.criterion(logits.view(-1, self.num_labels), labels.view(-1))\n            loss = ce + self.beta * kl_val + self.alpha * l1_pen\n            output[\"loss\"] = loss\n        return output\n\n\ndef build_model(cfg):\n    return VIBClassifier(\n        base_model_name=cfg.model.name,\n        num_labels=2,\n        ib=cfg.model.ib,\n        ib_dim=cfg.model.ib_dim,\n        beta=cfg.model.beta,\n        alpha=cfg.model.get(\"alpha\", 0.0),\n    )",
            "main_py": "\"\"\"src/main.py\nOrchestrator that spawns `src.train` as a subprocess.\n\"\"\"\nfrom __future__ import annotations\n\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):\n    original_cwd = Path(get_original_cwd())\n\n    # ------------------------- Mode logic -------------------------------------------\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n        cfg.training.num_epochs = 1\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    # ------------------------- Spawn training subprocess ----------------------------\n    cmd = [\n        sys.executable,\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run_id}\",\n        f\"mode={cfg.mode}\",\n        f\"results_dir={cfg.results_dir}\",\n    ]\n    subprocess.run(cmd, cwd=original_cwd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()",
            "pyproject_toml": "[tool.poetry]\nname = \"sparse-vibert-experiments\"\nversion = \"0.1.0\"\ndescription = \"Reproducible experiments for Sparse-VIBERT paper\"\nauthors = [\"AI Researcher <research@example.com>\"]\nreadme = \"README.md\"\n\n[tool.poetry.dependencies]\npython = \"^3.10\"\n# Deep-learning stack\ntorch = \"^2.2.0\"\ntransformers = \"^4.38.0\"\ndatasets = \"^2.18.0\"\n# Experiment management\nhydra-core = \"^1.3.2\"\nwandb = \"^0.16.4\"\noptuna = \"^3.5.0\"\n# Analysis & visualisation\nmatplotlib = \"^3.8.2\"\nseaborn = \"^0.13.2\"\npandas = \"^2.2.0\"\nscipy = \"^1.12.0\"\nscikit-learn = \"^1.4.0\"\npyyaml = \"^6.0.1\"\ntqdm = \"^4.66.2\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"",
            "config_yaml": "# config/config.yaml\n# Root Hydra configuration – do NOT edit via code, override from CLI instead.\n\ndefaults:\n  - _self_\n  - runs: ${run}\n\n# The variable `run` is provided from CLI: run=<run_id>\nrun: null\n\nmode: full     # overridden with mode=trial in CI validation\nresults_dir: ./results\nseed: 42\n\nwandb:\n  entity: gengaru617-personal\n  project: 251106-test\n  mode: online   # auto-switched to \"disabled\" in trial mode\n\n# Prevent Hydra from changing working dir ---------------------------------------------\nhydra:\n  run:\n    dir: .\n  output_subdir: null"
          }
        },
        "experiment_runs": [
          {
            "run_id": "proposed-bert-base-uncased-110M--GLUE-RTE",
            "method_name": "proposed",
            "model_name": "bert-base-uncased (110M)",
            "dataset_name": "GLUE RTE",
            "run_config": "run_id: proposed-bert-base-uncased-110M--GLUE-RTE\nmethod: sparse_vibert\nseed: ${seed}\nmodel:\n  name: bert-base-uncased\n  architecture: Sparse-VIBERT\n  pretrained: true\n  hidden_size: 768\n  num_layers: 12\n  ib: true\n  ib_dim: 128\n  beta: 1.0\n  alpha: 1e-4  # default, tuned by Optuna\n\ndataset:\n  name: glue\n  subset: rte\n  setup: full  # use \"32-shot\" to switch to few-shot setting\n  max_length: 128\n  shuffle: true\n\ntraining:\n  num_epochs: 6\n  batch_size: 32      # default, tuned by Optuna\n  learning_rate: 2e-5 # default, tuned by Optuna\n  optimizer: adamw\n  weight_decay: 0.01\n  betas: [0.9, 0.999]\n  warmup_steps: 0\n  gradient_clip: 1.0\n  kl_annealing:\n    type: linear\n    num_steps: 500\n\nevaluation:\n  metric: accuracy\n\nhardware:\n  device: cuda\n  fp16: true\n  max_gpu_memory: 80gb\n\noptuna:\n  n_trials: 30\n  direction: maximize\n  metric_name: accuracy\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-5\n    batch_size:\n      type: categorical\n      choices: [16, 32]\n    alpha:\n      type: categorical\n      choices: [1e-5, 1e-4, 5e-4]\n"
          },
          {
            "run_id": "comparative-1-bert-base-uncased-110M--GLUE-RTE",
            "method_name": "comparative-1",
            "model_name": "bert-base-uncased (110M)",
            "dataset_name": "GLUE RTE",
            "run_config": "run_id: comparative-1-bert-base-uncased-110M--GLUE-RTE\nmethod: vibert\nseed: ${seed}\nmodel:\n  name: bert-base-uncased\n  architecture: VIBERT\n  pretrained: true\n  hidden_size: 768\n  num_layers: 12\n  ib: true\n  ib_dim: 128\n  beta: 1.0\n\ndataset:\n  name: glue\n  subset: rte\n  setup: full\n  max_length: 128\n  shuffle: true\n\ntraining:\n  num_epochs: 6\n  batch_size: 32      # default, tuned by Optuna\n  learning_rate: 2e-5 # default, tuned by Optuna\n  optimizer: adamw\n  weight_decay: 0.01\n  betas: [0.9, 0.999]\n  warmup_steps: 0\n  gradient_clip: 1.0\n  kl_annealing:\n    type: linear\n    num_steps: 500\n\nevaluation:\n  metric: accuracy\n\nhardware:\n  device: cuda\n  fp16: true\n  max_gpu_memory: 80gb\n\noptuna:\n  n_trials: 20\n  direction: maximize\n  metric_name: accuracy\n  search_space:\n    learning_rate:\n      type: loguniform\n      low: 1e-5\n      high: 5e-5\n    batch_size:\n      type: categorical\n      choices: [16, 32]\n"
          }
        ]
      }
    ]
  }
}